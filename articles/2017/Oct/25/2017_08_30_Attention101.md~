Title: 喽! Attention! Uwaga! ¡Atención! Achtung! 請注意!
Status: hidden
Category: Neural Nets
Date: 2017-08-25 19:30
Modified: 2017-08-25 19:30
Tags: Attention models, Seqence modeling,
Slug: Attention101
Related_posts:   s2s-with-RNN
Summary:

##### Recommended reading before approaching this post:
* RNN – Andrej Karpathy’s blog [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

https://www.oreilly.com/ideas/unsupervised-learning-attention-and-other-mysteries?log-out
https://www.reddit.com/r/MachineLearning/comments/3z982d/attention_and_memory_in_deep_learning_and_nlp/

### Intro


When looking at a picture of woman in red dress:

![Lady in red dress ]({attach}img/lird1.jpg)
<br>
Figure 1. Attention weights are represented by biologically inspired feature like the dress_color=red, smile=true.
{: align=center }

Well, what is the place that our perception is focusing on? Is it the big area of waterfall in the back that is making noise? (did you even know it was there?) This is all how our brain works and lies to us about what is true, and moreover what it believes is objectively important.

Our perception is directed towards the... red color, and some possibly other subjects temporarily considered as primal objects of interest. One way or another, the attention is on her, and the rest of the scene seem a bit more blurry or even unnoticed. That is even though the waterfall behind her is rather large and noisy. So the brain is focused on her. The perception and learning starts on her, while dismissing temporarily the remaining, surrounding input.

See the most interesting frame that moves along frame sequence is...  

![Lady in red dress frameset ]({attach}img/lird2.jpg)
<br>
Figure 2. Attention weight are represented in a sequence of frames might consider time related feature of the sequence e.g. eye_contact_length=long (constant across multiple time steps).
{: align=center }

the one that focus tends to be the most probable (purely subjective and hypothetical thesis ;) ).

What makes her so different. Well she is attracting our attention. How? She is the only character in the setting wearing a vibrant shade of red. In **contrast**, other pedestrians' attire is mostly black. She makes **eye contact **with the subject and **smiles**.

![Mem]({attach}img/mem.jpg)
<br>
Figure 3. When attention weights gets wrong.
{: align=center }

Loosing attention on her makes you no more focusing on the nature of the attention which in this case might have ended deadly.

![Agent smith]({attach}img/smith1.png)
<br>
Figure 4. When attention weights gets wrong.
{: align=center }

It turns out that this filtering out unnecessary data can bee applied to neural networks to help handle sequence processing tasks in cases where vanilla s2s fails.
###

Other means:
sum or average pooling

Praca doktorska Karol kurach jest o tym po co attencjion
How brain lies:
https://www.youtube.com/watch?v=-_2mj1pwveo

http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/

### Motivation
### Algorithm standard name and abbreviations
### Information on processing strategy of the algorithm
### Objective or goal for the algorithm
### Metaphors or analogies are commonly used to describe the behavior of the algorithm?
### Pseudocode or flowchart description of the algorithm
### Heuristics or rules of thumb
### What classes of problem is the algorithm well suited?
### Common benchmark or example datasets used to demonstrate the algorithm
### Useful resources for learning more about the algorithm
### Primary references or resources in which the algorithm was first described


---
#### References:

<script>
<!--  Instead adding {:target="_blank"} to every link this adds it automatically additionally it adds also   link icon from http://fontawesome.io/icon/external-link/-->
(function() {
    var hostname = window.location.hostname;
    var new_tab = true;
    var set_icon = true;
    for (var links = document.links, i = 0, a; a = links[i]; i++) {
        if (a.hostname !== hostname) {
            if (new_tab)
                a.target = '_blank';
            if (set_icon)
                a.innerHTML +=
                    '&nbsp<i class="fa fa-external-link fa-1 external-link-margin" />';
        }
    }
})();
</script>
