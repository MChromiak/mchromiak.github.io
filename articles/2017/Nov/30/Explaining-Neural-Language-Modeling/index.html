<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>NLP: Explaining Neural Language Modeling - Michał Chromiak's blog</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">


    <link href="/static_files/img/favicon.jpg" rel="icon">

<link rel="canonical" href="/articles/2017/Nov/30/Explaining-Neural-Language-Modeling/">

        <meta name="author" content="Michał Chromiak" />
        <meta name="keywords" content="NLP,language model,ngram,perplexity,smoothing" />
        <meta name="description" content="Language modeling (LM) is the essential part of Natural Language Processing (NLP) tasks such as Machine Translation, Spell Correction Speech Recognition, Summarization, Question Answering, Sentiment analysis etc. Goal of the Language Model is to compute the probability of sentence considered as a word sequence. This article explains how to model the language using probability and n-grams. It also discuss the language model evaluation with use of perplexity." />

        <meta property="og:site_name" content="Michał Chromiak's blog" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="NLP: Explaining Neural Language Modeling"/>
        <meta property="og:url" content="/articles/2017/Nov/30/Explaining-Neural-Language-Modeling/"/>
        <meta property="og:description" content="Language modeling (LM) is the essential part of Natural Language Processing (NLP) tasks such as Machine Translation, Spell Correction Speech Recognition, Summarization, Question Answering, Sentiment analysis etc. Goal of the Language Model is to compute the probability of sentence considered as a word sequence. This article explains how to model the language using probability and n-grams. It also discuss the language model evaluation with use of perplexity."/>
            <meta property="og:image" content="/articles/2017/Nov/30/img/nlp-cover.png" />

        <meta property="article:published_time" content="2017-11-30" />
            <meta property="article:section" content="Sequence Models" />
            <meta property="article:tag" content="NLP" />
            <meta property="article:tag" content="language model" />
            <meta property="article:tag" content="ngram" />
            <meta property="article:tag" content="perplexity" />
            <meta property="article:tag" content="smoothing" />
            <meta property="article:author" content="Michał Chromiak" />


    <meta name="twitter:dnt" content="on">
    <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@drChromiak">
        <meta name="twitter:creator" content="@drChromiak">
    <meta name="twitter:domain" content="">
        <meta property="twitter:image"
              content="/articles/2017/Nov/30/img/nlp-cover.png"/>


    <!-- Bootstrap -->
        <link rel="stylesheet" href="/theme/css/bootstrap.cerulean.min.css" type="text/css"/>
    <link href="/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="/theme/css/pygments/colorful.css" rel="stylesheet">
    <link rel="stylesheet" href="/theme/css/style.css" type="text/css"/>
        <link href="/static_files/css/custom.css" rel="stylesheet">

        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Michał Chromiak's blog ATOM Feed"/>




</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="/" class="navbar-brand">
<img class="img-responsive pull-left gap-right" src="/static_files/img/sitelogo40.png" width=""/> Michał Chromiak's blog            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                         <li><a href="/pages/about.html">
                             About
                          </a></li>
                         <li><a href="/pages/categorisation.html">
                             Categorisation
                          </a></li>
                         <li><a href="/pages/contact-detail.html">
                             Contact detail
                          </a></li>
                         <li><a href="/pages/links.html">
                             Links
                          </a></li>
                         <li><a href="/pages/resources.html">
                             Resources
                          </a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
            <ol class="breadcrumb">
                <li><a href="" title="Michał Chromiak's blog"><i class="fa fa-home fa-lg"></i></a></li>
                <li><a href="/category/sequence-models.html" title="Sequence Models">Sequence Models</a></li>
                <li class="active">NLP: Explaining Neural Language Modeling</li>
            </ol>
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="/articles/2017/Nov/30/Explaining-Neural-Language-Modeling/"
                       rel="bookmark"
                       title="Permalink to NLP: Explaining Neural Language Modeling">
                        NLP: Explaining Neural Language Modeling
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2017-11-30T19:30:00+01:00"> Thu, 30 Nov 2017</time>
    </span>
          <span class="label label-default">Modified</span>
            <span class="modified">
                <i class="fa fa-calendar"></i><time datetime="2017-11-30T19:30:00+01:00"> Thu, 30 Nov 2017</time>
            </span>


            <span class="label label-default">By</span>
            <a href="/author/michal-chromiak.html"><i class="fa fa-user"></i> Michał Chromiak</a>

        <span class="label label-default">Category</span>
        <a href="/category/sequence-models.html">Sequence Models</a>


<span class="label label-default">Tags</span>
	<a href="/tag/nlp.html">NLP</a>
        /
	<a href="/tag/language-model.html">language model</a>
        /
	<a href="/tag/ngram.html">ngram</a>
        /
	<a href="/tag/perplexity.html">perplexity</a>
        /
	<a href="/tag/smoothing.html">smoothing</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
<img src="/articles/2017/Nov/30/img/nlp-cover.png" alt="article cover">
                <p><strong>Read before:</strong>
* <a href="https://medium.com/@gon.esbuyo/get-started-with-nlp-part-i-d67ca26cc828">Get started with NLP (Part I)</a></p>
<p><strong>Key Takeaways:</strong></p>
<ul>
<li>The goal of a language model is to compute a probability of a token (e.g. a sentence or a sequence of words).</li>
<li>
<p>Language Model (LM) actually a grammar of a language as it gives the probability of word that will follow</p>
</li>
<li>
<p>Dictionary:</p>
<ul>
<li><strong>Corpus</strong> - Body of text, singular. Corpora is the plural of this. Example: A collection of medical journals.</li>
<li><strong>Token</strong>- Each "entity" that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is "tokenized" into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph.</li>
<li><strong>Lexicon</strong> - Words and their meanings. Example: English dictionary. Consider, however, that various fields will have different lexicons. For example: To a financial investor, the first meaning for thef word "Bull" is someone who is confident about the market, as compared to the common English lexicon, where the first meaning for the word "Bull" is an animal. As such, there is a special lexicon for financial investors, doctors, children, mechanics, and so on.</li>
</ul>
</li>
</ul>
<h3 id="intro">Intro<a class="headerlink" href="#intro" title="Permanent link">🔗</a></h3>
<p><em>Language Modeling (LM)</em> is one of the most important parts of modern Natural Language Processing (NLP). There are many sorts of applications for  Language Modeling, like: Machine Translation, Spell Correction Speech Recognition, Summarization, Question Answering, Sentiment analysis etc. Each of those tasks require use of <em>language model</em>. Language model is required to represent the text to a form understandable from the machine point of view.</p>
<p>Below I have elaborated on the means to model a corpus of text to use in multiple machine learning NLP tasks.    </p>
<h3 id="motivation">Motivation<a class="headerlink" href="#motivation" title="Permanent link">🔗</a></h3>
<p>Each part of natural language has to assume some representation. Written text in natural way is composed out of sentences that consist of words that in turn contains letters. Each of those pieces of  text sentence can be a subject for analysis. Of course we can also use morphemes [ref]Small meaningful units that make up words. [\ref] or n-grams of the text.</p>
<p>One way or another, the text needs to be normalized.  Normalized mean that indexed text and query terms must have same form. For example, <em>U.S.A.</em> and <em>u.s.a.</em> are to be considered same (remove dots and case fold - lowercase) and return same result while querying any of them.  Thus, an implicit word classes equivalence must be defined.</p>
<h4 id="tokenizing">Tokenizing<a class="headerlink" href="#tokenizing" title="Permanent link">🔗</a></h4>
<p>Choosing which text segmentation (aka tokenizing) technique will be used is arbitrary, but depends strictly on the undergoing tasks, their specification, heuristics and preconditions. For instance, if we are after a single character statistics of text task we will probably choose to operate on letters not on n-grams or words. The result for text fragmentation is called <strong>token</strong>. So by referring to a token in general one actually refers to the fragmentation technique outcome in form of a sentence, word, n-gram, morpheme, letter etc.  </p>
<p>However, even word-oriented tokens might give a different set of distinct tokens. For instance in a lemma based tokens the <em>cat</em> and plural <em>cats</em> would count as one word having same stem (core meaning-bearing unit). On the other hand, if we make more strict distinctions, that would e.g. differentiate <em>word form</em>, we would get two distinct tokens. It all depends how you define a token.</p>
<p>To make things more clear, lets settle three terms:</p>
<ul>
<li><strong>vocabulary</strong> set of different types</li>
<li><strong>type</strong>  an element of vocabulary, and a</li>
<li><strong>token</strong> as an instance of a <em>type</em>.</li>
</ul>
<p>Additionally, language modeling must consider the correlated ordering of tokens. This is because every language is based on some grammar, where order has a lot of influence on the meaning of a text.  </p>
<p>To complete the NLP tasks we must provide a measure to enable comparison operations and thus assessment method for grading our model. This measure is probability. This involves all kinds of tasks, for example:</p>
<ul>
<li>
<p><strong>Machine translation:</strong> translating a sentence saying about height it would probably state that <span class="math">\( P(tall\ man) &gt; P(large\ man)\)</span> as the ‘<em>large</em>’ might also refer to weight or general appearance thus, not as probable as ‘<em>tall</em>’</p>
</li>
<li>
<p><strong>Spelling Correction:</strong>  Spell correcting sentence: “Put you name into form”, so that <span class="math">\(P(name\ into\ \textbf{form}) &gt; P(name\ into\ \textbf{from})\)</span></p>
</li>
<li>
<p><strong>Speech Recognition:</strong> Call my nurse: <span class="math">\(P(Call\ my\ nurse.) \gg P(coal\ miners)\)</span>, I have no idea. <span class="math">\(P(no\ idea.) \gg P(No\ eye\ deer.)\)</span></p>
</li>
<li><strong>Summarization, question answering, sentiment analysis</strong> etc.</li>
</ul>
<h3 id="the-probabilistic-language-modeling">The probabilistic language modeling<a class="headerlink" href="#the-probabilistic-language-modeling" title="Permanent link">🔗</a></h3>
<p>The probability of a sentence <span class="math">\(S\)</span> (as a sequence of <em>words</em> <span class="math">\(w_i\)</span>) is : <span class="math">\(P(S) = P(w_1,w_2, w_3,\ldots,w_n)\)</span>. Now it is important to find the probability of upcoming word. It is an everyday task made e.g. while typing  your mobile keyboard. We will settle the <em>conditional probability</em>  of <span class="math">\(w_4\)</span> depending on all previous words. For a 4 word sentence this conditional probability is:</p>
<div class="math">$$ P(S)=P(w_1, w_2, w_3, w_4) \equiv P(w_4 |w_1, w_2, w3) $$</div>
<p>This way we can actually represent the language grammar, however, in NLP it is accustomed to use the LM term.</p>
<p>To calculate the joint probability  of a sentence (as a word sequence) <span class="math">\(P(W)\)</span> the chain rule of probability will be used.</p>
<p><strong>Probability Chain Rule:</strong></p>
<div class="math">$$P(A|B) = \frac{P(A\cap B)}{P(B)} \implies P(A\cap B)=P(A|B)P(B)$$</div>
<p>so in general for a token sequence we get:</p>
<div class="math">$$P(S)=P(w_1,\ldots,w_n) = P(w_1)P(w_2|w_1) P(w_3)P(w_1,w_2)\ldots P(w_n|w_1,\ldots w_{n-1}) ={\displaystyle \prod_{i} P(w_i|w_1,\ldots w_{i-1})}$$</div>
<p>To estimate each probability a straightforward solution could be to use simple counting.</p>
<div class="math">$$P(w_5|w_1,w_2,w_3,w_4)= \frac{count(w_1,w_2,w_3,w_4,w_5)}{count(w_1,w_2,w_3,w_4)}$$</div>
<p>but this gives us to many possible sequences to ever estimate. Imagine how much data (occurrences of each sentence) we would have to get to make this <em>count</em>s meaningful.</p>
<p>To cope with this issue we can simplify by applying the <strong>Markov Assumption</strong>, which states that it is enough to pick only one, or a couple of previous words as a prefix:</p>
<div class="math">$$P(w_1,\ldots,w_n) \approx {\displaystyle \prod_{i} P(w_i|w_{i-k},\ldots P(w_{i-1}))}$$</div>
<p>where <span class="math">\(k\)</span> is the number of previous tokens (<em>prefix</em> size) that we consider.</p>
<h3 id="n-grams">N-grams<a class="headerlink" href="#n-grams" title="Permanent link">🔗</a></h3>
<p>An N-gram is a contiguous (order matters) sequence of items, which in this case is the words in text. The n-grams depends on the size of the <em>prefix</em>. The simplest case is the <em>Unigram mode</em>.</p>
<p><strong> (Uni-) 1-gram model</strong>
The simplest case of <em>Markov assumption</em> is case when the size of prefix is one.
<span class="math">\(P(w_1,\ldots,w_n) \approx {\displaystyle \prod_{i} P(w_i)}\)</span></p>
<p>This will provide us with grammar that only consider one word. As a result it produces a set of unrelated words. It actually would generate sentences with random word order.</p>
<p><strong> Bigram </strong>
However, if we consider a 2-word (tandem) bigrams correlations where we condition each word on previous one we get some sens of meaning between couples of words.</p>
<div class="math">$$P(w_1,\ldots,w_n) \approx {\displaystyle \prod_{i} P(w_i|w_{i-1})}$$</div>
<p>This way we get a sequence of tandems that have meaning tandem-wise. This still is not enough to face a long range dependencies from natural languages, like English. This would be difficult even in case of n-grams as there can be very long sentences with dependencies. However, a 3-gram can get us a pretty nice <a href="http://tetration.xyz/Ngram-Tutorial/">approximation</a>.</p>
<div class="highlight"><pre><span></span><code>and how he probably more easily believe me i never see how much as that he and pride and unshackled
</code></pre></div>

<h4 id="estimate-n-gram-probabilities">Estimate n-gram probabilities<a class="headerlink" href="#estimate-n-gram-probabilities" title="Permanent link">🔗</a></h4>
<p>Estimation can be done with <strong>Maximum Likelihood Estimate (MLE)</strong>:</p>
<div class="math">$$P(w_i|w_{i-1})=\frac{count(w_{i-1},w_i)}{count(w_{i-1 })}$$</div>
<!-- z jupytera:
Z google ngram raw bigram count table
normalize by unigrams -->

<p>So the 2-gram estimate  of sentence probability would be the product of all component tandems ordered as in the sentence.</p>
<div class="math">$$P(w_1,\ldots,w_n) \approx {\displaystyle \prod_{i} P(w_i|w_{i-1})}$$</div>
<p>In practice, the outcome should be represented in <em>log</em> form. There are two reasons for this. Firstly, if the sentence is long and the probabilities are really small, then such product might end in arithmetic underflow.  Secondly, adding is faster - and when we use logarithm we know that: <span class="math">\(log(a*b) = log(a)+log(b)\)</span>, thus:</p>
<p><span class="math">\(log(P(w_1,\ldots,w_n)) \approx {\displaystyle \sum_{i} log(P(w_i|w_{i-1}))}\)</span></p>
<p>This is why the Language Model is stored in logarithmic values.</p>
<h3 id="publicly-available-corpora">Publicly available corpora<a class="headerlink" href="#publicly-available-corpora" title="Permanent link">🔗</a></h3>
<p>There is available <a href="http://www.gutenberg.org/ebooks/search/">Gutenberg Project</a> providing with text format of some books.</p>
<p>Google also released a publicly available corpus trillion word corpus with over 13 million unique words. It is nice corpus from Google Books with already implemented <a href="https://books.google.com/ngrams">N-Gram viewer</a> that enables to plot word counts. It includes number of corpora for multiple languages. See the below chart an example of unigram, 2-gram and 3-gram occurrences.</p>
<iframe name="ngram_chart" src="https://books.google.com/ngrams/interactive_chart?content=machine+learning%2Cdeep+learning%2Cnlp%2Cnatural+language+processing&year_start=1982&year_end=2008&corpus=0&smoothing=3&share=&direct_url=t1%3B%2Cmachine%20learning%3B%2Cc0%3B.t1%3B%2Cdeep%20learning%3B%2Cc0%3B.t1%3B%2Cnlp%3B%2Cc0%3B.t1%3B%2Cnatural%20language%20processing%3B%2Cc0" width=860 height=300 marginwidth=0 marginheight=0 hspace=0 vspace=0 frameborder=0 scrolling=no></iframe>

<p>Such a large scale text analysis can be done by downloading <a href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html">each dataset</a> using the <a href="https://books.google.com/ngrams/info">Ngram Viewer API</a>.</p>
<p>I have created a project to collect and analyze those datasets locally with custom configuration. See the <a href="https://github.com/MChromiak/MC_Utils">GitHub repo</a> for the source code.</p>
<h3 id="language-model-evaluation-perplexity">Language Model evaluation – Perplexity<a class="headerlink" href="#language-model-evaluation-perplexity" title="Permanent link">🔗</a></h3>
<p>The good LM should calculate higher probabilities to “real” and “frequently observed” sentences than the ones that are wrong accordingly to natural language grammar or those that are rarely observed.</p>
<p>To make such assumption, we first train the language model on one set and test it on completely new dataset. Then we can compare the results of our model on the new dataset and evaluate, how good (how accurate in terms of a task e.g. translation, spell check etc) <strong>on average</strong> it works on this new, previously unseen dataset. For instance how many words it translates correctly. This way one can compare and decide which language model fits the best to a task. This is called <em>extrinsic evaluation</em>.</p>
<p>The extrinsic evaluation approach however, requires multiple tests on models which are expensive. An alternative is the <em>intrinsic evaluation</em> which is about testing the LM itself not some particular task or application. The popular intrinsic evaluation is <strong>perplexity</strong>.</p>
<p>As <strong>perplexity</strong> is a bad approximation to an extreme extrinsic evaluation, in cases where the test dataset does NOT look just like the training set. Thus, it is useful only at the early stages of experiment. So later in experiment extrinsic evaluation should also be used.</p>
<p>Best model is the one that best predicts an unseen test set, or assigns on average the probability to all sentences that is sees.</p>
<p>Perprexity metric is the probability of the test set, normalized for the length of the probability by the number of words.</p>
<div class="math">$$ PP(S) = P(w_1,\ldots, w_N)^{-\frac{1}{N}} =\sqrt[\leftroot{0}\uproot{3}N]{\frac{1}{P(w_1,\ldots, w_N)}} = $$</div>
<p>This way the longer the sentence the less probable it will be. Then again by the chain rule:</p>
<div class="math">$$ = \sqrt[N]{{\displaystyle \prod_{i}^{N}\frac{1}{P(w_i|w_1,\ldots, w_{i-1})}}} $$</div>
<p>In case of 2-gram using Markov approximation of the chain rule:</p>
<div class="math">$$ PP(S) =  \sqrt[N]{{\displaystyle \prod_{i}^{N}\frac{1}{P(w_i|w_{i-1})}}}$$</div>
<p>So <em>perplexity</em> is a function of probability of the sentence. The meaning of the inversion in perplexity means that whenever we minimize the perplexity we maximize the probability.</p>
<p>Perplexity is weighted equivalent branching factor.</p>
<p>To be continued...</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </div>
            <!-- /.entry-content -->
<section class="well" id="related-posts">
    <h4>Related Posts:</h4>
    <ul>
        <li><a href="/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/">The Transformer – Attention is all you need.</a></li>
    </ul>
</section>
    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'mchromiak'; // required: replace example with your forum shortname

                var disqus_url = '/articles/2017/Nov/30/Explaining-Neural-Language-Modeling/';

            var disqus_config = function () {
                this.language = "en";
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<div id="aboutme">
        <p>
            <img width="100%" class="img-thumbnail" src="/static_files/img/me.png"/>
        </p>
    <p>
      <strong>About Michał Chromiak</strong><br/>
        Completed his PhD in 2016 by Polish Academy of Sciences (PAS) in Computer Science. Focus research on understanding chaos of data. Deeply understanding the phenomena makes it easy, but first you need to learn. Holds two MScs, in Mathematics and in Computer Science.
    </p>
</div><!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://www.linkedin.com/in/michal-chromiak"><i class="fa fa-linkedin-square fa-lg"></i> LinkedIn</a></li>
    <li class="list-group-item"><a href="https://github.com/MichalChromiak"><i class="fa fa-github-square fa-lg"></i> GitHub</a></li>
    <li class="list-group-item"><a href="https://twitter.com/drChromiak"><i class="fa fa-twitter-square fa-lg"></i> Twitter</a></li>
    <li class="list-group-item"><a href="https://www.researchgate.net/profile/Michal_Chromiak"><i class="fa fa-researchgate-square fa-lg"></i> ResearchGate</a></li>
    <li class="list-group-item"><a href="https://scholar.google.pl/citations?user=UeOad3YAAAAJ&hl=en"><i class="fa fa-google-scholar-square fa-lg"></i> Google Scholar</a></li>
    <li class="list-group-item"><a href="localhost:8000/feeds/all.rss"><i class="fa fa-rss-square fa-lg"></i> RSS</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Recent Posts -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Recent Posts</span></h4>
  <ul class="list-group" id="recentposts">
    <li class="list-group-item"><a href="/articles/2021/May/05/MLP-Mixer/">MLP is all you need... again? ... MLP-Mixer: An all-MLP Architecture for Vision</a></li>
    <li class="list-group-item"><a href="/articles/2019/Jul/30/ernie-2-0/">ERNIE 2.0: A continual pre-training framework for language understanding</a></li>
    <li class="list-group-item"><a href="/articles/2017/Nov/30/Explaining-Neural-Language-Modeling/">NLP: Explaining Neural Language Modeling</a></li>
  </ul>
</li>
<!-- End Sidebar/Recent Posts -->

<!-- Sidebar/Categories -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Categories</span></h4>
  <ul class="list-group" id="categories">
    <li class="list-group-item">
      <a href="/category/applications.html"><i class="fa fa-folder-open fa-lg"></i>Applications</a>
    </li>
    <li class="list-group-item">
      <a href="/category/computer-vision.html"><i class="fa fa-folder-open fa-lg"></i>Computer Vision</a>
    </li>
    <li class="list-group-item">
      <a href="/category/ml-dojo.html"><i class="fa fa-folder-open fa-lg"></i>ML Dojo</a>
    </li>
    <li class="list-group-item">
      <a href="/category/sequence-models.html"><i class="fa fa-folder-open fa-lg"></i>Sequence Models</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Categories -->

<!-- Sidebar/Tag Cloud -->
<li class="list-group-item">
  <a href="/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
  <ul class="list-group list-inline tagcloud" id="tags">
    <li class="list-group-item tag-4">
      <a href="/tag/application.html">application</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/attention-model.html">Attention model</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/basics.html">basics</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/bert.html">BERT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/cv.html">CV</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/elmo.html">ELMo</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/ernie-10.html">ERNIE 1.0</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/language-model.html">language model</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/machine-translation.html">Machine translation</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/mlp.html">MLP</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/ngram.html">ngram</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/nlp.html">NLP</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/nmt.html">NMT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/openai-gpt.html">OpenAI GPT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/patternrecognition.html">PatternRecognition</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/perplexity.html">perplexity</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/seq2seq.html">seq2seq</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/sequence-transduction.html">Sequence transduction</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/smoothing.html">smoothing</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/transformer.html">Transformer</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/vit.html">ViT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/xlnet.html">XLNet</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Tag Cloud -->

<!-- Sidebar/Links -->
<li class="list-group-item">
  <h4><i class="fa fa-external-link-square fa-lg"></i><span class="icon-label">Links</span></h4>
  <ul class="list-group" id="links">
    <li class="list-group-item">
      <a href="http://www.iclr.cc" target="_blank">ICLR Conf</a>
    </li>
    <li class="list-group-item">
      <a href="http://icml.cc" target="_blank">ICML Conf</a>
    </li>
    <li class="list-group-item">
      <a href="https://nips.cc/" target="_blank">NIPS Conf</a>
    </li>
    <li class="list-group-item">
      <a href="http://aifrontiers.com/" target="_blank">AI Frontiers</a>
    </li>
    <li class="list-group-item">
      <a href="https://developers.google.com/machine-learning/glossary/" target="_blank">ML Glossary</a>
    </li>
    <li class="list-group-item">
      <a href="https://deepdreamgenerator.com/" target="_blank">Deep Dream Generator</a>
    </li>
    <li class="list-group-item">
      <a href="https://deepart.io/" target="_blank">DeepArt Generator</a>
    </li>
    <li class="list-group-item">
      <a href="https://stanfordmlgroup.github.io/" target="_blank">Stanford ML Group Andrew Ng</a>
    </li>
    <li class="list-group-item">
      <a href="https://ai-on.org/" target="_blank">AI•ON open ML collaboration</a>
    </li>
    <li class="list-group-item">
      <a href="http://java-hive.blogspot.com/" target="_blank">My old blog on Java an SE</a>
    </li>
    <li class="list-group-item">
      <a href="http://karpathy.github.io/2016/09/07/phd/" target="_blank">PhD</a>
    </li>
    <li class="list-group-item">
      <a href="http://alt.qcri.org/semeval2017/" target="_blank">SemEval2017</a>
    </li>
    <li class="list-group-item">
      <a href="https://medium.freecodecamp.org/450-free-online-programming-computer-science-courses-you-can-start-in-september-59712e77635c" target="_blank">Free CS courses</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Links -->

<!-- Sidebar/Archive -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Archive</span></h4>
  <ul class="list-group" id="archive">
        <li class="list-group-item">
          <a href="/archive/2021/May/index.html"><i class="fa fa-calendar fa-lg"></i>May 2021 (1)
          </a>
        </li>
        <li class="list-group-item">
          <a href="/archive/2019/Jul/index.html"><i class="fa fa-calendar fa-lg"></i>July 2019 (1)
          </a>
        </li>
        <li class="list-group-item">
          <a href="/archive/2017/Nov/index.html"><i class="fa fa-calendar fa-lg"></i>November 2017 (1)
          </a>
        </li>
        <li class="list-group-item">
          <a href="/archive/2017/Sep/index.html"><i class="fa fa-calendar fa-lg"></i>September 2017 (2)
          </a>
        </li>
        <li class="list-group-item">
          <a href="/archive/2017/Aug/index.html"><i class="fa fa-calendar fa-lg"></i>August 2017 (1)
          </a>
        </li>
  </ul>
</li>
<!-- End Sidebar/Archive -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2021 Michał Chromiak
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>                <p><small>  <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en"><img alt="Creative Commons License" style="border-width:0" src="//i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
    Content
  licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">Creative Commons Attribution-ShareAlike 4.0 International License</a>, except where indicated otherwise.
</small></p>
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="/theme/js/respond.min.js"></script>

    <script src="/static_files/js/custom.js"></script>

    <script src="/theme/js/bodypadding.js"></script>
    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'mchromiak'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics Universal -->
    <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-108394162-1', 'auto');
        ga('send', 'pageview');
    </script>
    <!-- End Google Analytics Universal Code -->


        <script type="text/javascript">var addthis_config = {"data_track_addressbar": true};</script>
    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-59ea3c17b283c631"></script>
    <script src="/static_files/js/article.js"></script>
</body>
</html>