Title: Long-Short Time Memory NN
Status: hidden
Category: Neural Nets
Date: 2017-09-03 19:30
Modified: 2017-09-03 19:30
Tags: memory, LSTM
Slug: LSTM-101
Related_posts: slug1
Summary: The way to embrace sequences is when you go milti-layered of perceptron with  many hidden layers. $f(x)$
You need to know:

### Parallel is not enough
In recent years due to progress on hardware level of CPUs and what is more important GPUs we are able to apply more ML algorithms using the power of parallelism. Human brain can accurately solve extremely complex task in a fraction of a second. Because our neurons are rather slow (due to its spiking nature [ref]More speed requires more energy and produces more heat. The human brain (less than 2% of body weight for most people) already consumes 20% or more of your rest calories and produces as much heat as a 25 watt bulb.[/ref]) one can assume that the number of steps that bran makes is rather low. Instead the steps are probably heavily parallel.

This is where the deep neural network can bring all of its goodness. Combined with massive amount of input data algorithms can complete complex tasks. Neural networks contains input layer, $N$ hidden layers (containing hidden units or nodes; inspired by *neurons*) and the output layer.

#### So here is where the Neural Networks comes in.
Neural networks contains input layer, $N$ hidden layers (containing hidden units or nodes; inspired by *neurons*) and the output layer.

The Feedforward NN (FNN)  (a.k.a. multi-layer perceptron (MLP)) name comes form the fact that nodes’ structure is without cycles, meaning that it can not be represented as a Direct Acyclic Graph (DAG).

The general goal of FNN is to approximate a function $f(x)$, based only on its noisy set of evaluations in selected points  ($x_i$). So the FNN actually is a hypothesis function $h(x; \theta)$, where $\theta$ is a set of trainable parameters (a.k.a. *weights*; sometimes also referred to as the **model** [ref] A collection of weights, whether they are in their start or end state, is also called a model, because it is an attempt to model data’s relationship to ground-truth labels, to grasp the data’s structure. [\ref]) of the network. The goal is to train $\theta$ weights for given data ($x_i$) so that the *cost* (a.k.a. *loss*) between $f$ and $h$ is minimal.

See more on Neural Nets [DL4J Tutorial](https://deeplearning4j.org/neuralnet-overview.html), [DeepLearning Book](http://www.deeplearningbook.org/contents/rnn.html)

FNN however has no notion of time, and are amnesic about the previous examples that were used in the past while analyzing current example.

### A memory, a memory….

What appears to be the clue about  parallel processing (including deep neural networks) is the **sequential nature**. The **depth** in *deep learning* actually represents the sequential nature.
