<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>RL Primer - Micha≈Ç Chromiak's blog</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">


    <link href="https://mchromiak.github.io/static_files/img/favicon.jpg" rel="icon">

<link rel="canonical" href="https://mchromiak.github.io/articles/2021/May/01/RL-Primer/">

        <meta name="author" content="Micha≈Ç Chromiak" />
        <meta name="keywords" content="Reinforcement Learning,RL,MDP,Markov Decision Process" />
        <meta name="description" content="The objective of RL is to maximize the reward of an agent by taking a series of actions in response to a dynamic environment. Breaking it down, the process of Reinforcement Learning involves these simple steps: Observation of the environment, deciding how to act using some strategy, acting accordingly" />

        <meta property="og:site_name" content="Micha≈Ç Chromiak's blog" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="RL Primer"/>
        <meta property="og:url" content="https://mchromiak.github.io/articles/2021/May/01/RL-Primer/"/>
        <meta property="og:description" content="The objective of RL is to maximize the reward of an agent by taking a series of actions in response to a dynamic environment. Breaking it down, the process of Reinforcement Learning involves these simple steps: Observation of the environment, deciding how to act using some strategy, acting accordingly"/>
            <meta property="og:image" content="https://mchromiak.github.io/articles/2021/May/01/img/RL-primer-Cover.jpg" />

        <meta property="article:published_time" content="2021-05-01" />
            <meta property="article:section" content="Reinforcement learning" />
            <meta property="article:tag" content="Reinforcement Learning" />
            <meta property="article:tag" content="RL" />
            <meta property="article:tag" content="MDP" />
            <meta property="article:tag" content="Markov Decision Process" />
            <meta property="article:author" content="Micha≈Ç Chromiak" />


    <meta name="twitter:dnt" content="on">
    <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@drChromiak">
        <meta name="twitter:creator" content="@drChromiak">
    <meta name="twitter:domain" content="https://mchromiak.github.io">
        <meta property="twitter:image"
              content="https://mchromiak.github.io/articles/2021/May/01/img/RL-primer-Cover.jpg"/>


    <!-- Bootstrap -->
        <link rel="stylesheet" href="https://mchromiak.github.io/theme/css/bootstrap.cerulean.min.css" type="text/css"/>
    <link href="https://mchromiak.github.io/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="https://mchromiak.github.io/theme/css/pygments/colorful.css" rel="stylesheet">
    <link rel="stylesheet" href="https://mchromiak.github.io/theme/css/style.css" type="text/css"/>
        <link href="https://mchromiak.github.io/static_files/css/custom.css" rel="stylesheet">

        <link href="https://mchromiak.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Micha≈Ç Chromiak's blog ATOM Feed"/>



        <link href="https://mchromiak.github.io/feeds/reinforcement-learning.atom.xml" type="application/atom+xml" rel="alternate"
              title="Micha≈Ç Chromiak's blog Reinforcement learning ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://mchromiak.github.io/" class="navbar-brand">
<img class="img-responsive pull-left gap-right" src="https://mchromiak.github.io/static_files/img/sitelogo40.png" width=""/> Micha≈Ç Chromiak's blog            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                         <li><a href="https://mchromiak.github.io/pages/about.html">
                             About
                          </a></li>
                         <li><a href="https://mchromiak.github.io/pages/categorisation.html">
                             Categorisation
                          </a></li>
                         <li><a href="https://mchromiak.github.io/pages/contact-detail.html">
                             Contact detail
                          </a></li>
                         <li><a href="https://mchromiak.github.io/pages/links.html">
                             Links
                          </a></li>
                         <li><a href="https://mchromiak.github.io/pages/resources.html">
                             Resources
                          </a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
            <ol class="breadcrumb">
                <li><a href="https://mchromiak.github.io" title="Micha≈Ç Chromiak's blog"><i class="fa fa-home fa-lg"></i></a></li>
                <li><a href="https://mchromiak.github.io/category/reinforcement-learning.html" title="Reinforcement learning">Reinforcement learning</a></li>
                <li class="active">RL Primer</li>
            </ol>
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://mchromiak.github.io/articles/2021/May/01/RL-Primer/"
                       rel="bookmark"
                       title="Permalink to RL Primer">
                        RL Primer
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2021-05-01T19:30:00+02:00"> Sat, 01 May 2021</time>
    </span>
          <span class="label label-default">Modified</span>
            <span class="modified">
                <i class="fa fa-calendar"></i><time datetime="2021-05-01T19:30:00+02:00"> Sat, 01 May 2021</time>
            </span>


            <span class="label label-default">By</span>
            <a href="https://mchromiak.github.io/author/michal-chromiak.html"><i class="fa fa-user"></i> Micha≈Ç Chromiak</a>

        <span class="label label-default">Category</span>
        <a href="https://mchromiak.github.io/category/reinforcement-learning.html">Reinforcement learning</a>


<span class="label label-default">Tags</span>
	<a href="https://mchromiak.github.io/tag/reinforcement-learning.html">Reinforcement Learning</a>
        /
	<a href="https://mchromiak.github.io/tag/rl.html">RL</a>
        /
	<a href="https://mchromiak.github.io/tag/mdp.html">MDP</a>
        /
	<a href="https://mchromiak.github.io/tag/markov-decision-process.html">Markov Decision Process</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>Receiving a reward or penalty, learning from the experiences and refining our strategy, iterate until an optimal strategy is found. RL by itself is quite complex area of AI and by this requires some terms to be explained.</p>
<h3 id="prerequisite-rl-knowledge">Prerequisite RL knowledge<a class="headerlink" href="#prerequisite-rl-knowledge" title="Permanent link">üîó</a></h3>
<p>Here are explained some of the main terms that this paper uses to motivate its goal. Form more details please refer to free online book from <a href="http://incompleteideas.net/book/the-book.html">Richard S. Sutton
and Andrew G. Barto Second <em>Reinforcement Learning: An Introduction</em> Edition MIT Press, Cambridge, MA, 2018</a></p>
<h4 id="what-is-model-free-rl">What is <em>Model-free</em> RL?<a class="headerlink" href="#what-is-model-free-rl" title="Permanent link">üîó</a></h4>
<p>A ‚Äúpolicy‚Äù, is a strategy that an agent uses to pursue a goal.</p>
<blockquote>
<p><strong>TL;DR</strong></p>
<p>Model-free algorithm:
estimates the optimal policy without using or estimating the dynamics (transition and reward functions) of the environment.</p>
<p>Model-based algorithm:
uses the transition function (and the reward function) in order to estimate the optimal policy.</p>
</blockquote>
<p>In an <strong>online RL</strong> there is an <em>Agent</em> and the <em>Environment</em>. Agent <span class="math">\(A\)</span> performs actions <span class="math">\(a\)</span> in environment <span class="math">\(E\)</span>, and the <span class="math">\(E\)</span> response with the <em>reward</em> <span class="math">\(r\)</span> and <em>observation</em> <span class="math">\(o\)</span> (or a <em>state</em> <span class="math">\(s\)</span> if it is not partially observable environment). Additionally a <em>policy</em> (network) is the algorithm used by the agent to decide its actions. This is the part that can be <em>model-based</em> or <em>model-free</em>. Hence, agents gets to actively (interactively) interact with environment to maximize the reward with policy.</p>
<p>Policy can be model-based or model-free. Question is how to optimize the policy network via <em>policy gradient</em> (PG)?</p>
<p>PG algorithms directly try to optimize the policy to increase rewards, however this directly needs correction from environment in online fashion.</p>
<h4 id="markov-decision-processes-mdp">Markov decision processes (MDP).<a class="headerlink" href="#markov-decision-processes-mdp" title="Permanent link">üîó</a></h4>
<p>MDP is a process with a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from state A to state B is fixed.</p>
<h4 id="what-is-offline-rl">What is <em>Offline</em> RL?<a class="headerlink" href="#what-is-offline-rl" title="Permanent link">üîó</a></h4>
<p>In <strong>offline</strong> RL there is an agent and (instead of interactive environment) a limited, fixed dataset. The dataset contains experience (trajectory rollouts of arbitrary policies) from other scenarios where agents were learning a <em>policy</em> to gain a good <em>reward</em>. In contrast to online RL, such setup is more challenging as there is no dynamic environment to test hypothesis, and all is left is to have a set of <em>trajectories</em> without live feedback. By observing historical episodes of interaction from other agents, an offline agent needs to learn a good policy to achieve a high reward.</p>
<h4 id="reward-return">Reward /return<a class="headerlink" href="#reward-return" title="Permanent link">üîó</a></h4>
<p><strong>Reward</strong>
Reward is the quantity received from the environment in a given timestep as a result of an action</p>
<p><strong>Return</strong>:
Return is defined as a function of reward sequence, which can be:</p>
<ul>
<li>simple sum of rewards (also called cumulative reward), or a</li>
<li>sum of <em>discounted</em> rewards (also called <em>cumulative future discounted reward</em>):</li>
</ul>
<p><a name="tdr">
</p>
<div class="math">$$R_t=r_{t+1}+\gamma r_{t+2}+ \gamma^2 r_{r+3}+\ldots = \sum\limits_{k=0}^{\infty}\gamma^kr_{t+k+1}, \gamma \in [0,1]$$</div>
<p></a></p>
<ul>
<li>"Cumulative" refers to the summation.</li>
<li>"Future" refers to the fact that it's an expected value of all the future timesteps values until the end of the episode with respect to the present quantity.</li>
<li>"Discounted" refers to the "gamma" <span class="math">\(\gamma\)</span> <em>discount rate</em> factor, which is a way to adjust the importance of how much we value rewards at future time steps, i.e. starting from <span class="math">\(t+1\)</span>.</li>
<li>"Reward" refers to the main quantity of interest, i.e. the reward received from the environment.</li>
</ul>
<p>The goal of an RL algorithm is to select actions that maximize the expected cumulative reward (the return) of the agent.</p>
<h4 id="what-is-credit-assignment">What is <em>Credit Assignment</em>?<a class="headerlink" href="#what-is-credit-assignment" title="Permanent link">üîó</a></h4>
<p>In real world RL the state (and action) space usually needs to be very fine to cover all possibly relevant situations. This leads to the combinatorial explosion of states and actions which is referred as <em>curse of dimensionality</em>.
As an agent interacts with an environment in discrete timesteps, agent takes an action for current state and the environment emits a perception in form of a <em>reward</em> and an <em>observation</em>. In case of fully observable Markov Decision Process (MDP) it is the next state (of the environment and the agents). Agent's goal is to maximize the reward. In such <strong>fine grained</strong> state-action spaces the reward occur terribly temporally delayed.</p>
<p>The (temporal) <em>credit assignment problem</em> (CAP) is the problem of determining the actions that lead to a certain outcome. The problem of determining the contribution of each agent to the result of the training is the (temporal) CAP. In order to maximize the reward in the long run, the agent needs to determine which actions will lead to such outcome, which is essentially the temporal CAP. This way, an action that leads to a higher final cumulative reward should have more <em>credit</em> (value) than an action that lead to a lower final reward. For instance, in Q-learning (the <em>off-policy</em> algorithm) agents attempts to determine actions that will lead to the highest value in each state.</p>
<p>In RL, due to CAP, reward signals will only very weakly affect all temporally distant states that have preceded it. The influence of a reward gets more and more diluted over time and this can lead to bad convergence properties of the RL mechanism. Many steps must be performed by an iterative RL algorithm to propagate the influence of delayed reinforcement to all states and actions that have an effect on that reinforcement.</p>
<h4 id="value-functions">Value functions<a class="headerlink" href="#value-functions" title="Permanent link">üîó</a></h4>
<p>Q-Learning is about learning Q-function that takes state and action conditioned on the history to predict future rewards.
VF are state-action pair functions that estimate how good a particular action will be in a given state, or what the return for that action is expected to be.</p>
<ul>
<li><strong>V-function</strong> (State-Value) <span class="math">\(v^\pi (s)=  \mathbb{E_\pi} [\sum_{k=0}^T \gamma^k R_{t+k+1} | S_t = s]\)</span> <br > <strong>Value</strong>   of state <span class="math">\(s\)</span> under policy/strategy <span class="math">\(\pi\)</span>. The expected <strong>return</strong> while starting at <span class="math">\(s\)</span> and following the <span class="math">\(\pi\)</span> thereafter. Shows how good a certain state is, in terms of expected cumulative reward, for an agent following a certain policy. The <span class="math">\(\mathbb{E}[.]\)</span> because environment state transition function might act in a stochastic way.</li>
<li><strong>Q-function</strong>  (State-Action) <span class="math">\(q^\pi (s,a) = \mathbb{E_\pi}[\sum_{k=0}^T \gamma^k R_{t+k+1} | S_t = s, A_t = a]\)</span>  <br ><strong>Quality</strong>  of taking action <span class="math">\(a\)</span> in state <span class="math">\(s\)</span> with policy/strategy <span class="math">\(\pi\)</span>.  The expected <strong>return</strong> while starting at <span class="math">\(s\)</span> while taking action <span class="math">\(a\)</span> and following the <span class="math">\(\pi\)</span> thereafter. Shows how good action <span class="math">\(a\)</span> is, given a state for agent following a policy. The <span class="math">\(\mathbb{E}[.]\)</span> because environment state transition function might act in a stochastic way.</li>
<li><strong>Q-Value</strong> - value in state-action table.  The Q-function is implemented as a table of states and actions and Q-values for each s,a pair are stored there.</li>
</ul>
<p><strong>Estimating</strong> VF for a particular policy, helps to accurately choose an action that will provide the best total reward possible, after being in that given state.</p>
<h4 id="temporal-difference-td-learning">Temporal Difference (TD) Learning<a class="headerlink" href="#temporal-difference-td-learning" title="Permanent link">üîó</a></h4>
<p><em>Temporal Difference</em> (TD) (aka <em>bootstrapping</em> method) solves the problem of <strong>estimating</strong> <a href="#value-functions">value function</a>. If the value functions were to be calculated <strong>without estimation</strong>, the agent would need to wait until the final reward was received before any state-action pair values can be updated. Once the final reward was received, the path taken to reach the final state would need to be traced back and each value updated accordingly. TD address this issue.</p>
<p>TD learning is a unsupervised, RL model-free method learning by bootstrapping from current estimate of value function. In TD, agent is learning from an environment through episodes with no prior knowledge of the environment. TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known.</p>
<p>Instead of calculating the total future reward, at each step, TD tries to predict the combination of immediate reward and its own reward prediction at the next moment in time.</p>
<p>TD method is called a "bootstrapping" method, because the value is updated partly using an existing estimate and not a final reward.</p>
<h4 id="onoff-policy-algorithm">On/Off-policy algorithm<a class="headerlink" href="#onoff-policy-algorithm" title="Permanent link">üîó</a></h4>
<p>Off-policy algorithm - evaluate and improve a (target) policy that is different from (current) policy which is used for action selection. When passing the reward from the next state to the current state, it takes the maximum possible reward of the new state and ignores whatever policy we are using. Eg. Q-learning is off-policy as it updates its Q-values using the Q-value of the next state and the <em>greedy</em> action. In other words, it estimates the <strong>return</strong> (cumulative/total  <a href="#tdr">discounted return</a> future reward, starting from current timestep) for state-action pairs assuming a greedy policy were followed, despite the fact that it's not following a greedy policy.</p>
<p>On-policy algorithm - evaluate and improve the same policy which is being used to select actions, Eg. Sarsa  updates its Q-values using the Q-value of the next state and the current policy's action. It estimates the return for state-action pairs assuming the current policy continues to be followed.</p>
<h4 id="action-selection-policies">Action Selection Policies<a class="headerlink" href="#action-selection-policies" title="Permanent link">üîó</a></h4>
<p>There are three common policies used for action selection. The aim of these policies is to balance the trade-off between <strong>exploitation</strong> and <strong>exploration</strong>, by not always exploiting what has been learned so far.</p>
<ul>
<li>greedy - Will lock on one action that happened to have good results at one point of time but it is not in reality the optimal action. So Greedy will keep exploiting this action while ignoring the others which might be better. It Exploits too much.</li>
<li><span class="math">\(\epsilon\)</span>-greedy - most of the time the action with the highest estimated reward is chosen, called the greediest action. Every once in a while, say with a small probability <span class="math">\(\epsilon\)</span>, an action is selected at random. The action is selected uniformly, independently of the action-value estimates. This method ensures that if enough trials are done, each action will be tried an infinite number of times, thus ensuring optimal actions are discovered. Explores too much because even when one action seem to be the optimal one, the methods keeps allocating a fixed percentage of the time for exploration, thus missing opportunities and increasing total regret.</li>
<li><span class="math">\(\epsilon\)</span>-soft - very similar to <span class="math">\(\epsilon\)</span>-greedy. The best action is selected with probability <span class="math">\(1 - \epsilon\)</span> and the rest of the time a random action is chosen uniformly.</li>
<li>softmax - one drawback of <span class="math">\(\epsilon\)</span>-greedy and <span class="math">\(\epsilon\)</span>-soft is that they select random actions uniformly. The worst possible action is just as likely to be selected as the second best. Softmax remedies this by assigning a rank, or weight to each of the actions, according to their action-value estimate. A random action is selected with regards to the weight associated with each action, meaning the worst actions are unlikely to be chosen. This is a good approach to take where the worst actions are very unfavorable.</li>
</ul>
<p>It is not clear which of these policies produces the best results overall. The nature of the task will have some bearing on how well each policy influences learning. If the problem we are trying to solve is of a game playing nature, against a human opponent, human factors may also be influential.</p>
<h4 id="exploitation-vs-exploration">Expl<strong>oit</strong>ation vs explo<strong>r</strong>ation<a class="headerlink" href="#exploitation-vs-exploration" title="Permanent link">üîó</a></h4>
<p>Exploitation - keep the current approach. Chooses the greedy action to get the most reward by exploiting the agent‚Äôs current action-value estimates. But by being greedy with respect to action-value estimates, may not actually get the most reward and lead to sub-optimal behaviour.</p>
<p>Exploration - Try new approach.  Allows an agent to improve its current knowledge about each action, hopefully leading to long-term benefit. Improving the accuracy of the estimated action-values, enables an agent to make more informed decisions in the future.</p>
<p>When an agent explores, it gets more accurate estimates of action-values. And when it exploits, it might get more reward. It cannot, however, choose to do both simultaneously, which is also called the exploration-exploitation dilemma.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </div>
            <!-- /.entry-content -->
<section class="well" id="related-posts">
    <h4>Related Posts:</h4>
    <ul>
        <li><a href="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/">Decision Transformer: Unifying sequence modelling and model-free, offline RL</a></li>
    </ul>
</section>
    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'mchromiak'; // required: replace example with your forum shortname

                var disqus_url = 'https://mchromiak.github.io/articles/2021/May/01/RL-Primer/';

            var disqus_config = function () {
                this.language = "en";
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<div id="aboutme">
        <p>
            <img width="100%" class="img-thumbnail" src="https://mchromiak.github.io/static_files/img/me.png"/>
        </p>
    <p>
      <strong>About Micha≈Ç Chromiak</strong><br/>
         PhD in Computer Science by Polish Academy of Sciences (PAS). Focus research on understanding chaos of data. Deeply understanding the phenomena makes it easy, but first you need to learn. Holds two MScs, in Mathematics and in Computer Science.
    </p>
</div><!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://www.linkedin.com/in/michal-chromiak"><i class="fa fa-linkedin-square fa-lg"></i> LinkedIn</a></li>
    <li class="list-group-item"><a href="https://github.com/MichalChromiak"><i class="fa fa-github-square fa-lg"></i> GitHub</a></li>
    <li class="list-group-item"><a href="https://twitter.com/drChromiak"><i class="fa fa-twitter-square fa-lg"></i> Twitter</a></li>
    <li class="list-group-item"><a href="https://www.researchgate.net/profile/Michal_Chromiak"><i class="fa fa-researchgate-square fa-lg"></i> ResearchGate</a></li>
    <li class="list-group-item"><a href="https://scholar.google.pl/citations?user=UeOad3YAAAAJ&hl=en"><i class="fa fa-google-scholar-square fa-lg"></i> Google Scholar</a></li>
    <li class="list-group-item"><a href="localhost:8000/feeds/all.rss"><i class="fa fa-rss-square fa-lg"></i> RSS</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Recent Posts -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Recent Posts</span></h4>
  <ul class="list-group" id="recentposts">
    <li class="list-group-item"><a href="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/">Decision Transformer: Unifying sequence modelling and model-free, offline RL</a></li>
    <li class="list-group-item"><a href="https://mchromiak.github.io/articles/2021/May/05/MLP-Mixer/">MLP-Mixer: MLP is all you need... again? ...</a></li>
    <li class="list-group-item"><a href="https://mchromiak.github.io/articles/2021/May/01/RL-Primer/">RL Primer</a></li>
  </ul>
</li>
<!-- End Sidebar/Recent Posts -->

<!-- Sidebar/Categories -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Categories</span></h4>
  <ul class="list-group" id="categories">
    <li class="list-group-item">
      <a href="https://mchromiak.github.io/category/applications.html"><i class="fa fa-folder-open fa-lg"></i>Applications</a>
    </li>
    <li class="list-group-item">
      <a href="https://mchromiak.github.io/category/computer-vision.html"><i class="fa fa-folder-open fa-lg"></i>Computer Vision</a>
    </li>
    <li class="list-group-item">
      <a href="https://mchromiak.github.io/category/ml-dojo.html"><i class="fa fa-folder-open fa-lg"></i>ML Dojo</a>
    </li>
    <li class="list-group-item">
      <a href="https://mchromiak.github.io/category/reinforcement-learning.html"><i class="fa fa-folder-open fa-lg"></i>Reinforcement learning</a>
    </li>
    <li class="list-group-item">
      <a href="https://mchromiak.github.io/category/sequence-models.html"><i class="fa fa-folder-open fa-lg"></i>Sequence Models</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Categories -->

<!-- Sidebar/Tag Cloud -->
<li class="list-group-item">
  <a href="https://mchromiak.github.io/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
  <ul class="list-group list-inline tagcloud" id="tags">
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/application.html">application</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/attention-model.html">Attention model</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/basics.html">basics</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/bert.html">BERT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/cv.html">CV</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/elmo.html">ELMo</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/ernie-10.html">ERNIE 1.0</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/language-model.html">language model</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/machine-translation.html">Machine translation</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/markov-decision-process.html">Markov Decision Process</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/mdp.html">MDP</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/mlp.html">MLP</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/ngram.html">ngram</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="https://mchromiak.github.io/tag/nlp.html">NLP</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/nmt.html">NMT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/openai-gpt.html">OpenAI GPT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/patternrecognition.html">PatternRecognition</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/perplexity.html">perplexity</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/reinforcement-learning.html">Reinforcement Learning</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/rl.html">RL</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/seq2seq.html">seq2seq</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/sequence-transduction.html">Sequence transduction</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/smoothing.html">smoothing</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="https://mchromiak.github.io/tag/transformer.html">Transformer</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/vit.html">ViT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/xlnet.html">XLNet</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Tag Cloud -->

<!-- Sidebar/Links -->
<li class="list-group-item">
  <h4><i class="fa fa-external-link-square fa-lg"></i><span class="icon-label">Links</span></h4>
  <ul class="list-group" id="links">
    <li class="list-group-item">
      <a href="http://www.iclr.cc" target="_blank">ICLR Conf</a>
    </li>
    <li class="list-group-item">
      <a href="http://icml.cc" target="_blank">ICML Conf</a>
    </li>
    <li class="list-group-item">
      <a href="https://nips.cc/" target="_blank">NIPS Conf</a>
    </li>
    <li class="list-group-item">
      <a href="http://aifrontiers.com/" target="_blank">AI Frontiers</a>
    </li>
    <li class="list-group-item">
      <a href="https://developers.google.com/machine-learning/glossary/" target="_blank">ML Glossary</a>
    </li>
    <li class="list-group-item">
      <a href="https://deepdreamgenerator.com/" target="_blank">Deep Dream Generator</a>
    </li>
    <li class="list-group-item">
      <a href="https://deepart.io/" target="_blank">DeepArt Generator</a>
    </li>
    <li class="list-group-item">
      <a href="https://stanfordmlgroup.github.io/" target="_blank">Stanford ML Group Andrew Ng</a>
    </li>
    <li class="list-group-item">
      <a href="https://ai-on.org/" target="_blank">AI‚Ä¢ON open ML collaboration</a>
    </li>
    <li class="list-group-item">
      <a href="http://java-hive.blogspot.com/" target="_blank">My old blog on Java an SE</a>
    </li>
    <li class="list-group-item">
      <a href="http://karpathy.github.io/2016/09/07/phd/" target="_blank">PhD</a>
    </li>
    <li class="list-group-item">
      <a href="http://alt.qcri.org/semeval2017/" target="_blank">SemEval2017</a>
    </li>
    <li class="list-group-item">
      <a href="https://medium.freecodecamp.org/450-free-online-programming-computer-science-courses-you-can-start-in-september-59712e77635c" target="_blank">Free CS courses</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Links -->

<!-- Sidebar/Archive -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Archive</span></h4>
  <ul class="list-group" id="archive">
        <li class="list-group-item">
          <a href="https://mchromiak.github.io/archive/2021/Jun/index.html"><i class="fa fa-calendar fa-lg"></i>June 2021 (1)
          </a>
        </li>
        <li class="list-group-item">
          <a href="https://mchromiak.github.io/archive/2021/May/index.html"><i class="fa fa-calendar fa-lg"></i>May 2021 (2)
          </a>
        </li>
        <li class="list-group-item">
          <a href="https://mchromiak.github.io/archive/2019/Jul/index.html"><i class="fa fa-calendar fa-lg"></i>July 2019 (1)
          </a>
        </li>
        <li class="list-group-item">
          <a href="https://mchromiak.github.io/archive/2017/Nov/index.html"><i class="fa fa-calendar fa-lg"></i>November 2017 (1)
          </a>
        </li>
        <li class="list-group-item">
          <a href="https://mchromiak.github.io/archive/2017/Sep/index.html"><i class="fa fa-calendar fa-lg"></i>September 2017 (2)
          </a>
        </li>
        <li class="list-group-item">
          <a href="https://mchromiak.github.io/archive/2017/Aug/index.html"><i class="fa fa-calendar fa-lg"></i>August 2017 (1)
          </a>
        </li>
  </ul>
</li>
<!-- End Sidebar/Archive -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2021 Micha≈Ç Chromiak
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>                <p><small>  <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en"><img alt="Creative Commons License" style="border-width:0" src="//i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
    Content
  licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">Creative Commons Attribution-ShareAlike 4.0 International License</a>, except where indicated otherwise.
</small></p>
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="https://mchromiak.github.io/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="https://mchromiak.github.io/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="https://mchromiak.github.io/theme/js/respond.min.js"></script>

    <script src="https://mchromiak.github.io/static_files/js/custom.js"></script>

    <script src="https://mchromiak.github.io/theme/js/bodypadding.js"></script>
    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'mchromiak'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics Universal -->
    <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-108394162-1', 'auto');
        ga('send', 'pageview');
    </script>
    <!-- End Google Analytics Universal Code -->


        <script type="text/javascript">var addthis_config = {"data_track_addressbar": true};</script>
    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-59ea3c17b283c631"></script>
    <script src="https://mchromiak.github.io/static_files/js/article.js"></script>
</body>
</html>