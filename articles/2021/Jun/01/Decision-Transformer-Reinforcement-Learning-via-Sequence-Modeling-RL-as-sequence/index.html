<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Decision Transformer: Unifying sequence modeling and model-free, offline RL - Micha≈Ç Chromiak's blog</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">


    <link href="https://mchromiak.github.io/static_files/img/favicon.jpg" rel="icon">

<link rel="canonical" href="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/">

        <meta name="author" content="Micha≈Ç Chromiak" />
        <meta name="keywords" content="Transformer,Reinforcement Learning,RL,MDP,Markov Decision Process" />
        <meta name="description" content="Can we apply massive advancements of Transformer approach with its simplicity and scalability to Reinforcement Learning (RL)? Yes, but for that - one needs to approach RL as a sequence modeling problem. The Decision Transformer does that by abstracting RL as a conditional sequence modeling and using language modeling technique of casual masking of self-attention from GPT/BERT, enabling autoregressive generation of trajectories from the previous tokens in a sequence. The classical RL approach of fitting the value functions, or computing policy gradients, has been ditched in favor of masked Transformer yielding optimal actions. The Decision Transformer can match or outperform strong algorithms designed explicitly for offline RL with minimal modifications from standard language modeling architectures." />

        <meta property="og:site_name" content="Micha≈Ç Chromiak's blog" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Decision Transformer: Unifying sequence modeling and model-free, offline RL"/>
        <meta property="og:url" content="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/"/>
        <meta property="og:description" content="Can we apply massive advancements of Transformer approach with its simplicity and scalability to Reinforcement Learning (RL)? Yes, but for that - one needs to approach RL as a sequence modeling problem. The Decision Transformer does that by abstracting RL as a conditional sequence modeling and using language modeling technique of casual masking of self-attention from GPT/BERT, enabling autoregressive generation of trajectories from the previous tokens in a sequence. The classical RL approach of fitting the value functions, or computing policy gradients, has been ditched in favor of masked Transformer yielding optimal actions. The Decision Transformer can match or outperform strong algorithms designed explicitly for offline RL with minimal modifications from standard language modeling architectures."/>
            <meta property="og:image" content="https://mchromiak.github.io/articles/2021/Jun/01/img/DecisionTransformer-Cover.png" />

        <meta property="article:published_time" content="2021-06-01" />
            <meta property="article:section" content="Reinforcement learning" />
            <meta property="article:tag" content="Transformer" />
            <meta property="article:tag" content="Reinforcement Learning" />
            <meta property="article:tag" content="RL" />
            <meta property="article:tag" content="MDP" />
            <meta property="article:tag" content="Markov Decision Process" />
            <meta property="article:author" content="Micha≈Ç Chromiak" />


    <meta name="twitter:dnt" content="on">
    <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@drChromiak">
        <meta name="twitter:creator" content="@drChromiak">
    <meta name="twitter:domain" content="https://mchromiak.github.io">
        <meta property="twitter:image"
              content="https://mchromiak.github.io/articles/2021/Jun/01/img/DecisionTransformer-Cover.png"/>


    <!-- Bootstrap -->
        <link rel="stylesheet" href="https://mchromiak.github.io/theme/css/bootstrap.cerulean.min.css" type="text/css"/>
    <link href="https://mchromiak.github.io/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="https://mchromiak.github.io/theme/css/pygments/colorful.css" rel="stylesheet">
    <link rel="stylesheet" href="https://mchromiak.github.io/theme/css/style.css" type="text/css"/>
        <link href="https://mchromiak.github.io/static_files/css/custom.css" rel="stylesheet">

        <link href="https://mchromiak.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Micha≈Ç Chromiak's blog ATOM Feed"/>



        <link href="https://mchromiak.github.io/feeds/reinforcement-learning.atom.xml" type="application/atom+xml" rel="alternate"
              title="Micha≈Ç Chromiak's blog Reinforcement learning ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://mchromiak.github.io/" class="navbar-brand">
<img class="img-responsive pull-left gap-right" src="https://mchromiak.github.io/static_files/img/sitelogo40.png" width=""/> Micha≈Ç Chromiak's blog            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                         <li><a href="https://mchromiak.github.io/pages/about.html">
                             About
                          </a></li>
                         <li><a href="https://mchromiak.github.io/pages/categorisation.html">
                             Categorisation
                          </a></li>
                         <li><a href="https://mchromiak.github.io/pages/contact-detail.html">
                             Contact detail
                          </a></li>
                         <li><a href="https://mchromiak.github.io/pages/links.html">
                             Links
                          </a></li>
                         <li><a href="https://mchromiak.github.io/pages/resources.html">
                             Resources
                          </a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
            <ol class="breadcrumb">
                <li><a href="https://mchromiak.github.io" title="Micha≈Ç Chromiak's blog"><i class="fa fa-home fa-lg"></i></a></li>
                <li><a href="https://mchromiak.github.io/category/reinforcement-learning.html" title="Reinforcement learning">Reinforcement learning</a></li>
                <li class="active">Decision Transformer: Unifying sequence modeling and model-free, offline RL</li>
            </ol>
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/"
                       rel="bookmark"
                       title="Permalink to Decision Transformer: Unifying sequence modeling and model-free, offline RL">
                        Decision Transformer: Unifying sequence modeling and model-free, offline RL
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2021-06-01T19:30:00+02:00"> Tue, 01 Jun 2021</time>
    </span>
          <span class="label label-default">Modified</span>
            <span class="modified">
                <i class="fa fa-calendar"></i><time datetime="2021-06-01T19:30:00+02:00"> Tue, 01 Jun 2021</time>
            </span>


            <span class="label label-default">By</span>
            <a href="https://mchromiak.github.io/author/michal-chromiak.html"><i class="fa fa-user"></i> Micha≈Ç Chromiak</a>

        <span class="label label-default">Category</span>
        <a href="https://mchromiak.github.io/category/reinforcement-learning.html">Reinforcement learning</a>


<span class="label label-default">Tags</span>
	<a href="https://mchromiak.github.io/tag/transformer.html">Transformer</a>
        /
	<a href="https://mchromiak.github.io/tag/reinforcement-learning.html">Reinforcement Learning</a>
        /
	<a href="https://mchromiak.github.io/tag/rl.html">RL</a>
        /
	<a href="https://mchromiak.github.io/tag/mdp.html">MDP</a>
        /
	<a href="https://mchromiak.github.io/tag/markov-decision-process.html">Markov Decision Process</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <h4 id=the-decision-transformer-paper-explained>The Decision Transformer paper explained.<a class=headerlink href=#the-decision-transformer-paper-explained title="Permanent link">üîó</a></h4>
<p>In this article we will explain and discuss the paper:</p>
<p><strong><a href="https://arxiv.org/abs/2106.01345">"Decision Transformer: Reinforcement Learning via Sequence Modeling"</a></strong>: by Chen L. et al, ArXiv</p>
<p>that explores application of transformers to model sequential decision making problems - formalized as Reinforcement Learning (RL). By training a language model on a training dataset of random walk trajectories, it can figure out optimal trajectories by just conditioning on a large reward.</p>
<p align=center><img alt="Illustrative example of finding shortest path for a fixed graph (left) posed as reinforcement learning. Training dataset consists of random walk trajectories and their per-node returns-to-go (middle). \label{fig:dtgraph}" src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/DT_OptimalGraphPath.png">
Figure 1. Conditioned on a starting state and generating largest possible return at each node, Decision Transformer sequences optimal paths.  (<a href="https://arxiv.org/abs/2106.01345">Source</a>)</p>
<h4 id=tldr>TL;DR<a class=headerlink href=#tldr title="Permanent link">üîó</a></h4>
<ul>
<li>The idea is simple each modality (return, state, or action) is passed into an embedding network (convolutional encoder for images, linear layer for continuous states). The embeddings are then processed by an autoregressive transformer model, trained to predict the next action given the previous tokens using a linear output layer.</li>
<li>Instead of training_policy_ in a  a RL way, authors aim at sequence modeling objective, with use of sequence modeling algorithm, based on Transformer (the Transformer is not crucial here and could be replaced by any other autoregressive sequence modeling algorithm such as LSTM)</li>
<li>The algorithm is looking for <em>actions</em> conditioning based on the future (autoregressive) desired <em>reward</em>.</li>
<li>Presented algorithm requires more work/memorizing/learn by the network to get the <em>action</em> for a given <em>reward</em> (two inputs: state and reward), comparing to classical RL where the <em>action</em> is the output based on the maximized <em>reward</em> for a <em>policy</em>. One input: <em>state</em>, to train the <em>policy</em>. However, authors tested how transformers can cope with this approach minding recent advancements in sequence modeling.</li>
<li>Decision Transformer is based on <a href="../../../May/01/RL-Primer/#what-is-offline-rl">offline RL</a> to learn form historical sequence of (reward, state actions) tuples to output <em>action</em>, based on imitation of similar past <em>reward</em> and <em>state</em> inputs and <em>action</em> outputs.</li>
<li>Solution is learning about sequence evolution in a training dataset by learning from agents' past behaviors for similar inputs: <em>state</em>  and the presently required <em>reward</em>, to output an <em>action</em>.</li>
<li><strong>Limitation:</strong> If the <em>credit assignment</em> needs to happen longer than within one context, so when the relevant <em>action</em> for the <em>reward</em> is outside of the sequence that the Transformer attends to, it will fail as it would be out of the context.</li>
</ul>
<h4 id=contribution-of-paper>Contribution of paper:<a class=headerlink href=#contribution-of-paper title="Permanent link">üîó</a></h4>
<ul>
<li>use the GPT architecture to autoregressively model trajectories</li>
<li>bypass the need for bootstrapping (one of "deadly triad"<sup id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-1-back><a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-1 class=simple-footnote title="When one tries to combine time difference (TD) learning (or Bootstrapping), off-policy learning, and function approximations (such as Deep Neural Network), instabilities and divergence may arise.">1</a></sup>) by applying transformer models on collected experience using sequence modeling objective</li>
<li>enable to avoid discounting factor for future rewards used in TD thus, avoid inducing undesired short-range behaviors</li>
<li>bridge sequence modeling and transformers with RL</li>
<li>test if sequence modeling can perform policy optimization by evaluating Decision Transformer on offline RL benchmarks in Atari, OpenAI Gym, and Key-to-Door environments</li>
<li>DecisionTransformer (<strong>without</strong> dynamic programming) match, or exceeds performance of SOTA model-free offline RL: <a href="https://arxiv.org/abs/2006.04779">Conservative Q-Learing (CQL)</a>, <a href="https://arxiv.org/abs/1907.04543">Random Ensemble Mixture (REM)</a>, <a href="https://arxiv.org/abs/1710.10044">Quantile Regression Deep Q-Network (QR-DQN)</a> or behavioral cloning (BC)</li>
</ul>
<h3 id=intro>Intro<a class=headerlink href=#intro title="Permanent link">üîó</a></h3>
<p>In contrast to supervised learning, where the agent that imitates actions will (by definition) learn at its best to be on pair with the input data (trainer), the Reinforcement Learning can learn how to be much better like in case of AlphaGo (Go), AlphaStar (StarCraft II), AlphaZero (Chess) or OpenAI Five (Dota2).</p>
<p>Transformer framework is known of its performance, scalability and simplicity, as well as its generic approach. The level of abstraction that is available via transformer framework reduces the <em>inductive bias</em> and thus, is more flexible and effective in multiple applications relying more on the data than on assumed, hand designed concepts. As this has already been proven in NLP with GPT and BERT models and Computer Vision with Vision Transformer (ViT), authors have adapted transformers to the realm of Reinforcement Learning (RL).</p>
<p>If you are already familiar with RL terms used in the intro you might want to go directly to the <a href=#motivation>Decision Transformer paper motivation explained</a>. For more explanation of the terms referred in the paper please see <a href="../../../May/01/RL-Primer">prerequisite RL knowledge</a> section.</p>
<h3 id=motivation>Motivation<a class=headerlink href=#motivation title="Permanent link">üîó</a></h3>
<p>Transformer framework has proven to be very efficient in multiple applications in NLP and CV by modeling high-dimensional distributions of semantic concepts at scale.
Due to natural consequence of transformer nature, the input should be in form of a sequence thus, in case of <em>Decision Transformer</em> the RL has been casted as conditional sequence modeling. Specifically, the sequence modeling with transformers is used for policy optimization in RL as a strong algorithmic paradigm.</p>
<p>Transformers can also perform <em>credit assignment</em> directly via self-attention, in contrast to Bellman backups which slowly propagate rewards and are prone to ‚Äúdistractor‚Äù signals. This might help transformers to work with sparse, or distracting rewards. It even made Decision Transformer (DT) to outperform RL baseline for tasks requiring long term credit assignment due to the usage of long contexts. The DT aims to avoid any inductive bias for credit assignment, e.g. by explicitly learning the reward function or a critic, in favor of natural emergence of such properties thanks to transformer approach.</p>
<h3 id=objective-or-goal-for-the-algorithm>Objective, or goal for the algorithm<a class=headerlink href=#objective-or-goal-for-the-algorithm title="Permanent link">üîó</a></h3>
<p>The goal is to investigate if generative trajectory modeling ‚Äì i.e. modeling the joint distribution of the sequence of states, actions, and rewards ‚Äì can serve as a replacement for conventional RL algorithms.</p>
<h3 id=the-processing-strategy-of-the-algorithm>The processing strategy of the algorithm<a class=headerlink href=#the-processing-strategy-of-the-algorithm title="Permanent link">üîó</a></h3>
<p>The classic RL is focused on maximizing the reward_for a state by finding an action that actually will maximize the reward. Conversely, the key aspect of DT is to condition on future desired reward. This way the algorithm has additional - reward - input, along with the state. This way it is expected not only to maximize, but to fit to the actual reward value and that requires more learning/memorization. This is where transformers have proven to work particularly well for sequence modeling. Natural consequence of this design decision is to use the offline RL. This is why the paper is focused on the world of model-free and offline RL algorithms.</p>
<p>The tasks expects to find the right action for a given state and reward. The offline nature allows the Transformer to look into historical (offline, training dataset) data to find the same state with similar history and about the same reward from future action. From such past experience similar action will be deduced <sup id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-2-back><a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-2 class=simple-footnote title="Similarly to behavioral cloning which maximizes the reward based on expert demonstrations.[\ref]. This way based on the past sequence experiences,">2</a></sup></p>
<p><a name=fig:DT></a></p>
<script src="https://vjs.zencdn.net/7.11.4/video.min.js"></script>
<video id=my-player class="video-js vjs-theme-sea vjs-big-play-centered" controls preload=auto autoplay loop="" muted="" width=640 height=800>
 <source src="../img/dt.webm" type="video/webm">
 <p class=vjs-no-js>
      To view this video please enable JavaScript, and consider upgrading to a web browser that
      <a href="https://videojs.com/html5-video-support/" target=_blank> supports HTML5 video</a>
    </p>
</video>
<p align=center>Figure 2. Decision Transformer overview (<a href="https://arxiv.org/abs/2106.01345">Source</a>).</p>
<p>the Transformer outputs an <em>action</em> that in the past agents ended up with, for same <em>state</em>, while getting similar <em>reward</em> which in current timestep agent is expected to get (See <a href=#fig:DT>Figure 2.</a>).</p>
<blockquote>
<p>The major drawback of this approach is that it is limited to the context sequence length. Thus, in case when agent done action more aligned for current state-reward condition which however, is outside of the context (self-attended input of Transformer) - the sequence model will not be able to infer about that as it only attends to tokens within the limited length context.</p>
</blockquote>
<p>Therefore, the DT is better for credit assignment for longer distances than the LSTMs (see <a href=#the-considerations-of-the-DT>more</a>), but the drawback is that it still does not allow to ditch the classical RL dynamic programming methods as the are still suitable for the longer-than-context learning problems.</p>
<p>The key is that with minimal modifications to transformer, DT models trajectories autoregressively.  The goal here is to be able to conditionally generate actions at test time, based on <strong>future</strong> desired <em>returns</em> <span class=math>\(\hat{R}\)</span> (using modeled rewards) rather than past <em>rewards</em>.</p>
<p>A <em>trajectory</em> - <span class=math>\(\tau\)</span> - is made of sequence of states, actions and rewards at given timesteps: <span class=math>\(\tau = (s_0, a_0. r_0, s_1, a_1, r_1, \ldots, s_T, a_T, r_T)\)</span> The return of a trajectory at <span class=math>\(t\)</span> timestep, is the sum of future rewards of the trajectory starting from <span class=math>\(t\)</span>: <span class=math>\(R_t = \sum_{t'=t}^T r_{t'}\)</span> (without discount factor<sup id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-3-back><a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-3 class=simple-footnote title="The discount factor \(\gamma\) presence (in eg CQL) in the return calculation is a design decision for the problem formulation and it enables stability of the solution at price of discounting more of the future distant rewards than the closer ones in an episode.">3</a></sup>).</p>
<p>The goal of offline RL is to learn a policy which maximizes the <strong>expected return</strong> <span class=math>\(\mathbb{E}[\sum_{t=1}^T r_t]\)</span> in an  Markov Decision Process (MDP).
However, the DT instead of feeding rewards <span class=math>\(R\)</span> directly uses (future) <em>returns-to-go</em> <span class=math>\(\hat{R}\)</span> therefore, the trajectory becomes: <span class=math>\(\tau = (\hat{R}_1, s_1, a_1.  \hat{R}_2, s_2, a_2,  \ldots, \hat{R}_T), s_T, a_T\)</span>.
After executing the generated action for the current state, algorithm decrement the target return by the achieved reward and repeat until episode termination.</p>
<p>The input of the architecture is a state that is encoded with DQN encoder with linear layer to project to the embedding dimension followed by layer normalization. To obtain token embeddings, we learn a linear layer for each modality (return-to-go, state, or action), which projects raw inputs to the embedding dimension. The linear layer is replaced with convolution encoder for environments with visual inputs.
An embedding for each timestep is learned and added to each token ‚Äì  this is different from the standard positional embedding used by transformers, as one timestep corresponds to three tokens (one for each modality: return-to-go, state, or action). The tokens are then processed by a GPT model, which predicts future action tokens via autoregressive modeling.</p>
<p>The training use a dataset of offline trajectories. Samples minibatches of sequence length K are used from dataset. The Transformer head used for predicting for input token <span class=math>\(s_t\)</span> is used to predict <span class=math>\(a_t\)</span> either with cross-entropy for discrete actions or mean-squared for continuous actions while the loses are averaged.  </p>
<p align=center><a name=fig:DTalgo></a>
<img alt="Decision Transformer Pseudocode (for continuous actions)" src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/DT_algo.png">
Figure 3. Decision Transformer Pseudocode (<a href="https://arxiv.org/abs/2106.01345">Source</a>).</p>
<h3 id=metaphors-or-analogies-to-other-architectures-describing-the-behavior-of-the-algorithm>Metaphors, or analogies to other architectures describing the behavior of the algorithm<a class=headerlink href=#metaphors-or-analogies-to-other-architectures-describing-the-behavior-of-the-algorithm title="Permanent link">üîó</a></h3>
<p>There are two main approaches for offline learning that are compared to Decision Transformer: <em>Behavioral Cloning</em> (BC) and <em>Conservative Q-Learning</em> (CQL). The BC is to simply mimic behavior in episodes that has lead to good rewards.</p>
<h4 id=the-considerations-of-the-dt>The considerations of the DT<a class=headerlink href=#the-considerations-of-the-dt title="Permanent link">üîó</a></h4>
<p>An important aspect that should be mentioned about DT is related to its offline learning nature. That is the high dependency on the dataset that is picked as the training set of episodes. In the paper, the dataset is based on experience of DQN high performing agent, as an active reinforcement learner. This way the source episodes in dataset are not a random demonstrations, but are more of a set of <strong>expert</strong> demonstrations.</p>
<p>Additionally, DT is a sequence model and not an RL model. Interestingly, the transformer part of DT, could be replaced with any other autoregressive sequence modeling architecture such as LSTM.  In which case the past (state, action, reward) tuples can get propagated as a hidden representation, one step at a time, while producing next action (or a Q-value in case of Q-learning) that results with a certain reward.</p>
<p>The LSTMs finds the decisive action that lead to the expected result (during training) in a sequence of actions. As we are looking for this most meaningful actions to gain the expected final state, it is not the last action  in the sequence of state changing actions, that might be the best. The goal of RL is to assign the highest reward to this most meaningful action in an episode. This way in future this actions will be favored.</p>
<p>For LSTMs this requires to go back though the episode sequence of states (and actions) deeper with Back Propagation Though Time (BPTT)<sup id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-4-back><a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-4 class=simple-footnote title=" one trains recurrent neural networks by unrolling the network into a virtual feed forward network, and applying the backpropagation algorithm to that. This method is called Back-Propagation-Through-Time (BPTT), as it requires propagation of gradients backwards in time.">4</a></sup> which limits how far back the error is propagated (to at most the length of the trajectory) because it is computationally expensive, as the number of timesteps increase in LSTM (as a RNN variant).</p>
<p>Therefore, it is a very hard (esp. in architectures like RNN/LSTM) task to do the long-range time credit assignments.</p>
<p>To solve this problem a <a href="https://www.sciencedirect.com/science/article/pii/B9781558601413500304">dynamic programming</a> is used in RL.  Therefore, instead of having to just learn from the reward and assign it to an action, each timestep will output not only an <em>action</em>, but also a <em>value</em> (similarly as Q-function already returns a value in Q-learning). The algorithm that does this is called Temporal Difference learning. The <em>value</em> is the expected/estimated return (final reward in the future) of the episode, starting from the given state, and following till the final state (return) is reached. This way, the final return prediction is not the only learning signal, but also this gives predictions of future reward values for the next actions in each of the intermediate, consecutive states in the path that leads to the final return state. In other words, at each state the  <em>value function</em> not only tries to predict the final return (which is really noisy for states that are far from return), but also train the value function to predict all the values from states that are between the current state and the final one in an episode. Thus, trying to predict the output of the value function.</p>
<blockquote>
<p>Bellman (aka backup / update) operator - operation of updating the value of state <span class=math>\(s\)</span> from the value of other states that could be potentially reached from state <span class=math>\(s\)</span>.</p>
</blockquote>
<p>Bellman equation for the <a href="../../../May/01/RL-Primer/#value-functions">V-function</a> (State-Value) simplifies computation:</p>
<div class=math>\begin{equation}
v^\pi (s)=  \mathbb{E_\pi}[R_{t+1} + \gamma  v(S_{t+1})| S_t = s]
\end{equation}</div>
<p>Bellman equation decomposes the V-function into two parts, the immediate reward - <span class=math>\(R_{t+1}\)</span> - plus the discounted future values of successor states. This equation simplifies the computation of the value function. This way, rather than summing over multiple time steps, we find the optimal solution of a complex problem by breaking it down into simpler, recursive subproblems and utilizing the fact that finding their optimal solution to the overall problem depends upon the optimal solution to its subproblems. Which make it a <em>dynamic programming</em> algorithm similarly to the <a href="../../../May/01/RL-Primer/#temporal-difference-td-learning">Temporal Difference</a> approach,</p>
<p>It works similarly in terms of the <a href="../../../May/01/RL-Primer/#value-functions">Q-function</a> (State-Action value function) in Q-learning, where this approach is formalized with the Bellman's recurrence equation.</p>
<div class=math>\begin{equation}
q_\pi (s,a) = \mathbb{E_\pi}[R_{t+1} + \gamma q_\pi (S_{t+1}, A_{t+1})| S_t=s, A_t=a]
\end{equation}</div>
<p>Here, State-Action Value of a state can be decomposed into the immediate reward we get on performing a certain action in state <span class=math>\(s\)</span> and moving to another state  <span class=math>\(s‚Äô\)</span> plus the discounted value of the state-action value of the state <span class=math>\(s‚Äô\)</span> with respect to the some action <span class=math>\(a\)</span> our agent will take from that state on-wards.</p>
<p>In RL we use the TD learning and Q-functions to allow <em>credit assignment</em> for long time ranges.</p>
<p>However, as the DT is a sequence modeling problem there is no need for <em>dynamic programming</em> techniques such as Temporal Difference (TD) to be applied. This is due to the Transformer nature, that enables to attend (route information) from any sequence element to any other sequence element in a single step. Therefore, technically the <em>credit assignment</em> can be done in a single step - but under one crucial condition: <strong>as as long as if it fits into context</strong>.</p>
<h5 id=limitations>Limitations<a class=headerlink href=#limitations title="Permanent link">üîó</a></h5>
<p>The limitation of this <em>credit assignment</em> approach is that this works only works for a given length of Transformer input sequence (context). If one would like to predict correlations and do the <em>credit assignment</em> across longer spans, than the Transformer input sequence (as a context), still the <em>dynamic programming</em> would be required.</p>
<h3 id=heuristics-or-rules-of-thumb>Heuristics or rules of thumb<a class=headerlink href=#heuristics-or-rules-of-thumb title="Permanent link">üîó</a></h3>
<p>The DT has been tested on the Atari dataset. The show that DT outperformed remaining algorithms on most of the games however, with high standard deviation.</p>
<p align=center><a name=fig:dtatari></a>
<img alt="Gamer-normalized scores for the 1% DQN-replay Atari dataset. We report the mean and variance across 3 seeds. Best mean scores are highlighted in bold. Decision Transformer (DT) performs comparably to CQL on 3 out of 4 games, and outperforms other baselines." src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/dt_atari.png">
Figure 4. Decision Transformer Pseudocode (<a href="https://arxiv.org/abs/2106.01345">Source</a>).</p>
<p>The lower standard deviation and also mostly better results, comparing  to CQL, are presented for OpenAI Gym considering the continuous control tasks from the D4RL benchmark.</p>
<p align=center><a name=fig:dt_openai></a>
<img alt="DT results for D4RL datasets." src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/dt_openaigym.png">
Figure 5. Decision Transformer (DT) outperforms conventional RL algorithms on almost all tasks. (<a href="https://arxiv.org/abs/2106.01345">Source</a>).</p>
<p>Additionally, comparing to behavioral (see <a href=#ig:dt_bc>Figure 6.</a>) cloning trained on set percentent of the experience 10, 25, 40, 100 percent. The results show that BC can give better performance than DT if the choice will select certain percentage not not necessarily pickign the best <em>trajectories</em>. To find the right percentage additional hyper-parameter search while DT is just one run solution.</p>
<p align=center><a name=fig:dt_bc></a>
<img alt="DT vs BC." src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/dt_openaigym.png">
Figure 6. Comparison between Decision Transformer (DT) and Percentile Behavior Cloning (%BC). (<a href="https://arxiv.org/abs/2106.01345">Source</a>).</p>
<p>However, one should note that in each of the experiments the DT hyperparameters are different for each experiment. For instance whenever a task is a sparse reward task the context length is increased to limit the impact of the context boundary limitation as described in previous <a href=#limitations>sections</a>. To confirm this Table 5 in the paper shows that the longer the context the better.</p>
<p>Additionally, in experiments such as the game of Pong the reward is known to be at max 21 it helped to condition on the reward of 20 (see Table 8 of the appendix). Therefore, the fact of knowing the reward is crucial to setup the hperparameters correctly.</p>
<p>The limitation of DT seems to be that you need to know the reward that task is aiming for and according to Figure 4. of the paper putting arbitrary large value for reward is not possible. When green line of expected oracle is aligned with the blue line of the DT outcome, means that the reward is as expected. Soon after the received reward is higher than the one observed from the training dataset performance drops.  </p>
<p>One final example if the <em>Key-to-Door</em> environment where the use of Transformer with longer context where the one step credit assignment can be done is proven. This however is only limited to the size/length of the context. The <a href=#fig:dt_ktd>Figure 7.</a> shows that with the DT, if the agent does not pick up the key in the first room, it already knows in second room that the game is lost due to being withing the context of Transformer's attention and that knows from the past offline episodes that not picking the key means that the game is lost.</p>
<p align=center><a name=fig:dt_ktd></a>
<img alt=key-to-door. src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/dt_ktd.png">
Figure 7. Comparison between Decision Transformer (DT) and Percentile Behavior Cloning (%BC). (<a href="https://arxiv.org/abs/2106.01345">Source</a>).</p>
<h3 id=what-classes-of-problem-is-the-algorithm-well>What classes of problem is the algorithm well ?<a class=headerlink href=#what-classes-of-problem-is-the-algorithm-well title="Permanent link">üîó</a></h3>
<p>As shown in the test results as long as the context length of the Decision Transformer matches the task expectation the solutions behaves better than other methods like CQL or BC. However, to achieve this, one needs to know the length of the context in form of a hyperparameter.</p>
<p>Additionally, for the sake of the performance, knowing the maximum reward is also important.</p>
<h3 id=common-benchmark-or-example-datasets-used-to-demonstrate-the-algorithm>Common benchmark or example datasets used to demonstrate the algorithm<a class=headerlink href=#common-benchmark-or-example-datasets-used-to-demonstrate-the-algorithm title="Permanent link">üîó</a></h3>
<p>Decision Transformer can match the performance of well-studied and specialized TD learning algorithms developed for these settings.</p>
<p align=center><a name=fig:dt_perf></a>
<img alt="DT performance comparison." src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/dt_perf.png">
Figure 8. Results comparin DT to TD learning (CQL) and behavior cloning across Atari, OpenAI Gym, and Minigrid. (<a href="https://arxiv.org/abs/2106.01345">Source</a>).</p>
<p>As DT is based on a offline mode, a very important part of offline RL is how do we pick this fixed dataset to model upon. Here it is based on DQN expert learner so the results are good and far from being random.</p>
<p>In the paper authors compare to two approaches that are used in offline RL setup:</p>
<ul>
<li><em>Behavioral Cloning</em> (BC) where the agent tries to mimic those agents from the dataset, which have gained high rewards for their actions.</li>
<li><em>Q-Learning</em> (actually a <em>Conservative Q-Learning</em>) (CQL). Q-learning is a classic RL algorithm which uses Q-function to estimate Q-values, along the possible state and action space, to maximize the total reward. The Conservative Q-learning (CQL) is more of a pessimistic approach, that helps to mitigate the tendency of Q-function to overestimate the Q-values that one gets from certain actions.</li>
</ul>
<h3 id=useful-resources-for-learning-more-about-the-algorithm>Useful resources for learning more about the algorithm:<a class=headerlink href=#useful-resources-for-learning-more-about-the-algorithm title="Permanent link">üîó</a></h3>
<ul>
<li><a href="https://arxiv.org/abs/2106.01345">ArXiv Paper</a></li>
<li><a href="https://github.com/kzl/decision-transformer">GitHub implementation</a></li>
</ul>
<hr>

<h3 id=footnotes>Footnotes:<a class=headerlink href=#footnotes title="Permanent link">üîó</a></h3>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script><ol class=simple-footnotes><li id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-1>When one tries to combine <em>time difference</em> (TD) learning (or Bootstrapping), off-policy learning, and function approximations (such as Deep Neural Network), instabilities and divergence may arise. <a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-1-back class=simple-footnote-back>‚Ü©</a></li><li id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-2>Similarly to behavioral cloning which maximizes the reward based on expert demonstrations.[\ref]. This way based on the past sequence experiences, <a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-2-back class=simple-footnote-back>‚Ü©</a></li><li id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-3>The <em>discount</em> factor <span class=math>\(\gamma\)</span> presence (in eg CQL) in the return calculation is a design decision for the problem formulation and it enables stability of the solution at price of discounting more of the future distant rewards than the closer ones in an episode. <a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-3-back class=simple-footnote-back>‚Ü©</a></li><li id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-4> one trains recurrent neural networks by unrolling the network into a virtual feed forward network, and applying the backpropagation algorithm to that. This method is called Back-Propagation-Through-Time (BPTT), as it requires propagation of gradients backwards in time. <a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-4-back class=simple-footnote-back>‚Ü©</a></li></ol>
            </div>
            <!-- /.entry-content -->
<section class="well" id="related-posts">
    <h4>Related Posts:</h4>
    <ul>
        <li><a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/">The Transformer ‚Äì Attention is all you need.</a></li>
        <li><a href="https://mchromiak.github.io/articles/2021/May/01/RL-Primer/">RL Primer</a></li>
    </ul>
</section>
    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'mchromiak'; // required: replace example with your forum shortname

                var disqus_url = 'https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/';

            var disqus_config = function () {
                this.language = "en";
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<div id="aboutme">
        <p>
            <img width="100%" class="img-thumbnail" src="https://mchromiak.github.io/static_files/img/me.png"/>
        </p>
    <p>
      <strong>About Micha≈Ç Chromiak</strong><br/>
         PhD in Computer Science by Polish Academy of Sciences (PAS). Focus research on understanding chaos of data. Deeply understanding the phenomena makes it easy, but first you need to learn. Holds two MScs, in Mathematics and in Computer Science.
    </p>
</div><!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://www.linkedin.com/in/michal-chromiak"><i class="fa fa-linkedin-square fa-lg"></i> LinkedIn</a></li>
    <li class="list-group-item"><a href="https://github.com/MichalChromiak"><i class="fa fa-github-square fa-lg"></i> GitHub</a></li>
    <li class="list-group-item"><a href="https://twitter.com/drChromiak"><i class="fa fa-twitter-square fa-lg"></i> Twitter</a></li>
    <li class="list-group-item"><a href="https://www.researchgate.net/profile/Michal_Chromiak"><i class="fa fa-researchgate-square fa-lg"></i> ResearchGate</a></li>
    <li class="list-group-item"><a href="https://scholar.google.pl/citations?user=UeOad3YAAAAJ&hl=en"><i class="fa fa-google-scholar-square fa-lg"></i> Google Scholar</a></li>
    <li class="list-group-item"><a href="localhost:8000/feeds/all.rss"><i class="fa fa-rss-square fa-lg"></i> RSS</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Recent Posts -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Recent Posts</span></h4>
  <ul class="list-group" id="recentposts">
    <li class="list-group-item"><a href="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/">Decision Transformer: Unifying sequence modeling and model-free, offline RL</a></li>
    <li class="list-group-item"><a href="https://mchromiak.github.io/articles/2021/May/06/MLP-Mixer/">MLP-Mixer: MLP is all you need... again? ...</a></li>
    <li class="list-group-item"><a href="https://mchromiak.github.io/articles/2021/May/01/RL-Primer/">RL Primer</a></li>
  </ul>
</li>
<!-- End Sidebar/Recent Posts -->

<!-- Sidebar/Categories -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Categories</span></h4>
  <ul class="list-group" id="categories">
    <li class="list-group-item">
      <a href="https://mchromiak.github.io/category/applications.html"><i class="fa fa-folder-open fa-lg"></i>Applications</a>
    </li>
    <li class="list-group-item">
      <a href="https://mchromiak.github.io/category/computer-vision.html"><i class="fa fa-folder-open fa-lg"></i>Computer Vision</a>
    </li>
    <li class="list-group-item">
      <a href="https://mchromiak.github.io/category/ml-dojo.html"><i class="fa fa-folder-open fa-lg"></i>ML Dojo</a>
    </li>
    <li class="list-group-item">
      <a href="https://mchromiak.github.io/category/reinforcement-learning.html"><i class="fa fa-folder-open fa-lg"></i>Reinforcement learning</a>
    </li>
    <li class="list-group-item">
      <a href="https://mchromiak.github.io/category/sequence-models.html"><i class="fa fa-folder-open fa-lg"></i>Sequence Models</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Categories -->

<!-- Sidebar/Tag Cloud -->
<li class="list-group-item">
  <a href="https://mchromiak.github.io/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
  <ul class="list-group list-inline tagcloud" id="tags">
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/application.html">application</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/attention-model.html">Attention model</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/basics.html">basics</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/bert.html">BERT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/cv.html">CV</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/elmo.html">ELMo</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/ernie-10.html">ERNIE 1.0</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/language-model.html">language model</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/machine-translation.html">Machine translation</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/markov-decision-process.html">Markov Decision Process</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/mdp.html">MDP</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/mlp.html">MLP</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/ngram.html">ngram</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="https://mchromiak.github.io/tag/nlp.html">NLP</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/nmt.html">NMT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/openai-gpt.html">OpenAI GPT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/patternrecognition.html">PatternRecognition</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/perplexity.html">perplexity</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/reinforcement-learning.html">Reinforcement Learning</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/rl.html">RL</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/seq2seq.html">seq2seq</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="https://mchromiak.github.io/tag/sequence-transduction.html">Sequence transduction</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/smoothing.html">smoothing</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="https://mchromiak.github.io/tag/transformer.html">Transformer</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/vit.html">ViT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="https://mchromiak.github.io/tag/xlnet.html">XLNet</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Tag Cloud -->

<!-- Sidebar/Links -->
<li class="list-group-item">
  <h4><i class="fa fa-external-link-square fa-lg"></i><span class="icon-label">Links</span></h4>
  <ul class="list-group" id="links">
    <li class="list-group-item">
      <a href="http://www.iclr.cc" target="_blank">ICLR Conf</a>
    </li>
    <li class="list-group-item">
      <a href="http://icml.cc" target="_blank">ICML Conf</a>
    </li>
    <li class="list-group-item">
      <a href="https://nips.cc/" target="_blank">NIPS Conf</a>
    </li>
    <li class="list-group-item">
      <a href="http://aifrontiers.com/" target="_blank">AI Frontiers</a>
    </li>
    <li class="list-group-item">
      <a href="https://developers.google.com/machine-learning/glossary/" target="_blank">ML Glossary</a>
    </li>
    <li class="list-group-item">
      <a href="https://deepdreamgenerator.com/" target="_blank">Deep Dream Generator</a>
    </li>
    <li class="list-group-item">
      <a href="https://deepart.io/" target="_blank">DeepArt Generator</a>
    </li>
    <li class="list-group-item">
      <a href="https://stanfordmlgroup.github.io/" target="_blank">Stanford ML Group Andrew Ng</a>
    </li>
    <li class="list-group-item">
      <a href="https://ai-on.org/" target="_blank">AI‚Ä¢ON open ML collaboration</a>
    </li>
    <li class="list-group-item">
      <a href="http://java-hive.blogspot.com/" target="_blank">My old blog on Java an SE</a>
    </li>
    <li class="list-group-item">
      <a href="http://karpathy.github.io/2016/09/07/phd/" target="_blank">PhD</a>
    </li>
    <li class="list-group-item">
      <a href="http://alt.qcri.org/semeval2017/" target="_blank">SemEval2017</a>
    </li>
    <li class="list-group-item">
      <a href="https://medium.freecodecamp.org/450-free-online-programming-computer-science-courses-you-can-start-in-september-59712e77635c" target="_blank">Free CS courses</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Links -->

<!-- Sidebar/Archive -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Archive</span></h4>
  <ul class="list-group" id="archive">
        <li class="list-group-item">
          <a href="https://mchromiak.github.io/archive/2021/Jun/index.html"><i class="fa fa-calendar fa-lg"></i>June 2021 (1)
          </a>
        </li>
        <li class="list-group-item">
          <a href="https://mchromiak.github.io/archive/2021/May/index.html"><i class="fa fa-calendar fa-lg"></i>May 2021 (2)
          </a>
        </li>
        <li class="list-group-item">
          <a href="https://mchromiak.github.io/archive/2019/Jul/index.html"><i class="fa fa-calendar fa-lg"></i>July 2019 (1)
          </a>
        </li>
        <li class="list-group-item">
          <a href="https://mchromiak.github.io/archive/2017/Nov/index.html"><i class="fa fa-calendar fa-lg"></i>November 2017 (1)
          </a>
        </li>
        <li class="list-group-item">
          <a href="https://mchromiak.github.io/archive/2017/Sep/index.html"><i class="fa fa-calendar fa-lg"></i>September 2017 (2)
          </a>
        </li>
        <li class="list-group-item">
          <a href="https://mchromiak.github.io/archive/2017/Aug/index.html"><i class="fa fa-calendar fa-lg"></i>August 2017 (1)
          </a>
        </li>
  </ul>
</li>
<!-- End Sidebar/Archive -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2021 Micha≈Ç Chromiak
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>                <p><small>  <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en"><img alt="Creative Commons License" style="border-width:0" src="//i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
    Content
  licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">Creative Commons Attribution-ShareAlike 4.0 International License</a>, except where indicated otherwise.
</small></p>
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="https://mchromiak.github.io/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="https://mchromiak.github.io/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="https://mchromiak.github.io/theme/js/respond.min.js"></script>

    <script src="https://mchromiak.github.io/static_files/js/custom.js"></script>

    <script src="https://mchromiak.github.io/theme/js/bodypadding.js"></script>
    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'mchromiak'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics Universal -->
    <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-108394162-1', 'auto');
        ga('send', 'pageview');
    </script>
    <!-- End Google Analytics Universal Code -->


        <script type="text/javascript">var addthis_config = {"data_track_addressbar": true};</script>
    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-59ea3c17b283c631"></script>
    <script src="https://mchromiak.github.io/static_files/js/article.js"></script>
</body>
</html>