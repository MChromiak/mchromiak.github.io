<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>ERNIE 2.0: A continual pre-training framework for language understanding - MichaÅ‚ Chromiak's blog</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">


    <link href="/static_files/img/favicon.jpg" rel="icon">

<link rel="canonical" href="/articles/2019/Jul/30/ernie-2-0/">

        <meta name="author" content="MichaÅ‚ Chromiak" />
        <meta name="keywords" content="NMT,transformer,Sequence transduction,Attention model,Machine translation,seq2seq,NLP,ELMo,OpenAI GPT,BERT,ERNIE 1.0,XLNet" />
        <meta name="description" content="ERNIE 2.0 (Enhanced Representation through kNowledge IntEgration), a new knowledge integration language representation model that aims to beat SOTA results of BERT and XLNet. While pre-training with more than just several simple tasks to grasp the co-occurrence of words or sentences for language modeling, Ernie aims to explore named entities, semantic closeness and discourse relations to obtain valuable lexical, syntactic and semantic information from training corpora. Ernie 2.0 focus on building and learning incrementally pre-training tasks through constant multi-task learning. And it brings some interesting results." />

        <meta property="og:site_name" content="MichaÅ‚ Chromiak's blog" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="ERNIE 2.0: A continual pre-training framework for language understanding"/>
        <meta property="og:url" content="/articles/2019/Jul/30/ernie-2-0/"/>
        <meta property="og:description" content="ERNIE 2.0 (Enhanced Representation through kNowledge IntEgration), a new knowledge integration language representation model that aims to beat SOTA results of BERT and XLNet. While pre-training with more than just several simple tasks to grasp the co-occurrence of words or sentences for language modeling, Ernie aims to explore named entities, semantic closeness and discourse relations to obtain valuable lexical, syntactic and semantic information from training corpora. Ernie 2.0 focus on building and learning incrementally pre-training tasks through constant multi-task learning. And it brings some interesting results."/>
            <meta property="og:image" content="/articles/2019/Jul/30/img/enrieCover.jpeg" />

        <meta property="article:published_time" content="2019-07-30" />
            <meta property="article:section" content="Sequence Models" />
            <meta property="article:tag" content="NMT" />
            <meta property="article:tag" content="transformer" />
            <meta property="article:tag" content="Sequence transduction" />
            <meta property="article:tag" content="Attention model" />
            <meta property="article:tag" content="Machine translation" />
            <meta property="article:tag" content="seq2seq" />
            <meta property="article:tag" content="NLP" />
            <meta property="article:tag" content="ELMo" />
            <meta property="article:tag" content="OpenAI GPT" />
            <meta property="article:tag" content="BERT" />
            <meta property="article:tag" content="ERNIE 1.0" />
            <meta property="article:tag" content="XLNet" />
            <meta property="article:author" content="MichaÅ‚ Chromiak" />


    <meta name="twitter:dnt" content="on">
    <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@drChromiak">
        <meta name="twitter:creator" content="@drChromiak">
    <meta name="twitter:domain" content="">
        <meta property="twitter:image"
              content="/articles/2019/Jul/30/img/enrieCover.jpeg"/>


    <!-- Bootstrap -->
        <link rel="stylesheet" href="/theme/css/bootstrap.cerulean.min.css" type="text/css"/>
    <link href="/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="/theme/css/pygments/colorful.css" rel="stylesheet">
    <link rel="stylesheet" href="/theme/css/style.css" type="text/css"/>
        <link href="/static_files/css/custom.css" rel="stylesheet">

        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="MichaÅ‚ Chromiak's blog ATOM Feed"/>




</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="/" class="navbar-brand">
<img class="img-responsive pull-left gap-right" src="/static_files/img/sitelogo40.png" width=""/> MichaÅ‚ Chromiak's blog            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                         <li><a href="/pages/about.html">
                             About
                          </a></li>
                         <li><a href="/pages/categorisation.html">
                             Categorisation
                          </a></li>
                         <li><a href="/pages/contact-detail.html">
                             Contact detail
                          </a></li>
                         <li><a href="/pages/links.html">
                             Links
                          </a></li>
                         <li><a href="/pages/resources.html">
                             Resources
                          </a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
            <ol class="breadcrumb">
                <li><a href="" title="MichaÅ‚ Chromiak's blog"><i class="fa fa-home fa-lg"></i></a></li>
                <li><a href="/category/sequence-models.html" title="Sequence Models">Sequence Models</a></li>
                <li class="active">ERNIE 2.0: A continual pre-training framework for language understanding</li>
            </ol>
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="/articles/2019/Jul/30/ernie-2-0/"
                       rel="bookmark"
                       title="Permalink to ERNIE 2.0: A continual pre-training framework for language understanding">
                        ERNIE 2.0: A continual pre-training framework for language understanding
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2019-07-30T19:30:00+02:00"> Tue, 30 Jul 2019</time>
    </span>
          <span class="label label-default">Modified</span>
            <span class="modified">
                <i class="fa fa-calendar"></i><time datetime="2019-07-30T19:30:00+02:00"> Tue, 30 Jul 2019</time>
            </span>


            <span class="label label-default">By</span>
            <a href="/author/michal-chromiak.html"><i class="fa fa-user"></i> MichaÅ‚ Chromiak</a>

        <span class="label label-default">Category</span>
        <a href="/category/sequence-models.html">Sequence Models</a>


<span class="label label-default">Tags</span>
	<a href="/tag/nmt.html">NMT</a>
        /
	<a href="/tag/transformer.html">transformer</a>
        /
	<a href="/tag/sequence-transduction.html">Sequence transduction</a>
        /
	<a href="/tag/attention-model.html">Attention model</a>
        /
	<a href="/tag/machine-translation.html">Machine translation</a>
        /
	<a href="/tag/seq2seq.html">seq2seq</a>
        /
	<a href="/tag/nlp.html">NLP</a>
        /
	<a href="/tag/elmo.html">ELMo</a>
        /
	<a href="/tag/openai-gpt.html">OpenAI GPT</a>
        /
	<a href="/tag/bert.html">BERT</a>
        /
	<a href="/tag/ernie-10.html">ERNIE 1.0</a>
        /
	<a href="/tag/xlnet.html">XLNet</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>Transformer based models has become the dominant part of modern SOTA research for NLP. ERNIE 2.0 is no different. Lets deep dive into what novel is about the second version of Ernie.</p>
<h4 id="tldr">TL:DR<a class="headerlink" href="#tldr" title="Permanent link">ðŸ”—</a></h4>
<p><strong>Focus attention to more than just co-occurence.</strong>
Contribution of paper:</p>
<ul>
<li>Design of continual pre-training framework ERNIE 2.0, which supports customized training tasks and multi-task pre-training in an incremental way.</li>
<li>ERNIE 2.0 achieves significant improvements over BERT and XLNet on 16 tasks including English GLUE benchmarks and several Chinese tasks.</li>
</ul>
<h3 id="intro">Intro<a class="headerlink" href="#intro" title="Permanent link">ðŸ”—</a></h3>
<p>Co-occurence of words and sentences is the main workhorse of current NLP SOTA solutions. ERNIE 2.0 tries to get a bigger picture with its continuous pre-training.  </p>
<h3 id="motivation">Motivation<a class="headerlink" href="#motivation" title="Permanent link">ðŸ”—</a></h3>
<p>It is the fact that with current solutions, training of the model usually focus on couple of tasks to get the notion of co-occurrence of words and sentences. Those pre-training procedures involved word-level and sentence-level prediction or inference tasks. The closest competitors like Google's BERT captured co-occurrence information by combining a masked language model and a next-sentence prediction task. On the other hand, this co-occurrence information is captured by XLNet by constructing a permutation language model task.</p>
<p>With Ernie 2.0 however, the vision becomes broader than just co-occurence. This includes more lexical, syntactic and semantic information from training corpora in form of named entities (like person names, location names, and organization names), semantic closeness (proximity of sentences), sentence order or discourse relations. This is done with Ernie 2.0: continual pre-training framework, by building and learning incrementally pre-training tasks  through constant multi-task learning.</p>
<p>The important aspect is that at any time a new custom task can be introduced, while sharing the same encoding network. Thanks to this, a significant amount of information can be shared across different tasks. Ernie 2.0 framework is continuously pre-training and updating the model and gaining enhancements in information through multi-task learning, hence more holistically understand lexical, syntactic and semantic representations.   </p>
<ul>
<li>Paper: <a href="https://arxiv.org/abs/1907.12412"> Ernie 2.0: A continual pre-training framework for language understanding. ArXiv</a>.</li>
<li>Code for fine-tuning and pre-trained models for English: <a href="https://github.com/PaddlePaddle/ERNIE">GitHub</a></li>
</ul>
<p align="center"><img alt="ERNIE2" src="/articles/2019/Jul/30/ernie-2-0/img/ernie2.gif">
<br>
Figure 1. A continual pre-training framework for language understanding.</p>
<h3 id="the-continual-pre-training">The continual pre-training<a class="headerlink" href="#the-continual-pre-training" title="Permanent link">ðŸ”—</a></h3>
<p>The <em>continual</em> pre-training consists of two phases:</p>
<p><strong>1. Continuously construct unsupervised pre-training tasks using the big data and with prior knowledge involved</strong></p>
<p>This phase include three types of tasks:</p>
<ul>
<li><em>word</em>-aware tasks: Knowledge Masking, Capitalization Prediction, Token-Document Relation Prediction</li>
<li><em>structure</em>-aware tasks: Sentence Reordering, Sentence Distance</li>
<li><em>semantic</em>-aware tasks: Discourse Relation, IR Relevance</li>
</ul>
<p>For the details of the tasks please refer to the paper. All tasks are based on self/weak-supervised signals without human annotation.</p>
<p><strong>2. Incrementally update the ERNIE model via multi-task learning</strong>
Continuously, task-by-task new tasks are added starting from first, initial task to train initial model. Then next task's parameters are initialized to the values already learned form the previous one. This way the parameters' "knowledge" is passed on and accumulated within the consecutive tasks.</p>
<p>This goal is achieved with series of shared text encoding layers that <strong>encodes</strong> the contextual information possibly customized with RNNs or deep Transformer. The parameters of the encoder are the ones that are updated when a new task is introduced.</p>
<p>There are two types of loss functions: sequence-level and token-level. For each task sentence-level loss functions can be combined with token-level loss functions creating the task's loss function used to update the model.</p>
<h4 id="fine-tuning-for-tasks">Fine tuning for tasks<a class="headerlink" href="#fine-tuning-for-tasks" title="Permanent link">ðŸ”—</a></h4>
<p>The phase of fine-tuning of the pre-trained model for language understanding tasks (e.g question answering, natural language inference, semantic similarity) is done with task specific supervised data.</p>
<h3 id="ernie-20-model">ERNIE 2.0 model<a class="headerlink" href="#ernie-20-model" title="Permanent link">ðŸ”—</a></h3>
<p>The model is similar to BERT or XLNet.</p>
<p align="center"><img alt="ERNIE2struct" src="/articles/2019/Jul/30/ernie-2-0/img/enrie2struct.png">
<br>
Figure 2. ERNIE 2.0 model structure. The input embedding consists of the token embedding, the sentence embedding, the position embedding and the task embedding. There are seven pre-training tasks in the ERNIE 2.0 model.</p>
<h4 id="encoder">Encoder<a class="headerlink" href="#encoder" title="Permanent link">ðŸ”—</a></h4>
<p>Is a multi-layer Transformer same as in BERT, GPT, XLM. Transformer generate sequence of contextual embeddings for each token. Additional markers are added to the start of the sequence and separator symbol is added to separate intervals for the multiple input segment tasks.</p>
<h4 id="task-embeddings">Task embeddings<a class="headerlink" href="#task-embeddings" title="Permanent link">ðŸ”—</a></h4>
<p>The model feeds task embedding to modulate the characteristic of different tasks.Each task has an id ranging from 0 to N. Each task id is assigned to one unique task embedding. The model input contains the corresponding token, segment, position and task embedding. Any task id can initialize model in the fine-tuning process.</p>
<h1 id="results">Results<a class="headerlink" href="#results" title="Permanent link">ðŸ”—</a></h1>
<p align="center"><img alt="ERNIE2res1" src="/articles/2019/Jul/30/ernie-2-0/img/engTasks.png">
<br>
Figure 3. ERNIE 2.0 The results on GLUE benchmark for English tasks.</p>
<p align="center"><img alt="ERNIE2res2" src="/articles/2019/Jul/30/ernie-2-0/img/chnTasks.png">
<br>
Figure 4. ERNIE 2.0 The results of 9 common Chinese NLP tasks.</p>
            </div>
            <!-- /.entry-content -->
<section class="well" id="related-posts">
    <h4>Related Posts:</h4>
    <ul>
        <li><a href="/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/">The Transformer â€“ Attention is all you need.</a></li>
    </ul>
</section>
    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'mchromiak'; // required: replace example with your forum shortname

                var disqus_url = '/articles/2019/Jul/30/ernie-2-0/';

            var disqus_config = function () {
                this.language = "en";
            };

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<div id="aboutme">
        <p>
            <img width="100%" class="img-thumbnail" src="/static_files/img/me.png"/>
        </p>
    <p>
      <strong>About MichaÅ‚ Chromiak</strong><br/>
         PhD in Computer Science by Polish Academy of Sciences (PAS). Focus research on understanding chaos of data. Deeply understanding the phenomena makes it easy, but first you need to learn. Holds two MScs, in Mathematics and in Computer Science.
    </p>
</div><!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://www.linkedin.com/in/michal-chromiak"><i class="fa fa-linkedin-square fa-lg"></i> LinkedIn</a></li>
    <li class="list-group-item"><a href="https://github.com/MichalChromiak"><i class="fa fa-github-square fa-lg"></i> GitHub</a></li>
    <li class="list-group-item"><a href="https://twitter.com/drChromiak"><i class="fa fa-twitter-square fa-lg"></i> Twitter</a></li>
    <li class="list-group-item"><a href="https://www.researchgate.net/profile/Michal_Chromiak"><i class="fa fa-researchgate-square fa-lg"></i> ResearchGate</a></li>
    <li class="list-group-item"><a href="https://scholar.google.pl/citations?user=UeOad3YAAAAJ&hl=en"><i class="fa fa-google-scholar-square fa-lg"></i> Google Scholar</a></li>
    <li class="list-group-item"><a href="localhost:8000/feeds/all.rss"><i class="fa fa-rss-square fa-lg"></i> RSS</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Recent Posts -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Recent Posts</span></h4>
  <ul class="list-group" id="recentposts">
    <li class="list-group-item"><a href="/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/">Decision Transformer: Unifying sequence modelling and model-free, offline RL</a></li>
    <li class="list-group-item"><a href="/articles/2021/May/05/MLP-Mixer/">MLP-Mixer: MLP is all you need... again? ...</a></li>
    <li class="list-group-item"><a href="/articles/2021/May/01/RL-Primer/">RL Primer</a></li>
  </ul>
</li>
<!-- End Sidebar/Recent Posts -->

<!-- Sidebar/Categories -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Categories</span></h4>
  <ul class="list-group" id="categories">
    <li class="list-group-item">
      <a href="/category/applications.html"><i class="fa fa-folder-open fa-lg"></i>Applications</a>
    </li>
    <li class="list-group-item">
      <a href="/category/computer-vision.html"><i class="fa fa-folder-open fa-lg"></i>Computer Vision</a>
    </li>
    <li class="list-group-item">
      <a href="/category/ml-dojo.html"><i class="fa fa-folder-open fa-lg"></i>ML Dojo</a>
    </li>
    <li class="list-group-item">
      <a href="/category/reinforcement-learning.html"><i class="fa fa-folder-open fa-lg"></i>Reinforcement learning</a>
    </li>
    <li class="list-group-item">
      <a href="/category/sequence-models.html"><i class="fa fa-folder-open fa-lg"></i>Sequence Models</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Categories -->

<!-- Sidebar/Tag Cloud -->
<li class="list-group-item">
  <a href="/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
  <ul class="list-group list-inline tagcloud" id="tags">
    <li class="list-group-item tag-4">
      <a href="/tag/application.html">application</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/attention-model.html">Attention model</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/basics.html">basics</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/bert.html">BERT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/cv.html">CV</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/elmo.html">ELMo</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/ernie-10.html">ERNIE 1.0</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/language-model.html">language model</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/machine-translation.html">Machine translation</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/markov-decision-process.html">Markov Decision Process</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/mdp.html">MDP</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/mlp.html">MLP</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/ngram.html">ngram</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/nlp.html">NLP</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/nmt.html">NMT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/openai-gpt.html">OpenAI GPT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/patternrecognition.html">PatternRecognition</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/perplexity.html">perplexity</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/reinforcement-learning.html">Reinforcement Learning</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/rl.html">RL</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/seq2seq.html">seq2seq</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/sequence-transduction.html">Sequence transduction</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/smoothing.html">smoothing</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/transformer.html">Transformer</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/vit.html">ViT</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/xlnet.html">XLNet</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Tag Cloud -->

<!-- Sidebar/Links -->
<li class="list-group-item">
  <h4><i class="fa fa-external-link-square fa-lg"></i><span class="icon-label">Links</span></h4>
  <ul class="list-group" id="links">
    <li class="list-group-item">
      <a href="http://www.iclr.cc" target="_blank">ICLR Conf</a>
    </li>
    <li class="list-group-item">
      <a href="http://icml.cc" target="_blank">ICML Conf</a>
    </li>
    <li class="list-group-item">
      <a href="https://nips.cc/" target="_blank">NIPS Conf</a>
    </li>
    <li class="list-group-item">
      <a href="http://aifrontiers.com/" target="_blank">AI Frontiers</a>
    </li>
    <li class="list-group-item">
      <a href="https://developers.google.com/machine-learning/glossary/" target="_blank">ML Glossary</a>
    </li>
    <li class="list-group-item">
      <a href="https://deepdreamgenerator.com/" target="_blank">Deep Dream Generator</a>
    </li>
    <li class="list-group-item">
      <a href="https://deepart.io/" target="_blank">DeepArt Generator</a>
    </li>
    <li class="list-group-item">
      <a href="https://stanfordmlgroup.github.io/" target="_blank">Stanford ML Group Andrew Ng</a>
    </li>
    <li class="list-group-item">
      <a href="https://ai-on.org/" target="_blank">AIâ€¢ON open ML collaboration</a>
    </li>
    <li class="list-group-item">
      <a href="http://java-hive.blogspot.com/" target="_blank">My old blog on Java an SE</a>
    </li>
    <li class="list-group-item">
      <a href="http://karpathy.github.io/2016/09/07/phd/" target="_blank">PhD</a>
    </li>
    <li class="list-group-item">
      <a href="http://alt.qcri.org/semeval2017/" target="_blank">SemEval2017</a>
    </li>
    <li class="list-group-item">
      <a href="https://medium.freecodecamp.org/450-free-online-programming-computer-science-courses-you-can-start-in-september-59712e77635c" target="_blank">Free CS courses</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Links -->

<!-- Sidebar/Archive -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Archive</span></h4>
  <ul class="list-group" id="archive">
        <li class="list-group-item">
          <a href="/archive/2021/Jun/index.html"><i class="fa fa-calendar fa-lg"></i>June 2021 (1)
          </a>
        </li>
        <li class="list-group-item">
          <a href="/archive/2021/May/index.html"><i class="fa fa-calendar fa-lg"></i>May 2021 (2)
          </a>
        </li>
        <li class="list-group-item">
          <a href="/archive/2019/Jul/index.html"><i class="fa fa-calendar fa-lg"></i>July 2019 (1)
          </a>
        </li>
        <li class="list-group-item">
          <a href="/archive/2017/Nov/index.html"><i class="fa fa-calendar fa-lg"></i>November 2017 (1)
          </a>
        </li>
        <li class="list-group-item">
          <a href="/archive/2017/Sep/index.html"><i class="fa fa-calendar fa-lg"></i>September 2017 (2)
          </a>
        </li>
        <li class="list-group-item">
          <a href="/archive/2017/Aug/index.html"><i class="fa fa-calendar fa-lg"></i>August 2017 (1)
          </a>
        </li>
  </ul>
</li>
<!-- End Sidebar/Archive -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2021 MichaÅ‚ Chromiak
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>                <p><small>  <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en"><img alt="Creative Commons License" style="border-width:0" src="//i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a>
    Content
  licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">Creative Commons Attribution-ShareAlike 4.0 International License</a>, except where indicated otherwise.
</small></p>
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="/theme/js/respond.min.js"></script>

    <script src="/static_files/js/custom.js"></script>

    <script src="/theme/js/bodypadding.js"></script>
    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'mchromiak'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics Universal -->
    <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-108394162-1', 'auto');
        ga('send', 'pageview');
    </script>
    <!-- End Google Analytics Universal Code -->


        <script type="text/javascript">var addthis_config = {"data_track_addressbar": true};</script>
    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-59ea3c17b283c631"></script>
    <script src="/static_files/js/article.js"></script>
</body>
</html>