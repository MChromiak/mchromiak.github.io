<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Micha≈Ç Chromiak's blog</title><link href="https://mchromiak.github.io/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>https://mchromiak.github.io/</id><updated>2021-06-01T19:30:00+02:00</updated><subtitle>Be a fool to become a Polymath.</subtitle><entry><title>Decision Transformer: Unifying sequence modelling and model-free, offline RL</title><link href="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/" rel="alternate"></link><published>2021-06-01T19:30:00+02:00</published><updated>2021-06-01T19:30:00+02:00</updated><author><name>Micha≈Ç Chromiak</name></author><id>tag:mchromiak.github.io,2021-06-01:/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/</id><summary type="html">&lt;p&gt;Can we apply massive advancements of Transformer approach with its simplicity and scalability to Reinforcement Learning (RL)? Yes, but for that - one needs to approach RL as a sequence modeling problem. The &lt;em&gt;Decision Transformer&lt;/em&gt; does that by abstracting RL as a &lt;em&gt;conditional sequence modeling&lt;/em&gt; and using language modeling technique of casual masking of self-attention from GPT/BERT, enabling autoregressive generation of &lt;em&gt;trajectories&lt;/em&gt; from the previous tokens in a sequence. The classical RL approach of fitting the &lt;em&gt;value functions&lt;/em&gt;, or computing &lt;em&gt;policy gradients&lt;/em&gt; (needs live correction; online), has been ditched in favor of masked Transformer yielding optimal &lt;em&gt;actions&lt;/em&gt;. The Decision Transformer can match or outperform strong algorithms designed explicitly for offline RL with minimal modifications from standard language modeling architectures.&lt;/p&gt;</summary><content type="html">&lt;h4 id=the-decision-transformer-paper-explained&gt;The Decision Transformer paper explained.&lt;a class=headerlink href=#the-decision-transformer-paper-explained title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;In this article we will explain and discuss the paper:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://arxiv.org/abs/2106.01345"&gt;"Decision Transformer: Reinforcement Learning via Sequence Modeling"&lt;/a&gt;&lt;/strong&gt;: by Chen L. et al, ArXiv&lt;/p&gt;
&lt;p&gt;that explores application of transformers to model sequential decision making problems - formalized as Reinforcement Learning (RL). By training a language model on a training dataset of random walk trajectories, it can figure out optimal trajectories by just conditioning on a large reward.&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="Illustrative example of finding shortest path for a fixed graph (left) posed as reinforcement learning. Training dataset consists of random walk trajectories and their per-node returns-to-go (middle). \label{fig:dtgraph}" src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/DT_OptimalGraphPath.png"&gt;
Figure 1. Conditioned on a starting state and generating largest possible return at each node, Decision Transformer sequences optimal paths.  (&lt;a href="https://arxiv.org/abs/2106.01345"&gt;Source&lt;/a&gt;)&lt;/p&gt;
&lt;h4 id=tldr&gt;TL;DR&lt;a class=headerlink href=#tldr title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The idea is simple. 1) Each modality (return, state, or action) is passed into an embedding network (convolutional encoder for images, linear layer for continuous states). 2) embeddings are processed by an autoregressive transformer model, trained to predict the next action given the previous tokens using a linear output layer.&lt;/li&gt;
&lt;li&gt;Instead of training policy in a  a RL way, authors aim at sequence modeling objective, with use of sequence modeling algorithm, based on Transformer (the Transformer is not crucial here and could be replaced by any other autoregressive sequence modeling algorithm such as LSTM)&lt;/li&gt;
&lt;li&gt;The algorithm is looking for &lt;em&gt;actions&lt;/em&gt; conditioning based on the future (autoregressive) desired &lt;em&gt;reward&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Presented algorithm requires more work/memorizing/learn by the network to get the &lt;em&gt;action&lt;/em&gt; for a given &lt;em&gt;reward&lt;/em&gt; (two inputs: state and reward), comparing to classical RL where the &lt;em&gt;action&lt;/em&gt; is the output based on the maximized &lt;em&gt;reward&lt;/em&gt; for a &lt;em&gt;policy&lt;/em&gt;. One input: &lt;em&gt;state&lt;/em&gt;, to train the &lt;em&gt;policy&lt;/em&gt;. However, authors tested how transformers can cope with this approach minding recent advancements in sequence modeling.&lt;/li&gt;
&lt;li&gt;Decision Transformer is based on &lt;a href="../../../May/01/RL-Primer/#what-is-offline-rl"&gt;offline RL&lt;/a&gt; to learn form historical sequence of (reward, state, action) tuples to output &lt;em&gt;action&lt;/em&gt;, based on imitation of similar past &lt;em&gt;reward&lt;/em&gt; and &lt;em&gt;state&lt;/em&gt; inputs and &lt;em&gt;action&lt;/em&gt; outputs.&lt;/li&gt;
&lt;li&gt;Solution is learning about sequence evolution in a training dataset by learning from agents' past behaviors for similar inputs: &lt;em&gt;state&lt;/em&gt;  and the presently required &lt;em&gt;reward&lt;/em&gt;, to output an &lt;em&gt;action&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limitation:&lt;/strong&gt; If the &lt;em&gt;credit assignment&lt;/em&gt; needs to happen longer than within one context. Thus, when relevant &lt;em&gt;action&lt;/em&gt; for the &lt;em&gt;reward&lt;/em&gt; is outside of the sequence that the Transformer attends to it will fail - as it would be out of the context.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=contribution-of-paper&gt;Contribution of paper:&lt;a class=headerlink href=#contribution-of-paper title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;use the GPT architecture to autoregressively model trajectories&lt;/li&gt;
&lt;li&gt;bypass the need for bootstrapping (one of "deadly triad"&lt;sup id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-1-back&gt;&lt;a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-1 class=simple-footnote title="When one tries to combine time difference (TD) learning (or Bootstrapping), off-policy learning, and function approximations (such as Deep Neural Network), instabilities and divergence may arise."&gt;1&lt;/a&gt;&lt;/sup&gt;) by applying transformer models on collected experience using sequence modeling objective&lt;/li&gt;
&lt;li&gt;enable to avoid discounting factor for future rewards used in TD thus, avoid inducing undesired short-range behaviors&lt;/li&gt;
&lt;li&gt;bridge sequence modeling and transformers with RL&lt;/li&gt;
&lt;li&gt;test if sequence modeling can perform policy optimization by evaluating Decision Transformer on offline RL benchmarks in Atari, OpenAI Gym, and Key-to-Door environments&lt;/li&gt;
&lt;li&gt;DecisionTransformer (&lt;strong&gt;without&lt;/strong&gt; dynamic programming) match, or exceeds performance of SOTA model-free offline RL: &lt;a href="https://arxiv.org/abs/2006.04779"&gt;Conservative Q-Learing (CQL)&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1907.04543"&gt;Random Ensemble Mixture (REM)&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1710.10044"&gt;Quantile Regression Deep Q-Network (QR-DQN)&lt;/a&gt; or behavioral cloning (BC)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=intro&gt;Intro&lt;a class=headerlink href=#intro title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In contrast to supervised learning, where the agent that imitates actions will (by definition) learn at its best to be on pair with the input data (trainer), the Reinforcement Learning can learn how to be much better like in case of AlphaGo (Go), AlphaStar (StarCraft II), AlphaZero (Chess) or OpenAI Five (Dota2).&lt;/p&gt;
&lt;p&gt;Transformer framework is known of its performance, scalability and simplicity, as well as its generic approach. The level of abstraction that is available via transformer framework reduces the &lt;em&gt;inductive bias&lt;/em&gt; and thus, is more flexible and effective in multiple applications relying more on the data than on assumed, hand designed concepts. As this has already been proven in NLP with GPT and BERT models and Computer Vision with Vision Transformer (ViT), authors have adapted transformers to the realm of Reinforcement Learning (RL).&lt;/p&gt;
&lt;p&gt;If you are already familiar with RL terms used in the intro you might want to go directly to the &lt;a href=#motivation&gt;Decision Transformer paper motivation explained&lt;/a&gt;. For more explanation of the terms referred in the paper please see &lt;a href="../../../May/01/RL-Primer"&gt;prerequisite RL knowledge&lt;/a&gt; section.&lt;/p&gt;
&lt;h3 id=motivation&gt;Motivation&lt;a class=headerlink href=#motivation title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Transformer framework has proven to be very efficient in multiple applications in NLP and CV by modeling high-dimensional distributions of semantic concepts at scale.
Due to natural consequence of transformer nature, the input should be in form of a sequence thus, in case of &lt;em&gt;Decision Transformer&lt;/em&gt; the RL has been casted as conditional sequence modeling. Specifically, the sequence modeling (with transformers) for policy optimization in RL is adapted by the Decision Transformer as a strong algorithmic paradigm.&lt;/p&gt;
&lt;p&gt;One very interesting motivation for using transformers is that they can perform &lt;a href="../../../May/01/RL-Primer/#what-is-credit-assignment"&gt;credit assignment&lt;/a&gt; directly via self-attention, in contrast to Bellman backups which slowly propagate rewards and are prone to ‚Äúdistractor‚Äù signals. This might help transformers to work with sparse, or distracting rewards. It even made Decision Transformer (DT) to outperform RL baseline for tasks requiring long term credit assignment due to the usage of long contexts. The DT aims to avoid any inductive bias for credit assignment, e.g. by explicitly learning the reward function or a critic, in favor of natural emergence of such properties thanks to transformer approach.&lt;/p&gt;
&lt;h3 id=objective-or-goal-for-the-algorithm&gt;Objective, or goal for the algorithm&lt;a class=headerlink href=#objective-or-goal-for-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The goal is to investigate if generative trajectory modeling ‚Äì i.e. modeling the joint distribution of the sequence of states, actions, and rewards ‚Äì can serve as a replacement for conventional RL algorithms.&lt;/p&gt;
&lt;h3 id=the-processing-strategy-of-the-algorithm&gt;The processing strategy of the algorithm&lt;a class=headerlink href=#the-processing-strategy-of-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The classic RL is focused on maximizing the reward for a state, by finding an action that actually will maximize the reward. Conversely, the key aspect of DT is to condition on future desired reward. Consequently, algorithm has additional - reward - input, along with the state. This way it is expected not only to maximize, but to fit to the actual reward value. That however, requires more learning/memorization. This is where transformers have proven to work particularly well for sequence modeling. Natural consequence of this design decision is to use the offline RL. This is why the paper is focused on the world of &lt;a href="../../../May/01/RL-Primer/#what-is-model-free-rl"&gt;model-free&lt;/a&gt; and &lt;a href="../../../May/01/RL-Primer/#what-is-offline-rl"&gt;offline RL&lt;/a&gt; algorithms.&lt;/p&gt;
&lt;p&gt;The tasks expects to find the right action for a given state and reward. The offline nature allows the Transformer to look into historical (offline, training dataset) data to find the same state with similar history and about the same reward for future action. From such past experience, similar action will be deduced &lt;sup id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-2-back&gt;&lt;a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-2 class=simple-footnote title="Similarly to behavioral cloning which maximizes the reward based on expert demonstrations.[\ref]. This way based on the past sequence experiences,"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;a name=fig:DT&gt;&lt;/a&gt;&lt;/p&gt;
&lt;script src="https://vjs.zencdn.net/7.11.4/video.min.js"&gt;&lt;/script&gt;
&lt;video id=my-player class="video-js vjs-theme-sea vjs-big-play-centered" controls preload=auto autoplay loop="" muted="" width=640 height=800&gt;
 &lt;source src="../img/dt.webm" type="video/webm"&gt;
 &lt;p class=vjs-no-js&gt;
      To view this video please enable JavaScript, and consider upgrading to a web browser that
      &lt;a href="https://videojs.com/html5-video-support/" target=_blank&gt; supports HTML5 video&lt;/a&gt;
    &lt;/p&gt;
&lt;/video&gt;
&lt;p align=center&gt;Figure 2. Decision Transformer overview (&lt;a href="https://arxiv.org/abs/2106.01345"&gt;Source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;the Transformer outputs an &lt;em&gt;action&lt;/em&gt; that agents (in the past) ended up with, for same &lt;em&gt;state&lt;/em&gt;, while getting similar &lt;em&gt;reward&lt;/em&gt;. Such &lt;em&gt;action&lt;/em&gt; is  what agent is expected to get in current timestep. (See &lt;a href=#fig:DT&gt;Figure 2.&lt;/a&gt;).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The major drawback of this approach is that it is limited to the context sequence length. Thus, in case when agent commenced action more aligned for current state-reward condition, but which is outside of the context (self-attended input of Transformer) - the sequence model will not be able to infer about that, as it only attends to tokens within the limited length context.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Therefore, the DT is better for credit assignment for longer distances than the LSTMs (see &lt;a href=#the-considerations-of-the-DT&gt;more&lt;/a&gt;), but the drawback is that it still does not allow to ditch the classical RL dynamic programming methods, as the are still suitable for the longer-than-context learning problems.&lt;/p&gt;
&lt;p&gt;Another important aspect to note is that with minimal modifications to transformer, DT models trajectories autoregressively.  The goal here is to be able to conditionally generate actions at test time, based on &lt;strong&gt;future&lt;/strong&gt; desired &lt;em&gt;returns&lt;/em&gt; &lt;span class=math&gt;\(\hat{R}\)&lt;/span&gt; (using modeled rewards) rather than past &lt;em&gt;rewards&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;trajectory&lt;/em&gt; - &lt;span class=math&gt;\(\tau\)&lt;/span&gt; - is made of sequence of states, actions and rewards at given timesteps: &lt;span class=math&gt;\(\tau = (s_0, a_0. r_0, s_1, a_1, r_1, \ldots, s_T, a_T, r_T)\)&lt;/span&gt; The return of a trajectory at &lt;span class=math&gt;\(t\)&lt;/span&gt; timestep, is the sum of future rewards of the trajectory starting from &lt;span class=math&gt;\(t\)&lt;/span&gt;: &lt;span class=math&gt;\(R_t = \sum_{t'=t}^T r_{t'}\)&lt;/span&gt; (without discount factor&lt;sup id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-3-back&gt;&lt;a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-3 class=simple-footnote title="The discount factor \(\gamma\) presence (in eg CQL) in the return calculation is a design decision for the problem formulation and it enables stability of the solution at price of discounting more of the future distant rewards than the closer ones in an episode."&gt;3&lt;/a&gt;&lt;/sup&gt;).&lt;/p&gt;
&lt;p&gt;The goal of offline RL is to learn a policy which maximizes the &lt;strong&gt;expected return&lt;/strong&gt; &lt;span class=math&gt;\(\mathbb{E}[\sum_{t=1}^T r_t]\)&lt;/span&gt; in an  Markov Decision Process (MDP).
However, the DT instead of feeding rewards &lt;span class=math&gt;\(R\)&lt;/span&gt; directly, uses (future) &lt;em&gt;returns-to-go&lt;/em&gt; &lt;span class=math&gt;\(\hat{R}\)&lt;/span&gt;. Therefore, the trajectory becomes: &lt;span class=math&gt;\(\tau = (\hat{R}_1, s_1, a_1.  \hat{R}_2, s_2, a_2,  \ldots, \hat{R}_T, s_T, a_T)\)&lt;/span&gt;.
After executing the generated action for the current state, algorithm decrement the target return by the achieved reward and repeat until episode termination.&lt;/p&gt;
&lt;p&gt;The input of the architecture is a state that is encoded with DQN encoder with linear layer to project to the embedding dimension followed by layer normalization. To obtain token embeddings, a linear layer is learned for each modality (return-to-go, state, or action), which projects raw inputs to the embedding dimension. The linear layer is replaced with convolution encoder for environments with visual inputs.
An embedding for each timestep is learned and added to each token ‚Äì  this is different from the standard positional embedding used by transformers, as one timestep corresponds to three tokens (one for each modality: return-to-go, state, or action). The tokens are then processed by a GPT model, which predicts future action tokens via autoregressive modeling.&lt;/p&gt;
&lt;p&gt;The training use a dataset of offline trajectories. Samples minibatches of sequence length K are used from dataset. TThe prediction head corresponding to the input token &lt;span class=math&gt;\(s_t\)&lt;/span&gt; is trained to predict &lt;span class=math&gt;\(a_t\)&lt;/span&gt; either with cross-entropy for discrete actions, or mean-squared for continuous actions while the loses are averaged.  &lt;/p&gt;
&lt;p align=center&gt;&lt;a name=fig:DTalgo&gt;&lt;/a&gt;
&lt;img alt="Decision Transformer Pseudocode (for continuous actions)" src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/DT_algo.png"&gt;
Figure 3. Decision Transformer Pseudocode (&lt;a href="https://arxiv.org/abs/2106.01345"&gt;Source&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=metaphors-or-analogies-to-other-architectures-describing-the-behavior-of-the-algorithm&gt;Metaphors, or analogies to other architectures describing the behavior of the algorithm&lt;a class=headerlink href=#metaphors-or-analogies-to-other-architectures-describing-the-behavior-of-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;There are two main approaches for offline learning that are compared to Decision Transformer: &lt;em&gt;Behavioral Cloning&lt;/em&gt; (BC) and &lt;em&gt;Conservative Q-Learning&lt;/em&gt; (CQL). The BC is to simply mimic behavior in episodes that has lead to good rewards.&lt;/p&gt;
&lt;p&gt;CQL limits the tendency of Q-functions in the offline setting to overestimate the Q-value that results from certain actions.&lt;/p&gt;
&lt;h4 id=the-considerations-of-the-dt&gt;The considerations of the DT&lt;a class=headerlink href=#the-considerations-of-the-dt title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;An important aspect that should be mentioned about DT is related to its offline learning nature. That is, the high dependency on the dataset that is picked as the training set of episodes. In the paper, the dataset is based on experience of DQN high performing agent, as an active reinforcement learner. This way the source episodes in dataset are not a random demonstrations, but are more of a set of &lt;strong&gt;expert&lt;/strong&gt; demonstrations.&lt;/p&gt;
&lt;p&gt;Additionally, DT is a sequence model and not an RL model. Interestingly, the transformer part of DT, could be replaced with any other autoregressive sequence modeling architecture such as LSTM.  In which case the past (state, action, reward) tuples can get propagated as a hidden representation, one step at a time, while producing next action (or a Q-value in case of Q-learning) that results with a certain reward.&lt;/p&gt;
&lt;p&gt;The LSTMs finds the decisive action that lead to the expected result (during training) in a sequence of actions. As we are looking for this most meaningful actions to gain the expected final state, it is not the last action  in the sequence of state changing actions, that might be the best. The goal of RL is to assign the highest reward to this most meaningful action in an episode. This way in future this actions will be favored.&lt;/p&gt;
&lt;p&gt;For LSTMs this requires to go back though the episode sequence of states (and actions) deeper with Back Propagation Though Time (BPTT)&lt;sup id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-4-back&gt;&lt;a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-4 class=simple-footnote title=" one trains recurrent neural networks by unrolling the network into a virtual feed forward network, and applying the backpropagation algorithm to that. This method is called Back-Propagation-Through-Time (BPTT), as it requires propagation of gradients backwards in time."&gt;4&lt;/a&gt;&lt;/sup&gt; which limits how far back the error is propagated (to at most the length of the trajectory) because it is computationally expensive, as the number of timesteps increase in LSTM (as a RNN variant).&lt;/p&gt;
&lt;p&gt;Therefore, it is a very hard (esp. in architectures like RNN/LSTM) task to do the long-range time credit assignments.&lt;/p&gt;
&lt;p&gt;To solve this problem a &lt;a href="https://www.sciencedirect.com/science/article/pii/B9781558601413500304"&gt;dynamic programming&lt;/a&gt; is used in RL.  Therefore, instead of having to just learn from the reward and assign it to an action, each timestep will output not only an &lt;em&gt;action&lt;/em&gt;, but also a &lt;em&gt;value&lt;/em&gt; (similarly as Q-function already returns a value in Q-learning). The algorithm that does this is called Temporal Difference learning.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;value&lt;/em&gt; is the expected/estimated return (final reward in the future) of the episode, starting from the given state, and following till the final state (return) is reached. This way, the final return prediction is not the only learning signal, but also this gives predictions of future reward values for the next actions in each of the intermediate, consecutive states in the path that leads to the final return state. In other words, at each state the  &lt;em&gt;value function&lt;/em&gt; not only tries to predict the final return (which is really noisy for states that are far from return), but also train the value function to predict all the values from states that are between the current state and the final one in an episode. Thus, trying to predict the output of the value function.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Bellman (aka backup / update) operator - operation of updating the value of state &lt;span class=math&gt;\(s\)&lt;/span&gt; from the value of other states that could be potentially reached from state &lt;span class=math&gt;\(s\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Bellman equation for the &lt;a href="../../../May/01/RL-Primer/#value-functions"&gt;V-function&lt;/a&gt; (State-Value) simplifies computation:&lt;/p&gt;
&lt;div class=math&gt;\begin{equation}
v^\pi (s)=  \mathbb{E_\pi}[R_{t+1} + \gamma  v(S_{t+1})| S_t = s]
\end{equation}&lt;/div&gt;
&lt;p&gt;Bellman equation decomposes the V-function into two parts, the immediate reward - &lt;span class=math&gt;\(R_{t+1}\)&lt;/span&gt; - plus the discounted future values of successor states. This equation simplifies the computation of the value function. This way, rather than summing over multiple time steps, we find the optimal solution of a complex problem by breaking it down into simpler, recursive subproblems and utilizing the fact that finding their optimal solution to the overall problem depends upon the optimal solution to its subproblems. Which make it a &lt;em&gt;dynamic programming&lt;/em&gt; algorithm similarly to the &lt;a href="../../../May/01/RL-Primer/#temporal-difference-td-learning"&gt;Temporal Difference&lt;/a&gt; approach,&lt;/p&gt;
&lt;p&gt;It works similarly in terms of the &lt;a href="../../../May/01/RL-Primer/#value-functions"&gt;Q-function&lt;/a&gt; (State-Action value function) in Q-learning, where this approach is formalized with the Bellman's recurrence equation.&lt;/p&gt;
&lt;div class=math&gt;\begin{equation}
q_\pi (s,a) = \mathbb{E_\pi}[R_{t+1} + \gamma q_\pi (S_{t+1}, A_{t+1})| S_t=s, A_t=a]
\end{equation}&lt;/div&gt;
&lt;p&gt;Here, State-Action Value of a state can be decomposed into the immediate reward we get on performing a certain action in state &lt;span class=math&gt;\(s\)&lt;/span&gt; and moving to another state  &lt;span class=math&gt;\(s_{t+1}\)&lt;/span&gt;, plus the discounted value of the state-action value of the state &lt;span class=math&gt;\(s_{t+1}\)&lt;/span&gt; with respect to the some action &lt;span class=math&gt;\(a\)&lt;/span&gt; our agent will take from that state on-wards.&lt;/p&gt;
&lt;p&gt;In RL we use the TD learning and Q-functions to allow &lt;em&gt;credit assignment&lt;/em&gt; for long time ranges.&lt;/p&gt;
&lt;p&gt;However, as the DT is a sequence modeling problem there is no need for &lt;em&gt;dynamic programming&lt;/em&gt; techniques such as Temporal Difference (TD) to be applied. This is due to the Transformer nature, that enables to attend (route information) from any sequence element to any other sequence element in a single step. Therefore, technically the &lt;em&gt;credit assignment&lt;/em&gt; can be done in a single step - but under one crucial condition: &lt;strong&gt;as as long as if it fits into context&lt;/strong&gt;.&lt;/p&gt;
&lt;h5 id=limitations&gt;Limitations&lt;a class=headerlink href=#limitations title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h5&gt;
&lt;p&gt;The limitation of this &lt;em&gt;credit assignment&lt;/em&gt; approach is that this works only works for a given length of Transformer input sequence (context). If one would like to predict correlations and do the &lt;em&gt;credit assignment&lt;/em&gt; across longer spans, than the Transformer input sequence (as a context), still the &lt;em&gt;dynamic programming&lt;/em&gt; would be required.&lt;/p&gt;
&lt;h3 id=heuristics-or-rules-of-thumb&gt;Heuristics or rules of thumb&lt;a class=headerlink href=#heuristics-or-rules-of-thumb title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The DT has been tested on the Atari dataset. The &lt;a href=#fig:dtatari&gt;Figure 4.&lt;/a&gt; show that DT outperformed remaining algorithms on most of the games however, with high standard deviation.&lt;/p&gt;
&lt;p align=center&gt;&lt;a name=fig:dtatari&gt;&lt;/a&gt;
&lt;img alt="Gamer-normalized scores for the 1% DQN-replay Atari dataset. We report the mean and variance across 3 seeds. Best mean scores are highlighted in bold. Decision Transformer (DT) performs comparably to CQL on 3 out of 4 games, and outperforms other baselines." src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/dt_atari.png"&gt;
Figure 4. Decision Transformer Pseudocode (&lt;a href="https://arxiv.org/abs/2106.01345"&gt;Source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The lower standard deviation and also mostly better results, comparing  to CQL, are presented for OpenAI Gym considering the continuous control tasks from the D4RL benchmark.&lt;/p&gt;
&lt;p align=center&gt;&lt;a name=fig:dt_openai&gt;&lt;/a&gt;
&lt;img alt="DT results for D4RL datasets." src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/dt_openaigym.png"&gt;
Figure 5. Decision Transformer (DT) outperforms conventional RL algorithms on almost all tasks. (&lt;a href="https://arxiv.org/abs/2106.01345"&gt;Source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Additionally, comparing to behavioral (see &lt;a href=#ig:dt_bc&gt;Figure 6.&lt;/a&gt;) cloning trained on set percent of the experience 10, 25, 40, 100 percent. The results show that BC can give better performance than DT if the choice will select certain percentage not necessarily picking the best &lt;em&gt;trajectories&lt;/em&gt;. To find the right percentage additional hyper-parameter search i requires, while DT is just one run solution.&lt;/p&gt;
&lt;p align=center&gt;&lt;a name=fig:dt_bc&gt;&lt;/a&gt;
&lt;img alt="DT vs BC." src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/dt_openaigym.png"&gt;
Figure 6. Comparison between Decision Transformer (DT) and Percentile Behavior Cloning (%BC). (&lt;a href="https://arxiv.org/abs/2106.01345"&gt;Source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;However, one should note that in each of the experiments the DT hyperparameters are different. For instance, whenever a task is a sparse reward task, the context length is increased to limit the impact of the context boundary limitation as described in previous &lt;a href=#limitations&gt;sections&lt;/a&gt;. To confirm this Table 5 in the paper shows that the longer the context the better performance DT achieves.&lt;/p&gt;
&lt;p&gt;Additionally, in experiments such as the game of Pong, where the reward is known to be at max 21, this fact has been used to condition on the reward of 20 (see Table 8 of the appendix). Therefore, the fact of knowing the reward seems very important to setup the hperparameters correctly for DT.&lt;/p&gt;
&lt;p&gt;The limitation of DT seems to be that you need to know the reward that task is aiming for and according to Figure 4. of the paper putting arbitrary large value for reward is not possible. From that graph, when the green line of expected oracle is aligned with the blue line of the DT outcome, this means that the reward is as expected. However, soon after the received reward is higher than the one observed from the training dataset performance drops.  &lt;/p&gt;
&lt;h4 id=key-to-door&gt;Key-to-Door&lt;a class=headerlink href=#key-to-door title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;One final example is the &lt;em&gt;Key-to-Door&lt;/em&gt; environment, where the use of Transformer with longer context is proving being appropriate for long-term credit assignment.
This however is only limited to the size/length of the context. Interestingly, authors have used entire episode length as the context.&lt;/p&gt;
&lt;p&gt;The process has three phases:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;agent is placed in a room with a key&lt;/li&gt;
&lt;li&gt;agent is placed in an empty room ("distractor" room to allow to forget the information from the first room)&lt;/li&gt;
&lt;li&gt;agent is placed in a room with a door&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The agent receives a binary reward when reaching the door in the third phase, but only if it picked up the key in the first phase. This problem is difficult for credit assignment because credit must be propagated from the beginning to the end of the episode, skipping over actions taken in the middle.&lt;/p&gt;
&lt;p&gt;The &lt;a href=#fig:dt_ktd&gt;Figure 7.&lt;/a&gt; shows that with the DT, if the agent does not pick up the key in the first room, it already knows in second room that the game is lost due to being within the context of Transformer's attention. This knowledge is from the past offline episodes where not picking the key means that the game is lost.&lt;/p&gt;
&lt;p align=center&gt;&lt;a name=fig:dt_ktd&gt;&lt;/a&gt;
&lt;img alt=key-to-door. src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/dt_ktd.png"&gt;
Figure 7. Comparison between Decision Transformer (DT) and Percentile Behavior Cloning (%BC). (&lt;a href="https://arxiv.org/abs/2106.01345"&gt;Source&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=what-classes-of-problem-is-the-algorithm-well&gt;What classes of problem is the algorithm well ?&lt;a class=headerlink href=#what-classes-of-problem-is-the-algorithm-well title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;According to the test results, as long as the context length of the Decision Transformer matches the task expectation, the solutions behaves better than other methods like CQL or BC. However, to achieve this, one needs to know the length of the context in form of a hyperparameter.&lt;/p&gt;
&lt;p&gt;Additionally, for the sake of the performance, knowing the maximum reward is also important.&lt;/p&gt;
&lt;h3 id=common-benchmark-or-example-datasets-used-to-demonstrate-the-algorithm&gt;Common benchmark or example datasets used to demonstrate the algorithm&lt;a class=headerlink href=#common-benchmark-or-example-datasets-used-to-demonstrate-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Decision Transformer can match the performance of well-studied and specialized TD learning algorithms developed for these settings.&lt;/p&gt;
&lt;p align=center&gt;&lt;a name=fig:dt_perf&gt;&lt;/a&gt;
&lt;img alt="DT performance comparison." src="https://mchromiak.github.io/articles/2021/Jun/01/Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence/img/dt_perf.png"&gt;
Figure 8. Results comparin DT to TD learning (CQL) and behavior cloning across Atari, OpenAI Gym, and Minigrid. (&lt;a href="https://arxiv.org/abs/2106.01345"&gt;Source&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;As DT is based on a offline mode, a very important part of offline RL is how do we pick this fixed dataset to model upon. Here it is based on DQN expert learner, so the results are good and far from being random.&lt;/p&gt;
&lt;p&gt;In the paper authors compare DT to two approaches that are used in offline RL setup:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Behavioral Cloning&lt;/em&gt; (BC) where the agent tries to mimic those agents from the dataset, which have gained high rewards for their actions.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Q-Learning&lt;/em&gt; (actually a &lt;em&gt;Conservative Q-Learning&lt;/em&gt;) (CQL). Q-learning is a classic RL algorithm which uses Q-function to estimate Q-values, along the possible state and action space, to maximize the total reward. The Conservative Q-learning (CQL) is more of a pessimistic approach, that helps to mitigate the tendency of Q-function to overestimate the Q-values that one gets from certain actions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=useful-resources-for-learning-more-about-the-algorithm&gt;Useful resources for learning more about the algorithm:&lt;a class=headerlink href=#useful-resources-for-learning-more-about-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2106.01345"&gt;ArXiv Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kzl/decision-transformer"&gt;GitHub implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;h3 id=footnotes&gt;Footnotes:&lt;a class=headerlink href=#footnotes title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;ol class=simple-footnotes&gt;&lt;li id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-1&gt;When one tries to combine &lt;em&gt;time difference&lt;/em&gt; (TD) learning (or Bootstrapping), off-policy learning, and function approximations (such as Deep Neural Network), instabilities and divergence may arise. &lt;a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-1-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-2&gt;Similarly to behavioral cloning which maximizes the reward based on expert demonstrations.[\ref]. This way based on the past sequence experiences, &lt;a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-2-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-3&gt;The &lt;em&gt;discount&lt;/em&gt; factor &lt;span class=math&gt;\(\gamma\)&lt;/span&gt; presence (in eg CQL) in the return calculation is a design decision for the problem formulation and it enables stability of the solution at price of discounting more of the future distant rewards than the closer ones in an episode. &lt;a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-3-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-4&gt; one trains recurrent neural networks by unrolling the network into a virtual feed forward network, and applying the backpropagation algorithm to that. This method is called Back-Propagation-Through-Time (BPTT), as it requires propagation of gradients backwards in time. &lt;a href=#sf-Decision-Transformer-Reinforcement-Learning-via-Sequence-Modeling-RL-as-sequence-4-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><category term="Reinforcement learning"></category><category term="Transformer"></category><category term="Reinforcement Learning"></category><category term="RL"></category><category term="MDP"></category><category term="Markov Decision Process"></category></entry><entry><title>MLP-Mixer: MLP is all you need... again? ...</title><link href="https://mchromiak.github.io/articles/2021/May/06/MLP-Mixer/" rel="alternate"></link><published>2021-05-06T15:54:00+02:00</published><updated>2021-05-06T15:54:00+02:00</updated><author><name>Micha≈Ç Chromiak</name></author><id>tag:mchromiak.github.io,2021-05-06:/articles/2021/May/06/MLP-Mixer/</id><summary type="html">&lt;p&gt;Let's try to answer the question: is it enough to have the FFN MLP, with matrix multiplication routines and scalar non-linearities to compete with modern architectures such as ViT or CNNs? No need for convolution, attention? It sounds that we have been here in the past. However, does it mean that the researchers are lost and go rounding in circles? It turns out that what has changes along the way is the increase in the scale of the resources and the data which originally helped ML and especially DL flourish past 5-7 years ago. We will discuss the paper which proves that MLP based solutions can replace CNN and attention based Transformers with comparable scores at image classification benchmarks and at pre-training/inference costs similar to SOTA models.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="MLP-Mixer general architecture" src="https://mchromiak.github.io/articles/2021/May/06/MLP-Mixer/img/Mixer.png"&gt;
&lt;br&gt;&lt;/p&gt;
&lt;h4 id=tldr&gt;TL:DR&lt;a class=headerlink href=#tldr title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;MLP is all you need to get adequately comparable results to SOTA CNNs and Transformer - while reaching linear complexity in number of input pixels. Conversely to CNNs, MLP is not localized as its filter spans entire spatial area&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The return to this idea deserves some deeper consideration. If you start thinking that researchers have already been here please read:&lt;/p&gt;
&lt;p&gt;(...) &lt;em&gt;As in the games, researchers always tried to make systems that worked the way the researchers thought their own minds worked---they tried to put that
knowledge in their systems---but it proved ultimately counterproductive, and a colossal waste of researcher's time&lt;/em&gt; (...)&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"&gt;The Bitter Lesson form Rich Sutton, March 13. 2019&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=contribution-of-paper&gt;Contribution of paper:&lt;a class=headerlink href=#contribution-of-paper title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Proves that MLP based architecture can be on pair with modern SOTA in terms of accuracy and computational resource trade-off for training and inference.&lt;/li&gt;
&lt;li&gt;Mixer has linear computation complexity in number of input pixels, in contrast to ViT and similarly to CNNs.&lt;/li&gt;
&lt;li&gt;Unlike ViT, no need for position embedding (token-mixing MLP is sensitive to the order of input tokens)&lt;/li&gt;
&lt;li&gt;Uses standard classification head with global average pooling followed by linear classifier.&lt;/li&gt;
&lt;li&gt;Mixer-MLP scales better than attention based counterparts for bigger training sets to an extend that it  is on pair with them&lt;sup id=sf-MLP-Mixer-1-back&gt;&lt;a href=#sf-MLP-Mixer-1 class=simple-footnote title=" One interesting question is what would happen with even bigger training sets than the ones used in experiments? Would Mixer supersede remaining SOTA architectures?"&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Raise questions on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How useful would it be to compare the features learned from CNN/Transformer solutions and those learned from such a simple architecture?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Consequently:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the role of the inductive biases of such features and how they influence the generalization?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=intro&gt;Intro&lt;a class=headerlink href=#intro title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In this article we will investigate the paper:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;"MLP-Mixer: An all-MLP Architecture for Vision" Ilya Tolstikhin et al. 2021&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2105.01601"&gt;ArXiv - Submitted on 4 May 2021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The current, established computer vision architectures are based on CNNs and attention. The self-attention oriented modern Vision Transformer (ViT) models relies heavily on learning from raw data. The idea presented in the paper is simply to apply MLP repeatedly for spacial locations and feature channels.&lt;/p&gt;
&lt;h3 id=motivation&gt;Motivation&lt;a class=headerlink href=#motivation title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The Transformer/CNN trend in the area has dominated research in terms of SOTA results. Authors claim that the paper goal is to initiate discussion and opening questions on how the feature learned from both the MLP and present dominating approaches can be compared? It is also interesting how the induced biases contained within the features compares and influence the ability to generalize.&lt;/p&gt;
&lt;h3 id=the-processing-strategy-of-the-algorithm&gt;The processing strategy of the algorithm&lt;a class=headerlink href=#the-processing-strategy-of-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Similarly to the  &lt;a href="https://arxiv.org/abs/2105.01601"&gt;&lt;em&gt;Vision Transformer&lt;/em&gt;&lt;/a&gt;. Each input image is divided into a grid of linearly projected, non-overlapping &lt;span class=math&gt;\(16\times 16\)&lt;/span&gt; pixel patches (&lt;em&gt;tokens&lt;/em&gt;). Patches are put into one table of &lt;span class=math&gt;\(patches\times channels\)&lt;/span&gt; as an input for the architecture. This is conversely to previous architectures, where patches would be unrolled into one, single vector of consecutive patches.&lt;/p&gt;
&lt;p&gt;The patches are basic parts that the architecture is working on as they are propagated through the network. The Mixer stacked layers are all of the same size. This is unlike CNNs where we shrink the resolution, but increase the channels.   &lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="MLP-Mixer general architecture" src="https://mchromiak.github.io/articles/2021/May/06/MLP-Mixer/img/MLP-Mixer.png"&gt;
&lt;br&gt;
Figure 1. MLP-Mixer general architecture.&lt;/p&gt;
&lt;p&gt;At first every patch is feed through per-patch, fully connected layer, providing the latent vector representation (embeddings) per each patch. This is then passed to multiple &lt;em&gt;Mixer layers&lt;/em&gt; of the same size ‚Äì each layer build out of two MLP blocks.&lt;/p&gt;
&lt;p&gt;There are two types of Mixer-MLP blocks that acts on rows and columns of the input table:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Token-Mixing&lt;/strong&gt; ; &lt;span class=math&gt;\(Channels \times Patches\)&lt;/span&gt; (acts on columns) enables &lt;em&gt;communication across different spacial locations&lt;/em&gt; (tokens) of an image present in a single channel. A single column of the input table collects all tokens‚Äô parts (across different patches) of a single channel.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Channel-Mixing&lt;/strong&gt; &lt;span class=math&gt;\(Patches \times Channels\)&lt;/span&gt; (acts on rows); enables communication across different channels as their input is an individual token as a row from the input table.    &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;MLP-Mixer use the idea of small kernel convolutions. The channel-mixing MLP_2 with &lt;span class=math&gt;\(1\times1\)&lt;/span&gt; convolutions becomes effectively dense matrix multiplication applied independently to each spacial location. However, it does not allow aggregation of cross-spacial information. This motivates the token-mixing MLP_1 phase that includes the dense matrix multiplications applied to each feature/channel across all spatial locations.&lt;/p&gt;
&lt;h4 id=the-mlp-block-structure&gt;The MLP block structure&lt;a class=headerlink href=#the-mlp-block-structure title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;The latent representation of patches form the fully-connected layer is first feed into the &lt;em&gt;token-mixer&lt;/em&gt; in a &lt;span class=math&gt;\(channel x patch\)&lt;/span&gt; table. Whit that in mind, it takes all the patches (columns) per channel (row) and feeds it to the  &lt;em&gt;MLP1&lt;/em&gt; (two fully connected layers connected with GELU). Each MLP consists of two fully-connected layers and a non-linearity - GELU -  applied independently to each row of its input data tensor.&lt;/p&gt;
&lt;p&gt;The Mixer approach, is based on the idea that channels in individual patches mean similar things. It is because they come from same function of the per-patch fully-connected layer. This first layer put same object information into same channels for different patches. Thus, as they mean similar things, we will group them by channels, and aggregate over all patches within each channel. Therefore each channel have same information in form of feature.&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="MLP-Mixer layer details" src="https://mchromiak.github.io/articles/2021/May/06/MLP-Mixer/img/MLP-Mixer-layer.png"&gt;
&lt;br&gt;
Figure 2. Detailed architecture schema of MLP-Mixer layer.&lt;/p&gt;
&lt;p&gt;Each of the patches is feed through the same &lt;em&gt;MLP1&lt;/em&gt; with shared weights across all columns of the input table. Thus, we are dealing here with a dense matrix multiplication with a weight sharing across same channel of different patches&lt;sup id=sf-MLP-Mixer-2-back&gt;&lt;a href=#sf-MLP-Mixer-2 class=simple-footnote title="Tying parameters across channels is less common with contrast to previous solutions based on CNNs and self-attention."&gt;2&lt;/a&gt;&lt;/sup&gt;. In token-mixing, the &lt;em&gt;MLP_1&lt;/em&gt; share the same kernel (of full receptive field) for all of the channels. Therefore, if a channel reacts as a feature across the patches, it is easy to aggregate all the patches that include this feature.&lt;/p&gt;
&lt;p&gt;Second stage is to transpose back to &lt;span class=math&gt;\(patches x channels\)&lt;/span&gt; and repeat same  computation in &lt;em&gt;MLP_2&lt;/em&gt; however, this time with weight sharing across all patches (rows of the input table). This kind of sharing provides positional invariance, which happens to also be part of CNNs. For each patch we apply computation across all of the channels (features) of that patch.&lt;/p&gt;
&lt;p&gt;Other components include: skip-connections, dropout, layer norm on the channels, and linear classifier head&lt;/p&gt;
&lt;h3 id=objective-or-goal-for-the-algorithm&gt;Objective or goal for the algorithm&lt;a class=headerlink href=#objective-or-goal-for-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The general idea of the MLP-Mixer is to separate the &lt;em&gt;channel-mixing&lt;/em&gt; (per-location) operations and the &lt;em&gt;cross-location&lt;/em&gt; (token-mixing) operations. This is what distinguish it from the solutions described in following section.
This way if we aggregate among the same channels (token-mixing) then if a channel reacts across the patches we can aggregate all the patches that have that feature because the feature producing map was shared.&lt;/p&gt;
&lt;p&gt;Mixer computational complexity is linear in the number of input patches (input sequence length) in token-mixing, conversely to the previous ViT architecture - whose computational complexity is quadratic. Additionally, as the channel hidden width in channel-mixing, is also independent of the patch size, the overall complexity is &lt;strong&gt;linear&lt;/strong&gt; in the number of pixels. Which is also the case with CNNs.&lt;/p&gt;
&lt;h3 id=metaphors-or-analogies-to-other-architectures-describing-the-behavior-of-the-algorithm&gt;Metaphors, or analogies to other architectures describing the behavior of the algorithm&lt;a class=headerlink href=#metaphors-or-analogies-to-other-architectures-describing-the-behavior-of-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The MLP-Mixer does not use the convolutions, nor self-attention - which are heavily used by the current successful architectures.&lt;/p&gt;
&lt;p&gt;Reference to remaining, modern vision architectures can be based on two techniques of layers that mix features at:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;one spatial location&lt;/li&gt;
&lt;li&gt;across different spatial locations,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both of those can be applied exclusively, or at once.&lt;/p&gt;
&lt;p&gt;In CNNs - deeper layers have larger receptive field ‚Äì with &lt;span class=math&gt;\(N \times N, N&amp;gt;1\)&lt;/span&gt; convolutions and pooling implement the second approach while with &lt;span class=math&gt;\(1 \times 1\)&lt;/span&gt; would use first one.  &lt;br&gt;
The Mixer architecture is a very special case of CNN with &lt;span class=math&gt;\(1 \times 1\)&lt;/span&gt; convolutions in channel-mixing, and for token-mixing it is a single-channel depth-wise convolution of a full receptive field with parameter sharing. Conversely, CNNs are not special cases of MLP-Mixer as the plain matrix multiplication of Mixer is less complex than costly convolutions specialized implementation.&lt;/p&gt;
&lt;p&gt;While referring to the attention-based architectures such as Vision Transformers, the self-attention enables both techniques while the MLP blocks are focusing only on the first one.&lt;/p&gt;
&lt;p&gt;The Mixer architecture embrace both techniques (using MLPs), but separates them clearly: the first, per-location operations is implemented in &lt;em&gt;channel-mixing&lt;/em&gt;, and the cross-location operations are implemented with the &lt;em&gt;token-mixing&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Similarly to Transformers, each layer in Mixer (except for the initial patch projection layer) takes an input of the same (aka &lt;em&gt;isotropic&lt;/em&gt;) size. This is in contrast to typical CNN that have a pyramidal structure: deeper layers have a lower resolution input, but more channels&lt;sup id=sf-MLP-Mixer-3-back&gt;&lt;a href=#sf-MLP-Mixer-3 class=simple-footnote title="One can also refer to one of the non-typical designs, of other combinations such as isotropic ResNets and pyramidal ViTs."&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h3 id=discussion-is-mlp-mixer-a-simple-a-cnn-only-with-the-convolution-weights-being-decided-by-attention&gt;Discussion:  Is MLP-Mixer a simple a CNN only with the convolution weights being decided by attention?&lt;a class=headerlink href=#discussion-is-mlp-mixer-a-simple-a-cnn-only-with-the-convolution-weights-being-decided-by-attention title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;One can argue that this is simply a convolution in form of the "image patches" and that due to that the only thing that is proven here is that: the larger the receptive field is - the better results we can get. In which case this would be simply a &lt;span class=math&gt;\(16\times16\)&lt;/span&gt; non-overlapping filters (stride 16). Moreover, then the architecture would be simply an MLP over kernel&lt;/p&gt;
&lt;p&gt;However, if we take a closer look, the CNN is localized, so as such the filter is not covering the whole spacial area, with the Mixer it does.&lt;/p&gt;
&lt;h3 id=heuristics-or-rules-of-thumb&gt;Heuristics or rules of thumb&lt;a class=headerlink href=#heuristics-or-rules-of-thumb title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Authors have commenced series of tests with couple of parameters. The patch resolution e.g. &lt;span class=math&gt;\(16 \times 16\)&lt;/span&gt;. The scale expressed in number of mixer layers: &lt;strong&gt;S&lt;/strong&gt;mall: 8, &lt;strong&gt;B&lt;/strong&gt;ig: 16, &lt;strong&gt;L&lt;/strong&gt;arge: 24 and &lt;strong&gt;H&lt;/strong&gt;uge: 32 following the &lt;a href="https://arxiv.org/abs/2105.01601"&gt;&lt;em&gt;Vision Transformer (ViT)&lt;/em&gt;&lt;/a&gt; paper approach&lt;sup id=sf-MLP-Mixer-4-back&gt;&lt;a href=#sf-MLP-Mixer-4 class=simple-footnote title="One of the authors has actually also coauthored that ViT paper."&gt;4&lt;/a&gt;&lt;/sup&gt;.      &lt;/p&gt;
&lt;p&gt;The results of the experiments shows that the MLP-Mixer is comparable with current SOTA however, is not as good. Therefore, authors have found a metric that distinguish the Mixer architecture. It is the &lt;em&gt;Throuput&lt;/em&gt; (column 5 in the below figure) defined as number of images per second per computation core.&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="MLP-Mixer layer details" src="https://mchromiak.github.io/articles/2021/May/06/MLP-Mixer/img/Mixer-Throughput.png"&gt;
&lt;br&gt;
Figure 3. Transfer performance, inference throughput, and training cost.Sorted by inference throughput.&lt;/p&gt;
&lt;p&gt;Additionally, authors have found out that on a very particular task of training linear 5-shoot classifier on frozen representation of what the model outcomes evaluated on Top-1 accuracy.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: Top-1 accuracy is the conventional accuracy, model prediction (the one with the highest
probability) must be exactly the expected answer. It measures the proportion of examples for
which the predicted label matches the single target label.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This has been evaluated, and discussed in context of the role of model scale.&lt;/p&gt;
&lt;h4 id=scalability-mixer-is-catching-up&gt;Scalability - Mixer is catching up&lt;a class=headerlink href=#scalability-mixer-is-catching-up title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;With training set size increase the authors state that the scalability of Mixer is much more favorably comparing to big transfer (BiT) that plateaus and is on pair with ViT. Moreover, in all cases the differences in the scalability performance of Mixer is getting very close, if not same, as remaining models.&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="MLP-Mixer layer details" src="https://mchromiak.github.io/articles/2021/May/06/MLP-Mixer/img/Top1-scalability.png"&gt;
&lt;br&gt;
Figure 4. Transfer performance, inference throughput, and training cost.Sorted by inference throughput.&lt;/p&gt;
&lt;p&gt;This is the most significant result, but authors compare also some more tasks, for details please refer to the paper.
The conclusion from the results however might be, that the smaller the training set, the worse the Mixer performs in terms of pre-training, and the bigger training set, the gap between the Mixer and remaining architectures (ViT, BiT) shrinks.&lt;/p&gt;
&lt;h4 id=weight-visualization&gt;Weight visualization&lt;a class=headerlink href=#weight-visualization title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;The weight visualization analysis confirm the general assumption on how we observe the neural network work. In the image we see that lower layers learn first the most general features (left), then go to bit larger ones (middle) and the detailed (right).&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="MLP-Mixer weight visualization" src="https://mchromiak.github.io/articles/2021/May/06/MLP-Mixer/img/MLP-Mixer-Weights.png"&gt;
&lt;br&gt;
Figure 5. MLP-Mixer input weights to hidden units visualization in token-mixing MLPs of Mixer-B/16 model trained on JFT-300M proprietary Google dataset used for training image classification models.&lt;/p&gt;
&lt;p&gt;Interestingly the weights visualization differs based on the type of the training set that authors have used:&lt;/p&gt;
&lt;h3 id=what-classes-of-problem-is-the-algorithm-well-suited&gt;What classes of problem is the algorithm well suited?&lt;a class=headerlink href=#what-classes-of-problem-is-the-algorithm-well-suited title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The architecture seems useful especially for big scale training sets of vision tasks - as tested according to the description in next section. However, it might as well be applied to some NLP tasks however, with adequately prepared text ‚Äúpatches‚Äù.  &lt;/p&gt;
&lt;h3 id=common-benchmark-or-example-datasets-used-to-demonstrate-the-algorithm&gt;Common benchmark or example datasets used to demonstrate the algorithm&lt;a class=headerlink href=#common-benchmark-or-example-datasets-used-to-demonstrate-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Comparing to CNN or attention based is claimed to be on pair with the existing SOTA methods in terms of the trade-off between accuracy and computational resources required for training and inference.
Authors have testesd downstream tasks such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ILSVRC2012 ‚ÄúImageNet‚Äù (1.3M training examples, 1k classes) with the original validation labels and cleaned-up ReaL labels,&lt;/li&gt;
&lt;li&gt;CIFAR-10/100 (50k examples, 10/100 classes),&lt;/li&gt;
&lt;li&gt;Oxford-IIIT Pets (3.7k examples, 36 classes), and&lt;/li&gt;
&lt;li&gt;Oxford Flowers-102 (2k examples, 102 classes) and also evaluate on the&lt;/li&gt;
&lt;li&gt;Visual Task Adaptation Benchmark (VTAB-1k), which consists of 19 diverse datasets, each with 1k training examples.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=footnotes&gt;Footnotes:&lt;a class=headerlink href=#footnotes title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;ol class=simple-footnotes&gt;&lt;li id=sf-MLP-Mixer-1&gt; One interesting question is what would happen with even bigger training sets than the ones used in experiments? Would Mixer supersede remaining SOTA architectures? &lt;a href=#sf-MLP-Mixer-1-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-MLP-Mixer-2&gt;Tying parameters across channels is less common with contrast to previous solutions based on CNNs and self-attention. &lt;a href=#sf-MLP-Mixer-2-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-MLP-Mixer-3&gt;One can also refer to one of the non-typical designs, of other combinations such as &lt;a href="https://arxiv.org/abs/1909.03205"&gt;isotropic ResNets&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2102.12122"&gt;pyramidal ViTs&lt;/a&gt;. &lt;a href=#sf-MLP-Mixer-3-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-MLP-Mixer-4&gt;One of the authors has actually also coauthored that ViT paper. &lt;a href=#sf-MLP-Mixer-4-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><category term="Computer Vision"></category><category term="MLP"></category><category term="Transformer"></category><category term="CV"></category><category term="ViT"></category></entry><entry><title>RL Primer</title><link href="https://mchromiak.github.io/articles/2021/May/01/RL-Primer/" rel="alternate"></link><published>2021-05-01T19:30:00+02:00</published><updated>2021-05-01T19:30:00+02:00</updated><author><name>Micha≈Ç Chromiak</name></author><id>tag:mchromiak.github.io,2021-05-01:/articles/2021/May/01/RL-Primer/</id><summary type="html">&lt;p&gt;The objective of RL is to maximize the reward of an agent by taking a series of actions in response to a dynamic environment. Breaking it down, the process of Reinforcement Learning involves these simple steps: Observation of the environment, deciding how to act using some strategy, acting accordingly&lt;/p&gt;</summary><content type="html">&lt;p&gt;Receiving a reward or penalty, learning from the experiences and refining our strategy, iterate until an optimal strategy is found. RL by itself is quite complex area of AI and by this requires some terms to be explained.&lt;/p&gt;
&lt;h3 id="prerequisite-rl-knowledge"&gt;Prerequisite RL knowledge&lt;a class="headerlink" href="#prerequisite-rl-knowledge" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Here are explained some of the main terms that this paper uses to motivate its goal. Form more details please refer to free online book from &lt;a href="http://incompleteideas.net/book/the-book.html"&gt;Richard S. Sutton
and Andrew G. Barto Second &lt;em&gt;Reinforcement Learning: An Introduction&lt;/em&gt; Edition MIT Press, Cambridge, MA, 2018&lt;/a&gt;&lt;/p&gt;
&lt;h4 id="what-is-model-free-rl"&gt;What is &lt;em&gt;Model-free&lt;/em&gt; RL?&lt;a class="headerlink" href="#what-is-model-free-rl" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;A ‚Äúpolicy‚Äù, is a strategy that an agent uses to pursue a goal.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Model-free algorithm:
estimates the optimal policy without using or estimating the dynamics (transition and reward functions) of the environment.&lt;/p&gt;
&lt;p&gt;Model-based algorithm:
uses the transition function (and the reward function) in order to estimate the optimal policy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In an &lt;strong&gt;online RL&lt;/strong&gt; there is an &lt;em&gt;Agent&lt;/em&gt; and the &lt;em&gt;Environment&lt;/em&gt;. Agent &lt;span class="math"&gt;\(A\)&lt;/span&gt; performs actions &lt;span class="math"&gt;\(a\)&lt;/span&gt; in environment &lt;span class="math"&gt;\(E\)&lt;/span&gt;, and the &lt;span class="math"&gt;\(E\)&lt;/span&gt; response with the &lt;em&gt;reward&lt;/em&gt; &lt;span class="math"&gt;\(r\)&lt;/span&gt; and &lt;em&gt;observation&lt;/em&gt; &lt;span class="math"&gt;\(o\)&lt;/span&gt; (or a &lt;em&gt;state&lt;/em&gt; &lt;span class="math"&gt;\(s\)&lt;/span&gt; if it is not partially observable environment). Additionally a &lt;em&gt;policy&lt;/em&gt; (network) is the algorithm used by the agent to decide its actions. This is the part that can be &lt;em&gt;model-based&lt;/em&gt; or &lt;em&gt;model-free&lt;/em&gt;. Hence, agents gets to actively (interactively) interact with environment to maximize the reward with policy.&lt;/p&gt;
&lt;p&gt;Policy can be model-based or model-free. Question is how to optimize the policy network via &lt;em&gt;policy gradient&lt;/em&gt; (PG)?&lt;/p&gt;
&lt;p&gt;PG algorithms directly try to optimize the policy to increase rewards, however this directly needs correction from environment in online fashion.&lt;/p&gt;
&lt;h4 id="markov-decision-processes-mdp"&gt;Markov decision processes (MDP).&lt;a class="headerlink" href="#markov-decision-processes-mdp" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;MDP is a process with a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from state A to state B is fixed.&lt;/p&gt;
&lt;h4 id="what-is-offline-rl"&gt;What is &lt;em&gt;Offline&lt;/em&gt; RL?&lt;a class="headerlink" href="#what-is-offline-rl" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;In &lt;strong&gt;offline&lt;/strong&gt; RL there is an agent and (instead of interactive environment) a limited, fixed dataset. The dataset contains experience (trajectory rollouts of arbitrary policies) from other scenarios where agents were learning a &lt;em&gt;policy&lt;/em&gt; to gain a good &lt;em&gt;reward&lt;/em&gt;. In contrast to online RL, such setup is more challenging as there is no dynamic environment to test hypothesis, and all is left is to have a set of &lt;em&gt;trajectories&lt;/em&gt; without live feedback. By observing historical episodes of interaction from other agents, an offline agent needs to learn a good policy to achieve a high reward.&lt;/p&gt;
&lt;h4 id="reward-return"&gt;Reward /return&lt;a class="headerlink" href="#reward-return" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Reward&lt;/strong&gt;
Reward is the quantity received from the environment in a given timestep as a result of an action&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Return&lt;/strong&gt;:
Return is defined as a function of reward sequence, which can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;simple sum of rewards (also called cumulative reward), or a&lt;/li&gt;
&lt;li&gt;sum of &lt;em&gt;discounted&lt;/em&gt; rewards (also called &lt;em&gt;cumulative future discounted reward&lt;/em&gt;):&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a name="tdr"&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$R_t=r_{t+1}+\gamma r_{t+2}+ \gamma^2 r_{r+3}+\ldots = \sum\limits_{k=0}^{\infty}\gamma^kr_{t+k+1}, \gamma \in [0,1]$$&lt;/div&gt;
&lt;p&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"Cumulative" refers to the summation.&lt;/li&gt;
&lt;li&gt;"Future" refers to the fact that it's an expected value of all the future timesteps values until the end of the episode with respect to the present quantity.&lt;/li&gt;
&lt;li&gt;"Discounted" refers to the "gamma" &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; &lt;em&gt;discount rate&lt;/em&gt; factor, which is a way to adjust the importance of how much we value rewards at future time steps, i.e. starting from &lt;span class="math"&gt;\(t+1\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;"Reward" refers to the main quantity of interest, i.e. the reward received from the environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The goal of an RL algorithm is to select actions that maximize the expected cumulative reward (the return) of the agent.&lt;/p&gt;
&lt;h4 id="what-is-credit-assignment"&gt;What is &lt;em&gt;Credit Assignment&lt;/em&gt;?&lt;a class="headerlink" href="#what-is-credit-assignment" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;In real world RL the state (and action) space usually needs to be very fine to cover all possibly relevant situations. This leads to the combinatorial explosion of states and actions which is referred as &lt;em&gt;curse of dimensionality&lt;/em&gt;.
As an agent interacts with an environment in discrete timesteps, agent takes an action for current state and the environment emits a perception in form of a &lt;em&gt;reward&lt;/em&gt; and an &lt;em&gt;observation&lt;/em&gt;. In case of fully observable Markov Decision Process (MDP) it is the next state (of the environment and the agents). Agent's goal is to maximize the reward. In such &lt;strong&gt;fine grained&lt;/strong&gt; state-action spaces the reward occur terribly temporally delayed.&lt;/p&gt;
&lt;p&gt;The (temporal) &lt;em&gt;credit assignment problem&lt;/em&gt; (CAP) is the problem of determining the actions that lead to a certain outcome. The problem of determining the contribution of each agent to the result of the training is the (temporal) CAP. In order to maximize the reward in the long run, the agent needs to determine which actions will lead to such outcome, which is essentially the temporal CAP. This way, an action that leads to a higher final cumulative reward should have more &lt;em&gt;credit&lt;/em&gt; (value) than an action that lead to a lower final reward. For instance, in Q-learning (the &lt;em&gt;off-policy&lt;/em&gt; algorithm) agents attempts to determine actions that will lead to the highest value in each state.&lt;/p&gt;
&lt;p&gt;In RL, due to CAP, reward signals will only very weakly affect all temporally distant states that have preceded it. The influence of a reward gets more and more diluted over time and this can lead to bad convergence properties of the RL mechanism. Many steps must be performed by an iterative RL algorithm to propagate the influence of delayed reinforcement to all states and actions that have an effect on that reinforcement.&lt;/p&gt;
&lt;h4 id="value-functions"&gt;Value functions&lt;a class="headerlink" href="#value-functions" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Q-Learning is about learning Q-function that takes state and action conditioned on the history to predict future rewards.
VF are state-action pair functions that estimate how good a particular action will be in a given state, or what the return for that action is expected to be.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;V-function&lt;/strong&gt; (State-Value) &lt;span class="math"&gt;\(v^\pi (s)=  \mathbb{E_\pi} [\sum_{k=0}^T \gamma^k R_{t+k+1} | S_t = s]\)&lt;/span&gt; &lt;br &gt; &lt;strong&gt;Value&lt;/strong&gt;   of state &lt;span class="math"&gt;\(s\)&lt;/span&gt; under policy/strategy &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;. The expected &lt;strong&gt;return&lt;/strong&gt; while starting at &lt;span class="math"&gt;\(s\)&lt;/span&gt; and following the &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; thereafter. Shows how good a certain state is, in terms of expected cumulative reward, for an agent following a certain policy. The &lt;span class="math"&gt;\(\mathbb{E}[.]\)&lt;/span&gt; because environment state transition function might act in a stochastic way.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q-function&lt;/strong&gt;  (State-Action) &lt;span class="math"&gt;\(q^\pi (s,a) = \mathbb{E_\pi}[\sum_{k=0}^T \gamma^k R_{t+k+1} | S_t = s, A_t = a]\)&lt;/span&gt;  &lt;br &gt;&lt;strong&gt;Quality&lt;/strong&gt;  of taking action &lt;span class="math"&gt;\(a\)&lt;/span&gt; in state &lt;span class="math"&gt;\(s\)&lt;/span&gt; with policy/strategy &lt;span class="math"&gt;\(\pi\)&lt;/span&gt;.  The expected &lt;strong&gt;return&lt;/strong&gt; while starting at &lt;span class="math"&gt;\(s\)&lt;/span&gt; while taking action &lt;span class="math"&gt;\(a\)&lt;/span&gt; and following the &lt;span class="math"&gt;\(\pi\)&lt;/span&gt; thereafter. Shows how good action &lt;span class="math"&gt;\(a\)&lt;/span&gt; is, given a state for agent following a policy. The &lt;span class="math"&gt;\(\mathbb{E}[.]\)&lt;/span&gt; because environment state transition function might act in a stochastic way.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Q-Value&lt;/strong&gt; - value in state-action table.  The Q-function is implemented as a table of states and actions and Q-values for each s,a pair are stored there.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Estimating&lt;/strong&gt; VF for a particular policy, helps to accurately choose an action that will provide the best total reward possible, after being in that given state.&lt;/p&gt;
&lt;h4 id="temporal-difference-td-learning"&gt;Temporal Difference (TD) Learning&lt;a class="headerlink" href="#temporal-difference-td-learning" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Temporal Difference&lt;/em&gt; (TD) (aka &lt;em&gt;bootstrapping&lt;/em&gt; method) solves the problem of &lt;strong&gt;estimating&lt;/strong&gt; &lt;a href="#value-functions"&gt;value function&lt;/a&gt;. If the value functions were to be calculated &lt;strong&gt;without estimation&lt;/strong&gt;, the agent would need to wait until the final reward was received before any state-action pair values can be updated. Once the final reward was received, the path taken to reach the final state would need to be traced back and each value updated accordingly. TD address this issue.&lt;/p&gt;
&lt;p&gt;TD learning is a unsupervised, RL model-free method learning by bootstrapping from current estimate of value function. In TD, agent is learning from an environment through episodes with no prior knowledge of the environment. TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known.&lt;/p&gt;
&lt;p&gt;Instead of calculating the total future reward, at each step, TD tries to predict the combination of immediate reward and its own reward prediction at the next moment in time.&lt;/p&gt;
&lt;p&gt;TD method is called a "bootstrapping" method, because the value is updated partly using an existing estimate and not a final reward.&lt;/p&gt;
&lt;h4 id="onoff-policy-algorithm"&gt;On/Off-policy algorithm&lt;a class="headerlink" href="#onoff-policy-algorithm" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Off-policy algorithm - evaluate and improve a (target) policy that is different from (current) policy which is used for action selection. When passing the reward from the next state to the current state, it takes the maximum possible reward of the new state and ignores whatever policy we are using. Eg. Q-learning is off-policy as it updates its Q-values using the Q-value of the next state and the &lt;em&gt;greedy&lt;/em&gt; action. In other words, it estimates the &lt;strong&gt;return&lt;/strong&gt; (cumulative/total  &lt;a href="#tdr"&gt;discounted return&lt;/a&gt; future reward, starting from current timestep) for state-action pairs assuming a greedy policy were followed, despite the fact that it's not following a greedy policy.&lt;/p&gt;
&lt;p&gt;On-policy algorithm - evaluate and improve the same policy which is being used to select actions, Eg. Sarsa  updates its Q-values using the Q-value of the next state and the current policy's action. It estimates the return for state-action pairs assuming the current policy continues to be followed.&lt;/p&gt;
&lt;h4 id="action-selection-policies"&gt;Action Selection Policies&lt;a class="headerlink" href="#action-selection-policies" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;There are three common policies used for action selection. The aim of these policies is to balance the trade-off between &lt;strong&gt;exploitation&lt;/strong&gt; and &lt;strong&gt;exploration&lt;/strong&gt;, by not always exploiting what has been learned so far.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;greedy - Will lock on one action that happened to have good results at one point of time but it is not in reality the optimal action. So Greedy will keep exploiting this action while ignoring the others which might be better. It Exploits too much.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;-greedy - most of the time the action with the highest estimated reward is chosen, called the greediest action. Every once in a while, say with a small probability &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;, an action is selected at random. The action is selected uniformly, independently of the action-value estimates. This method ensures that if enough trials are done, each action will be tried an infinite number of times, thus ensuring optimal actions are discovered. Explores too much because even when one action seem to be the optimal one, the methods keeps allocating a fixed percentage of the time for exploration, thus missing opportunities and increasing total regret.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;-soft - very similar to &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;-greedy. The best action is selected with probability &lt;span class="math"&gt;\(1 - \epsilon\)&lt;/span&gt; and the rest of the time a random action is chosen uniformly.&lt;/li&gt;
&lt;li&gt;softmax - one drawback of &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;-greedy and &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;-soft is that they select random actions uniformly. The worst possible action is just as likely to be selected as the second best. Softmax remedies this by assigning a rank, or weight to each of the actions, according to their action-value estimate. A random action is selected with regards to the weight associated with each action, meaning the worst actions are unlikely to be chosen. This is a good approach to take where the worst actions are very unfavorable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is not clear which of these policies produces the best results overall. The nature of the task will have some bearing on how well each policy influences learning. If the problem we are trying to solve is of a game playing nature, against a human opponent, human factors may also be influential.&lt;/p&gt;
&lt;h4 id="exploitation-vs-exploration"&gt;Expl&lt;strong&gt;oit&lt;/strong&gt;ation vs explo&lt;strong&gt;r&lt;/strong&gt;ation&lt;a class="headerlink" href="#exploitation-vs-exploration" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Exploitation - keep the current approach. Chooses the greedy action to get the most reward by exploiting the agent‚Äôs current action-value estimates. But by being greedy with respect to action-value estimates, may not actually get the most reward and lead to sub-optimal behaviour.&lt;/p&gt;
&lt;p&gt;Exploration - Try new approach.  Allows an agent to improve its current knowledge about each action, hopefully leading to long-term benefit. Improving the accuracy of the estimated action-values, enables an agent to make more informed decisions in the future.&lt;/p&gt;
&lt;p&gt;When an agent explores, it gets more accurate estimates of action-values. And when it exploits, it might get more reward. It cannot, however, choose to do both simultaneously, which is also called the exploration-exploitation dilemma.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Reinforcement learning"></category><category term="Reinforcement Learning"></category><category term="RL"></category><category term="MDP"></category><category term="Markov Decision Process"></category></entry><entry><title>ERNIE 2.0: A continual pre-training framework for language understanding</title><link href="https://mchromiak.github.io/articles/2019/Jul/30/ernie-2-0/" rel="alternate"></link><published>2019-07-30T19:30:00+02:00</published><updated>2019-07-30T19:30:00+02:00</updated><author><name>Micha≈Ç Chromiak</name></author><id>tag:mchromiak.github.io,2019-07-30:/articles/2019/Jul/30/ernie-2-0/</id><summary type="html">&lt;p&gt;ERNIE 2.0 (&lt;em&gt;Enhanced Representation through kNowledge IntEgration&lt;/em&gt;), a new knowledge integration language representation model that aims to beat SOTA results of BERT and XLNet. While pre-training with more than just several simple tasks to grasp the co-occurrence of words or sentences for language modeling, Ernie aims to explore named entities, semantic closeness and discourse relations to obtain valuable lexical, syntactic and semantic information from training corpora. Ernie 2.0 focus on building and learning incrementally pre-training tasks through constant multi-task learning. And it brings some interesting results.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Transformer based models has become the dominant part of modern SOTA research for NLP. ERNIE 2.0 is no different. Lets deep dive into what novel is about the second version of Ernie.&lt;/p&gt;
&lt;h4 id="tldr"&gt;TL:DR&lt;a class="headerlink" href="#tldr" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Focus attention to more than just co-occurence.&lt;/strong&gt;
Contribution of paper:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Design of continual pre-training framework ERNIE 2.0, which supports customized training tasks and multi-task pre-training in an incremental way.&lt;/li&gt;
&lt;li&gt;ERNIE 2.0 achieves significant improvements over BERT and XLNet on 16 tasks including English GLUE benchmarks and several Chinese tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="intro"&gt;Intro&lt;a class="headerlink" href="#intro" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Co-occurence of words and sentences is the main workhorse of current NLP SOTA solutions. ERNIE 2.0 tries to get a bigger picture with its continuous pre-training.  &lt;/p&gt;
&lt;h3 id="motivation"&gt;Motivation&lt;a class="headerlink" href="#motivation" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;It is the fact that with current solutions, training of the model usually focus on couple of tasks to get the notion of co-occurrence of words and sentences. Those pre-training procedures involved word-level and sentence-level prediction or inference tasks. The closest competitors like Google's BERT captured co-occurrence information by combining a masked language model and a next-sentence prediction task. On the other hand, this co-occurrence information is captured by XLNet by constructing a permutation language model task.&lt;/p&gt;
&lt;p&gt;With Ernie 2.0 however, the vision becomes broader than just co-occurence. This includes more lexical, syntactic and semantic information from training corpora in form of named entities (like person names, location names, and organization names), semantic closeness (proximity of sentences), sentence order or discourse relations. This is done with Ernie 2.0: continual pre-training framework, by building and learning incrementally pre-training tasks  through constant multi-task learning.&lt;/p&gt;
&lt;p&gt;The important aspect is that at any time a new custom task can be introduced, while sharing the same encoding network. Thanks to this, a significant amount of information can be shared across different tasks. Ernie 2.0 framework is continuously pre-training and updating the model and gaining enhancements in information through multi-task learning, hence more holistically understand lexical, syntactic and semantic representations.   &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/1907.12412"&gt; Ernie 2.0: A continual pre-training framework for language understanding. ArXiv&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Code for fine-tuning and pre-trained models for English: &lt;a href="https://github.com/PaddlePaddle/ERNIE"&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;&lt;img alt="ERNIE2" src="https://mchromiak.github.io/articles/2019/Jul/30/ernie-2-0/img/ernie2.gif"&gt;
&lt;br&gt;
Figure 1. A continual pre-training framework for language understanding.&lt;/p&gt;
&lt;h3 id="the-continual-pre-training"&gt;The continual pre-training&lt;a class="headerlink" href="#the-continual-pre-training" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;continual&lt;/em&gt; pre-training consists of two phases:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Continuously construct unsupervised pre-training tasks using the big data and with prior knowledge involved&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This phase include three types of tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;word&lt;/em&gt;-aware tasks: Knowledge Masking, Capitalization Prediction, Token-Document Relation Prediction&lt;/li&gt;
&lt;li&gt;&lt;em&gt;structure&lt;/em&gt;-aware tasks: Sentence Reordering, Sentence Distance&lt;/li&gt;
&lt;li&gt;&lt;em&gt;semantic&lt;/em&gt;-aware tasks: Discourse Relation, IR Relevance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the details of the tasks please refer to the paper. All tasks are based on self/weak-supervised signals without human annotation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Incrementally update the ERNIE model via multi-task learning&lt;/strong&gt;
Continuously, task-by-task new tasks are added starting from first, initial task to train initial model. Then next task's parameters are initialized to the values already learned form the previous one. This way the parameters' "knowledge" is passed on and accumulated within the consecutive tasks.&lt;/p&gt;
&lt;p&gt;This goal is achieved with series of shared text encoding layers that &lt;strong&gt;encodes&lt;/strong&gt; the contextual information possibly customized with RNNs or deep Transformer. The parameters of the encoder are the ones that are updated when a new task is introduced.&lt;/p&gt;
&lt;p&gt;There are two types of loss functions: sequence-level and token-level. For each task sentence-level loss functions can be combined with token-level loss functions creating the task's loss function used to update the model.&lt;/p&gt;
&lt;h4 id="fine-tuning-for-tasks"&gt;Fine tuning for tasks&lt;a class="headerlink" href="#fine-tuning-for-tasks" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;The phase of fine-tuning of the pre-trained model for language understanding tasks (e.g question answering, natural language inference, semantic similarity) is done with task specific supervised data.&lt;/p&gt;
&lt;h3 id="ernie-20-model"&gt;ERNIE 2.0 model&lt;a class="headerlink" href="#ernie-20-model" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The model is similar to BERT or XLNet.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="ERNIE2struct" src="https://mchromiak.github.io/articles/2019/Jul/30/ernie-2-0/img/enrie2struct.png"&gt;
&lt;br&gt;
Figure 2. ERNIE 2.0 model structure. The input embedding consists of the token embedding, the sentence embedding, the position embedding and the task embedding. There are seven pre-training tasks in the ERNIE 2.0 model.&lt;/p&gt;
&lt;h4 id="encoder"&gt;Encoder&lt;a class="headerlink" href="#encoder" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Is a multi-layer Transformer same as in BERT, GPT, XLM. Transformer generate sequence of contextual embeddings for each token. Additional markers are added to the start of the sequence and separator symbol is added to separate intervals for the multiple input segment tasks.&lt;/p&gt;
&lt;h4 id="task-embeddings"&gt;Task embeddings&lt;a class="headerlink" href="#task-embeddings" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;The model feeds task embedding to modulate the characteristic of different tasks.Each task has an id ranging from 0 to N. Each task id is assigned to one unique task embedding. The model input contains the corresponding token, segment, position and task embedding. Any task id can initialize model in the fine-tuning process.&lt;/p&gt;
&lt;h1 id="results"&gt;Results&lt;a class="headerlink" href="#results" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h1&gt;
&lt;p align="center"&gt;&lt;img alt="ERNIE2res1" src="https://mchromiak.github.io/articles/2019/Jul/30/ernie-2-0/img/engTasks.png"&gt;
&lt;br&gt;
Figure 3. ERNIE 2.0 The results on GLUE benchmark for English tasks.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="ERNIE2res2" src="https://mchromiak.github.io/articles/2019/Jul/30/ernie-2-0/img/chnTasks.png"&gt;
&lt;br&gt;
Figure 4. ERNIE 2.0 The results of 9 common Chinese NLP tasks.&lt;/p&gt;</content><category term="Sequence Models"></category><category term="NMT"></category><category term="transformer"></category><category term="Sequence transduction"></category><category term="Attention model"></category><category term="Machine translation"></category><category term="seq2seq"></category><category term="NLP"></category><category term="ELMo"></category><category term="OpenAI GPT"></category><category term="BERT"></category><category term="ERNIE 1.0"></category><category term="XLNet"></category></entry><entry><title>NLP: Explaining Neural Language Modeling</title><link href="https://mchromiak.github.io/articles/2017/Nov/30/Explaining-Neural-Language-Modeling/" rel="alternate"></link><published>2017-11-30T19:30:00+01:00</published><updated>2017-11-30T19:30:00+01:00</updated><author><name>Micha≈Ç Chromiak</name></author><id>tag:mchromiak.github.io,2017-11-30:/articles/2017/Nov/30/Explaining-Neural-Language-Modeling/</id><summary type="html">&lt;p&gt;Language modeling (LM) is the essential part of Natural Language Processing (NLP) tasks such as  Machine Translation, Spell Correction Speech Recognition, Summarization, Question Answering, Sentiment analysis etc. Goal of the Language Model is to compute the probability of sentence considered as a word sequence. This article explains how to model the language using probability and n-grams. It also discuss the language model evaluation with  use of &lt;em&gt;perplexity&lt;/em&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Read before:&lt;/strong&gt;
* &lt;a href="https://medium.com/@gon.esbuyo/get-started-with-nlp-part-i-d67ca26cc828"&gt;Get started with NLP (Part I)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Key Takeaways:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The goal of a language model is to compute a probability of a token (e.g. a sentence or a sequence of words).&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Language Model (LM) actually a grammar of a language as it gives the probability of word that will follow&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dictionary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Corpus&lt;/strong&gt; - Body of text, singular. Corpora is the plural of this. Example: A collection of medical journals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Token&lt;/strong&gt;- Each "entity" that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is "tokenized" into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lexicon&lt;/strong&gt; - Words and their meanings. Example: English dictionary. Consider, however, that various fields will have different lexicons. For example: To a financial investor, the first meaning for thef word "Bull" is someone who is confident about the market, as compared to the common English lexicon, where the first meaning for the word "Bull" is an animal. As such, there is a special lexicon for financial investors, doctors, children, mechanics, and so on.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="intro"&gt;Intro&lt;a class="headerlink" href="#intro" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Language Modeling (LM)&lt;/em&gt; is one of the most important parts of modern Natural Language Processing (NLP). There are many sorts of applications for  Language Modeling, like: Machine Translation, Spell Correction Speech Recognition, Summarization, Question Answering, Sentiment analysis etc. Each of those tasks require use of &lt;em&gt;language model&lt;/em&gt;. Language model is required to represent the text to a form understandable from the machine point of view.&lt;/p&gt;
&lt;p&gt;Below I have elaborated on the means to model a corpus of text to use in multiple machine learning NLP tasks.    &lt;/p&gt;
&lt;h3 id="motivation"&gt;Motivation&lt;a class="headerlink" href="#motivation" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Each part of natural language has to assume some representation. Written text in natural way is composed out of sentences that consist of words that in turn contains letters. Each of those pieces of  text sentence can be a subject for analysis. Of course we can also use morphemes [ref]Small meaningful units that make up words. [\ref] or n-grams of the text.&lt;/p&gt;
&lt;p&gt;One way or another, the text needs to be normalized.  Normalized mean that indexed text and query terms must have same form. For example, &lt;em&gt;U.S.A.&lt;/em&gt; and &lt;em&gt;u.s.a.&lt;/em&gt; are to be considered same (remove dots and case fold - lowercase) and return same result while querying any of them.  Thus, an implicit word classes equivalence must be defined.&lt;/p&gt;
&lt;h4 id="tokenizing"&gt;Tokenizing&lt;a class="headerlink" href="#tokenizing" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Choosing which text segmentation (aka tokenizing) technique will be used is arbitrary, but depends strictly on the undergoing tasks, their specification, heuristics and preconditions. For instance, if we are after a single character statistics of text task we will probably choose to operate on letters not on n-grams or words. The result for text fragmentation is called &lt;strong&gt;token&lt;/strong&gt;. So by referring to a token in general one actually refers to the fragmentation technique outcome in form of a sentence, word, n-gram, morpheme, letter etc.  &lt;/p&gt;
&lt;p&gt;However, even word-oriented tokens might give a different set of distinct tokens. For instance in a lemma based tokens the &lt;em&gt;cat&lt;/em&gt; and plural &lt;em&gt;cats&lt;/em&gt; would count as one word having same stem (core meaning-bearing unit). On the other hand, if we make more strict distinctions, that would e.g. differentiate &lt;em&gt;word form&lt;/em&gt;, we would get two distinct tokens. It all depends how you define a token.&lt;/p&gt;
&lt;p&gt;To make things more clear, lets settle three terms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;vocabulary&lt;/strong&gt; set of different types&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;type&lt;/strong&gt;  an element of vocabulary, and a&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;token&lt;/strong&gt; as an instance of a &lt;em&gt;type&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally, language modeling must consider the correlated ordering of tokens. This is because every language is based on some grammar, where order has a lot of influence on the meaning of a text.  &lt;/p&gt;
&lt;p&gt;To complete the NLP tasks we must provide a measure to enable comparison operations and thus assessment method for grading our model. This measure is probability. This involves all kinds of tasks, for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Machine translation:&lt;/strong&gt; translating a sentence saying about height it would probably state that &lt;span class="math"&gt;\( P(tall\ man) &amp;gt; P(large\ man)\)&lt;/span&gt; as the ‚Äò&lt;em&gt;large&lt;/em&gt;‚Äô might also refer to weight or general appearance thus, not as probable as ‚Äò&lt;em&gt;tall&lt;/em&gt;‚Äô&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Spelling Correction:&lt;/strong&gt;  Spell correcting sentence: ‚ÄúPut you name into form‚Äù, so that &lt;span class="math"&gt;\(P(name\ into\ \textbf{form}) &amp;gt; P(name\ into\ \textbf{from})\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Speech Recognition:&lt;/strong&gt; Call my nurse: &lt;span class="math"&gt;\(P(Call\ my\ nurse.) \gg P(coal\ miners)\)&lt;/span&gt;, I have no idea. &lt;span class="math"&gt;\(P(no\ idea.) \gg P(No\ eye\ deer.)\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Summarization, question answering, sentiment analysis&lt;/strong&gt; etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="the-probabilistic-language-modeling"&gt;The probabilistic language modeling&lt;a class="headerlink" href="#the-probabilistic-language-modeling" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The probability of a sentence &lt;span class="math"&gt;\(S\)&lt;/span&gt; (as a sequence of &lt;em&gt;words&lt;/em&gt; &lt;span class="math"&gt;\(w_i\)&lt;/span&gt;) is : &lt;span class="math"&gt;\(P(S) = P(w_1,w_2, w_3,\ldots,w_n)\)&lt;/span&gt;. Now it is important to find the probability of upcoming word. It is an everyday task made e.g. while typing  your mobile keyboard. We will settle the &lt;em&gt;conditional probability&lt;/em&gt;  of &lt;span class="math"&gt;\(w_4\)&lt;/span&gt; depending on all previous words. For a 4 word sentence this conditional probability is:&lt;/p&gt;
&lt;div class="math"&gt;$$ P(S)=P(w_1, w_2, w_3, w_4) \equiv P(w_4 |w_1, w_2, w3) $$&lt;/div&gt;
&lt;p&gt;This way we can actually represent the language grammar, however, in NLP it is accustomed to use the LM term.&lt;/p&gt;
&lt;p&gt;To calculate the joint probability  of a sentence (as a word sequence) &lt;span class="math"&gt;\(P(W)\)&lt;/span&gt; the chain rule of probability will be used.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Probability Chain Rule:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$P(A|B) = \frac{P(A\cap B)}{P(B)} \implies P(A\cap B)=P(A|B)P(B)$$&lt;/div&gt;
&lt;p&gt;so in general for a token sequence we get:&lt;/p&gt;
&lt;div class="math"&gt;$$P(S)=P(w_1,\ldots,w_n) = P(w_1)P(w_2|w_1) P(w_3)P(w_1,w_2)\ldots P(w_n|w_1,\ldots w_{n-1}) ={\displaystyle \prod_{i} P(w_i|w_1,\ldots w_{i-1})}$$&lt;/div&gt;
&lt;p&gt;To estimate each probability a straightforward solution could be to use simple counting.&lt;/p&gt;
&lt;div class="math"&gt;$$P(w_5|w_1,w_2,w_3,w_4)= \frac{count(w_1,w_2,w_3,w_4,w_5)}{count(w_1,w_2,w_3,w_4)}$$&lt;/div&gt;
&lt;p&gt;but this gives us to many possible sequences to ever estimate. Imagine how much data (occurrences of each sentence) we would have to get to make this &lt;em&gt;count&lt;/em&gt;s meaningful.&lt;/p&gt;
&lt;p&gt;To cope with this issue we can simplify by applying the &lt;strong&gt;Markov Assumption&lt;/strong&gt;, which states that it is enough to pick only one, or a couple of previous words as a prefix:&lt;/p&gt;
&lt;div class="math"&gt;$$P(w_1,\ldots,w_n) \approx {\displaystyle \prod_{i} P(w_i|w_{i-k},\ldots P(w_{i-1}))}$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(k\)&lt;/span&gt; is the number of previous tokens (&lt;em&gt;prefix&lt;/em&gt; size) that we consider.&lt;/p&gt;
&lt;h3 id="n-grams"&gt;N-grams&lt;a class="headerlink" href="#n-grams" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;An N-gram is a contiguous (order matters) sequence of items, which in this case is the words in text. The n-grams depends on the size of the &lt;em&gt;prefix&lt;/em&gt;. The simplest case is the &lt;em&gt;Unigram mode&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; (Uni-) 1-gram model&lt;/strong&gt;
The simplest case of &lt;em&gt;Markov assumption&lt;/em&gt; is case when the size of prefix is one.
&lt;span class="math"&gt;\(P(w_1,\ldots,w_n) \approx {\displaystyle \prod_{i} P(w_i)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This will provide us with grammar that only consider one word. As a result it produces a set of unrelated words. It actually would generate sentences with random word order.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; Bigram &lt;/strong&gt;
However, if we consider a 2-word (tandem) bigrams correlations where we condition each word on previous one we get some sens of meaning between couples of words.&lt;/p&gt;
&lt;div class="math"&gt;$$P(w_1,\ldots,w_n) \approx {\displaystyle \prod_{i} P(w_i|w_{i-1})}$$&lt;/div&gt;
&lt;p&gt;This way we get a sequence of tandems that have meaning tandem-wise. This still is not enough to face a long range dependencies from natural languages, like English. This would be difficult even in case of n-grams as there can be very long sentences with dependencies. However, a 3-gram can get us a pretty nice &lt;a href="http://tetration.xyz/Ngram-Tutorial/"&gt;approximation&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;and how he probably more easily believe me i never see how much as that he and pride and unshackled
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id="estimate-n-gram-probabilities"&gt;Estimate n-gram probabilities&lt;a class="headerlink" href="#estimate-n-gram-probabilities" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Estimation can be done with &lt;strong&gt;Maximum Likelihood Estimate (MLE)&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$P(w_i|w_{i-1})=\frac{count(w_{i-1},w_i)}{count(w_{i-1 })}$$&lt;/div&gt;
&lt;!-- z jupytera:
Z google ngram raw bigram count table
normalize by unigrams --&gt;

&lt;p&gt;So the 2-gram estimate  of sentence probability would be the product of all component tandems ordered as in the sentence.&lt;/p&gt;
&lt;div class="math"&gt;$$P(w_1,\ldots,w_n) \approx {\displaystyle \prod_{i} P(w_i|w_{i-1})}$$&lt;/div&gt;
&lt;p&gt;In practice, the outcome should be represented in &lt;em&gt;log&lt;/em&gt; form. There are two reasons for this. Firstly, if the sentence is long and the probabilities are really small, then such product might end in arithmetic underflow.  Secondly, adding is faster - and when we use logarithm we know that: &lt;span class="math"&gt;\(log(a*b) = log(a)+log(b)\)&lt;/span&gt;, thus:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(log(P(w_1,\ldots,w_n)) \approx {\displaystyle \sum_{i} log(P(w_i|w_{i-1}))}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is why the Language Model is stored in logarithmic values.&lt;/p&gt;
&lt;h3 id="publicly-available-corpora"&gt;Publicly available corpora&lt;a class="headerlink" href="#publicly-available-corpora" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;There is available &lt;a href="http://www.gutenberg.org/ebooks/search/"&gt;Gutenberg Project&lt;/a&gt; providing with text format of some books.&lt;/p&gt;
&lt;p&gt;Google also released a publicly available corpus trillion word corpus with over 13 million unique words. It is nice corpus from Google Books with already implemented &lt;a href="https://books.google.com/ngrams"&gt;N-Gram viewer&lt;/a&gt; that enables to plot word counts. It includes number of corpora for multiple languages. See the below chart an example of unigram, 2-gram and 3-gram occurrences.&lt;/p&gt;
&lt;iframe name="ngram_chart" src="https://books.google.com/ngrams/interactive_chart?content=machine+learning%2Cdeep+learning%2Cnlp%2Cnatural+language+processing&amp;year_start=1982&amp;year_end=2008&amp;corpus=0&amp;smoothing=3&amp;share=&amp;direct_url=t1%3B%2Cmachine%20learning%3B%2Cc0%3B.t1%3B%2Cdeep%20learning%3B%2Cc0%3B.t1%3B%2Cnlp%3B%2Cc0%3B.t1%3B%2Cnatural%20language%20processing%3B%2Cc0" width=860 height=300 marginwidth=0 marginheight=0 hspace=0 vspace=0 frameborder=0 scrolling=no&gt;&lt;/iframe&gt;

&lt;p&gt;Such a large scale text analysis can be done by downloading &lt;a href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html"&gt;each dataset&lt;/a&gt; using the &lt;a href="https://books.google.com/ngrams/info"&gt;Ngram Viewer API&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I have created a project to collect and analyze those datasets locally with custom configuration. See the &lt;a href="https://github.com/MChromiak/MC_Utils"&gt;GitHub repo&lt;/a&gt; for the source code.&lt;/p&gt;
&lt;h3 id="language-model-evaluation-perplexity"&gt;Language Model evaluation ‚Äì Perplexity&lt;a class="headerlink" href="#language-model-evaluation-perplexity" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The good LM should calculate higher probabilities to ‚Äúreal‚Äù and ‚Äúfrequently observed‚Äù sentences than the ones that are wrong accordingly to natural language grammar or those that are rarely observed.&lt;/p&gt;
&lt;p&gt;To make such assumption, we first train the language model on one set and test it on completely new dataset. Then we can compare the results of our model on the new dataset and evaluate, how good (how accurate in terms of a task e.g. translation, spell check etc) &lt;strong&gt;on average&lt;/strong&gt; it works on this new, previously unseen dataset. For instance how many words it translates correctly. This way one can compare and decide which language model fits the best to a task. This is called &lt;em&gt;extrinsic evaluation&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The extrinsic evaluation approach however, requires multiple tests on models which are expensive. An alternative is the &lt;em&gt;intrinsic evaluation&lt;/em&gt; which is about testing the LM itself not some particular task or application. The popular intrinsic evaluation is &lt;strong&gt;perplexity&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As &lt;strong&gt;perplexity&lt;/strong&gt; is a bad approximation to an extreme extrinsic evaluation, in cases where the test dataset does NOT look just like the training set. Thus, it is useful only at the early stages of experiment. So later in experiment extrinsic evaluation should also be used.&lt;/p&gt;
&lt;p&gt;Best model is the one that best predicts an unseen test set, or assigns on average the probability to all sentences that is sees.&lt;/p&gt;
&lt;p&gt;Perprexity metric is the probability of the test set, normalized for the length of the probability by the number of words.&lt;/p&gt;
&lt;div class="math"&gt;$$ PP(S) = P(w_1,\ldots, w_N)^{-\frac{1}{N}} =\sqrt[\leftroot{0}\uproot{3}N]{\frac{1}{P(w_1,\ldots, w_N)}} = $$&lt;/div&gt;
&lt;p&gt;This way the longer the sentence the less probable it will be. Then again by the chain rule:&lt;/p&gt;
&lt;div class="math"&gt;$$ = \sqrt[N]{{\displaystyle \prod_{i}^{N}\frac{1}{P(w_i|w_1,\ldots, w_{i-1})}}} $$&lt;/div&gt;
&lt;p&gt;In case of 2-gram using Markov approximation of the chain rule:&lt;/p&gt;
&lt;div class="math"&gt;$$ PP(S) =  \sqrt[N]{{\displaystyle \prod_{i}^{N}\frac{1}{P(w_i|w_{i-1})}}}$$&lt;/div&gt;
&lt;p&gt;So &lt;em&gt;perplexity&lt;/em&gt; is a function of probability of the sentence. The meaning of the inversion in perplexity means that whenever we minimize the perplexity we maximize the probability.&lt;/p&gt;
&lt;p&gt;Perplexity is weighted equivalent branching factor.&lt;/p&gt;
&lt;p&gt;To be continued...&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Sequence Models"></category><category term="NLP"></category><category term="language model"></category><category term="ngram"></category><category term="perplexity"></category><category term="smoothing"></category></entry><entry><title>The Transformer ‚Äì Attention is all you need.</title><link href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/" rel="alternate"></link><published>2017-09-12T19:30:00+02:00</published><updated>2017-10-30T19:30:00+01:00</updated><author><name>Micha≈Ç Chromiak</name></author><id>tag:mchromiak.github.io,2017-09-12:/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/</id><summary type="html">&lt;p&gt;Transformer - more than meets the eye! Are we there yet? Well... not really, but...&lt;/br&gt;   How about eliminating recurrence and convolution from transduction? Sequence modeling and transduction (e.g. language modeling, machine translation) problems solutions has been dominated by RNN (especially gated RNN) or LSTM, additionally employing the attention mechanism. Main sequence transduction models are based on RNN or CNN including encoder and decoder. The new &lt;em&gt;transformer&lt;/em&gt; architecture is claimed however, to be more parallelizable and requiring significantly less time to train, solely focusing on attention mechanisms.&lt;/p&gt;</summary><content type="html">&lt;h5 id=recommended-reading-before-approaching-this-post&gt;Recommended reading before approaching this post:&lt;a class=headerlink href=#recommended-reading-before-approaching-this-post title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;RNN ‚Äì Andrej Karpathy‚Äôs blog &lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Seq2Seq - Nathan Lintz &lt;a href="https://indico.io/blog/sequence-modeling-neuralnets-part1/"&gt;Sequence Modeling With Neural Networks (Part 1): Language &amp;amp; Seq2Seq&lt;/a&gt;,
Part2 &lt;a href="https://indico.io/blog/sequence-modeling-neural-networks-part2-attention-models/"&gt;Sequence modeling with attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;LSTM ‚Äì Christopher Olah‚Äôs blog &lt;a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"&gt;Understanding LSTM Networks&lt;/a&gt; and R2Rt.com &lt;a href="https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html"&gt;Written Memories: Understanding, Deriving and Extending the LSTM&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Attention ‚Äì Christopher Olah &lt;a href="https://distill.pub/2016/augmented-rnns/#attentional-interfaces"&gt;Attention and Augmented Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=objective-or-goal-for-the-algorithm&gt;Objective or goal for the algorithm&lt;a class=headerlink href=#objective-or-goal-for-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Parallelization of Seq2Seq:&lt;/strong&gt; RNN/CNN handle sequences word-by-word sequentially which is an obstacle to parallelize. Transformer achieve parallelization by replacing recurrence with attention and encoding the symbol position in sequence. This in turn leads to significantly shorter training time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reduce sequential computation:&lt;/strong&gt; Constant &lt;span class=math&gt;\(O(1)\)&lt;/span&gt; number of operations to learn dependency between two symbols independently of their position distance in sequence.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=tldr&gt;TL;DR&lt;a class=headerlink href=#tldr title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;RNN:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Advantages:&lt;/strong&gt; are popular and successful for variable-length representations such as sequences (e.g. languages), images, etc. RNN are considered core of seq2seq (with attention). The gating models such as LSTM or GRU are for long-range error propagation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problems:&lt;/strong&gt; The sequentiality prohibits parallelization within instances. Long-range dependencies still tricky, despite gating.  Sequence-aligned states in RNN are wasteful. Hard to model hierarchical-alike domains such as languages.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;CNN:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Advantages:&lt;/strong&gt; Trivial to parallelize (per layer) and fit intuition that most dependencies are local.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Problems:&lt;/strong&gt; Path length between positions can be logarithmic when using dilated convolutions, left-padding for text. (autoregressive CNNs WaveNet, ByteNET )&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Multi-head self-attention mechanism. Why attention? Table 2 of the paper shows that such attention networks can save 2‚Äì3 orders of magnitude of operations!&lt;/p&gt;
&lt;h3 id=intro&gt;Intro&lt;a class=headerlink href=#intro title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In this post I will elaborate on lately presented paper introducing the &lt;strong&gt;Transformer&lt;/strong&gt; architecture.
Paper: &lt;a href="https://arxiv.org/abs/1706.03762"&gt;ArXiv&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As the paper assumes the in-depth prior knowledge of some topics, I will try to explain the ideas of the paper so that they can be understood by a DL beginner.&lt;/p&gt;
&lt;p&gt;When RNN‚Äôs (or CNN) takes a sequence as an input, it handles sentences word by word. This sequentiality  is an obstacle toward parallelization of the process. What is more, in cases when such sequences are too long, the model is prone to forgetting the content of distant positions in sequence or mix it with following positions‚Äô content.&lt;/p&gt;
&lt;p&gt;One solution to this was Convolution seq2seq. Convolution enables parallelization for GPU processing. Thus &lt;a href="https://arxiv.org/abs/1705.03122"&gt;Gehring et al, 2017&lt;/a&gt; (Facebook AI) present a 100% convolutional architecture to represent hierarchical representation of input sequence. The crux is that close input elements interact at lower layers while distant interacts at higher layers. The stacking of convolution layers is being used to evaluate long range dependencies between words. This is all possible despite fixed width kernels of convolution filters. The authors has also included two tricks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Positional embeddings - that are added to the input embeddings, capturing a sense of order in a sequence&lt;/li&gt;
&lt;li&gt;Multi-step attention - attention is computed with current decoder state and embedding of previous target word token&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=center&gt;&lt;img alt="Multi-step attention " src="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/MultiStepAttention.gif"&gt;
&lt;br&gt;
Figure 1. Multi-step attention form ConvS2S.&lt;/p&gt;
&lt;p&gt;As an alternative to convolutions, a new approach is presented by the &lt;em&gt;Transformer&lt;/em&gt;. It proposes to encode each position and applying the attention mechanism, to relate two distant words, which then can be parallelized thus, accelerating the training.&lt;/p&gt;
&lt;p&gt;Currently, in NLP the SOTA (&lt;em&gt;state-of-the-art&lt;/em&gt;) performance achieved by seq2seq models is focused around the idea of encoding the input sentence into a fixed-size vector representation. The vector size is fixed, regardless of the length of the input sentence. In obvious way this must loose some information. To face this issue &lt;em&gt;Transformer&lt;/em&gt; employs an alternative approach based on attention.&lt;/p&gt;
&lt;p&gt;Due to multiple referred work it is beneficial to also read the mentioned research&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.03906"&gt;Denny Britz on Attention, 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1705.04304"&gt;Self-attention (a.k.a Intra-attention), 2017&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=motivation&gt;Motivation&lt;a class=headerlink href=#motivation title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The Transformer architecture is aimed at the problem of &lt;a href="https://arxiv.org/abs/1211.3711"&gt;&lt;em&gt;sequence transduction&lt;/em&gt; (by Alex Graves)&lt;/a&gt;, meaning any task where input sequences are transformed into output sequences. This includes speech recognition, text-to-speech transformation, machine translation, protein secondary structure prediction, Turing machines etc. Basically the goal is to design a single framework to handle as many sequences as possible.&lt;/p&gt;
&lt;p&gt;Currently, complex RNN and CNN based on encoder-decoder scheme are dominating transduction models (language modeling and machine learning). Recurrent models due to sequential nature (computations focused on the position of symbol in input and output) are not allowing for parallelization along training, thus have a problem with learning long-term dependencies (&lt;a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&amp;amp;arnumber=5264952"&gt;Kolen,, et  al.&lt;/a&gt;) from memory. The bigger the memory is, the better, but the memory eventually  constrains batching across learning examples for long sequences, and this is why parallelization can not help.&lt;/p&gt;
&lt;p&gt;Reducing this fundamental constraint of sequential computation has been target for numerous research like &lt;a href="https://arxiv.org/abs/1609.03499"&gt;Wavenet&lt;/a&gt; / &lt;a href="https://arxiv.org/abs/1610.10099"&gt;Bytenet&lt;/a&gt; or &lt;a href="https://arxiv.org/abs/1705.03122"&gt;ConvS2S&lt;/a&gt;. However, in those CNN-based approaches, the number of calculations in parallel computation of the hidden representation, for input&lt;span class=math&gt;\(\rightarrow\)&lt;/span&gt;output position in sequence, grows with the distance between those positions. The complexity of &lt;span class=math&gt;\(O(n)\)&lt;/span&gt; for ConvS2S and  &lt;span class=math&gt;\(O(nlogn)\)&lt;/span&gt; for ByteNet makes it harder to learn dependencies on distant positions.&lt;/p&gt;
&lt;p&gt;Transformer reduces the number of sequential operations to relate two symbols from input/output sequences  to a constant &lt;span class=math&gt;\(O(1)\)&lt;/span&gt; number of operations. Transformer achieves this with the &lt;em&gt;multi-head attention&lt;/em&gt; mechanism that allows to model dependencies regardless of their distance in input or output sentence.   &lt;/p&gt;
&lt;p&gt;Up till now, most of the research including attention is used along with RNNs. The novel approach of Transformer is however, to eliminate recurrence completely and replace it with attention to handle the dependencies between input and output. The Transformer moves the sweet spot of current ideas toward attention entirely. It eliminates the not only recurrence but also convolution in favor of applying &lt;strong&gt;self-attention&lt;/strong&gt; (a.k.a intra-attention). Additionally Transformer gives more space for parallelization (details present in paper).&lt;/p&gt;
&lt;p&gt;The top performance in the paper is achieved while applying the &lt;strong&gt;attention&lt;/strong&gt; mechanism connecting encoder and decoder.&lt;/p&gt;
&lt;p&gt;Transformer is claimed by authors to be the first to rely entirely on self-attention to compute representations of input and output.&lt;/p&gt;
&lt;h3 id=information-on-processing-strategy-of-the-algorithm&gt;Information on processing strategy of the algorithm&lt;a class=headerlink href=#information-on-processing-strategy-of-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Transformer is based on sequence-to-sequence model for &lt;em&gt;Statistical Machine Translation&lt;/em&gt; (SMT) as introduced in &lt;a href="https://arxiv.org/abs/1406.1078"&gt;Cho et al., 2014&lt;/a&gt;. It includes two RNNs, one for &lt;strong&gt;encoder&lt;/strong&gt; to process the input and the other as a &lt;strong&gt;decoder&lt;/strong&gt;, for generating the output.&lt;/p&gt;
&lt;p&gt;In general, transformer‚Äôs &lt;em&gt;encoder&lt;/em&gt; maps input sequence to its continuous representation &lt;span class=math&gt;\(z\)&lt;/span&gt; which in turn is used by &lt;em&gt;decoder&lt;/em&gt; to generate output, one symbol at a time.&lt;/p&gt;
&lt;p&gt;The final state of the encoder is a fixed size vector &lt;span class=math&gt;\(z\)&lt;/span&gt; that must encode entire source sentence which includes the sentence meaning. This final state is therefore called &lt;em&gt;sentence embedding&lt;/em&gt;&lt;sup id=sf-Transformer-Attention-is-all-you-need-1-back&gt;&lt;a href=#sf-Transformer-Attention-is-all-you-need-1 class=simple-footnote title="While applying dimensionality reduction techniques (e.g. PCA, t-SNE) on embeddings, the outcome plot gathers the semantically close sentences  together Sutskever et al., 2014."&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;The encoder-decoder model is designed at its each step to be &lt;strong&gt;auto-regressive&lt;/strong&gt; -  i.e. use previously generated  symbols as extra input while generating next symbol. Thus, &lt;span class=math&gt;\( x_i + y_{i-1}\rightarrow y_i\)&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=transformer-is-based-on-encoder-decoder&gt;Transformer is based on Encoder-Decoder&lt;a class=headerlink href=#transformer-is-based-on-encoder-decoder title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In Transformer (as in ByteNet or ConvS2S) the &lt;em&gt;decoder&lt;/em&gt; is stacked directly on top of &lt;em&gt;encoder&lt;/em&gt;. Encoder and decoder both are composed of stack of identical layers. Each of those stacked layers is composed out of two general types of sub-layers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;multi-head self-attention mechanism, and&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;position-wise fully connected FFN.&lt;/p&gt;
&lt;p&gt;In contrast to ConvS2S however, where input representation considered each input element  combined with its absolute position number in sequence (providing sense of ordering; ByteNet have dilated convolutions and no position-wise FNN), transformer introduces two different NN for these two types of information.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The way the attention mechanism is applied and customized is what makes the Transformer novel.&lt;/p&gt;
&lt;p&gt;One can find the reference Transformer model implementation from authors is present in &lt;a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py"&gt;Tensor2Tensor (T2T) library&lt;/a&gt;&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="Encoder Decoder architecture " src="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/encoder.png"&gt;
&lt;br&gt;
Figure 2. Single layer of Encoder (left) and Decoder (right) that is build out of &lt;span class=math&gt;\(N=6\)&lt;/span&gt; identical layers.&lt;/p&gt;
&lt;h4 id=encoder&gt;Encoder&lt;a class=headerlink href=#encoder title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Stage 1 ‚Äì Encoder Input&lt;/strong&gt;
Information on sequence ordering is very important. As there is no recurrence, nor convolution, this information on absolute (or relative) position of each token in a sequence is represented with use of "&lt;em&gt;positional encodings&lt;/em&gt;"
(&lt;a href="../../../../2017/Sep/12/Transformer-Attention-is-all-you-need/#positional-encoding-pe"&gt;Read more&lt;/a&gt;). The input for the encoder is therefore, represented  as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;positional encodings&lt;/em&gt;  added &lt;span class=math&gt;\(\oplus\)&lt;/span&gt; to&lt;/li&gt;
&lt;li&gt;&lt;em&gt;embedded inputs&lt;/em&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class=math&gt;\(N=6\)&lt;/span&gt; layers. In practice the &lt;span class=math&gt;\(N=6\)&lt;/span&gt; means more than 6 layers. Each of those ‚Äúlayers‚Äù are actually composed of two layers: position-wise FNN and one (encoder), or two (decoder), attention-based sublayers. Each of those additionally contains 4 linear projections and the attention logic. Thus, providing effectively deeper than 6 layer architecture.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Stage 2 ‚Äì Multi-head attention&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stage 3 ‚Äì position-wise FFN&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stages 2 and 3 use the residual connection (thus, all employ &lt;span class=math&gt;\(d_{model}=512\)&lt;/span&gt;) followed by normalization layer at its output.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, encoder works like this:&lt;/p&gt;
&lt;div class=highlight&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Stage1_out = Embedding512 + TokenPositionEncoding512
Stage2_out = layer_normalization(multihead_attention(Stage1_out) + Stage1_out)
Stage3_out = layer_normalization(FFN(Stage2_out) + Stage2_out)

out_enc = Stage3_out
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id=decoder&gt;Decoder&lt;a class=headerlink href=#decoder title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Decoder‚Äôs architecture is similar however, it employs additional layer in &lt;em&gt;Stage 3&lt;/em&gt; with mask multi-head attention over encoder output.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt; Stage 1 ‚Äì Decoder input&lt;/strong&gt;
The input is the &lt;em&gt;output embedding&lt;/em&gt;, offset by one position to ensure that the prediction for position &lt;span class=math&gt;\(i\)&lt;/span&gt; is only dependent on positions previous to/less than &lt;span class=math&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt; Stage 2 Masked Multi-head attention&lt;/strong&gt;
Modified to prevent positions to attend to subsequent positions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stages 2, 3 and 4 also use the residual connection followed by normalization layer at its output.&lt;/p&gt;
&lt;p&gt;The details of each mechanism applied in the mentioned layers is more elaborated in following section.&lt;/p&gt;
&lt;p&gt;Put together decoder works as follows:&lt;/p&gt;
&lt;div class=highlight&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Stage1_out = OutputEmbedding512 + TokenPositionEncoding512

Stage2_Mask = masked_multihead_attention(Stage1_out)
Stage2_Norm1 = layer_normalization(Stage2_Mask) + Stage1_out
Stage2_Multi = multihead_attention(Stage2_Norm1 + out_enc) +  Stage2_Norm1
Stage2_Norm2 = layer_normalization(Stage2_Multi) + Stage2_Multi

Stage3_FNN = FNN(Stage2_Norm2)
Stage3_Norm = layer_normalization(Stage3_FNN) + Stage2_Norm2

out_dec = Stage3_Norm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=mechanisms-used-to-compose-transformer-architecture&gt;Mechanisms used to compose Transformer architecture&lt;a class=headerlink href=#mechanisms-used-to-compose-transformer-architecture title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;There are couple types of layers that transformer consists of. Their details are depict in following sections.&lt;/p&gt;
&lt;h4 id=positional-encoding-pe&gt;Positional Encoding ‚Äì PE&lt;a class=headerlink href=#positional-encoding-pe title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;In RNN (LSTM), the notion of time step is encoded in the sequence as inputs/outputs flow one at a time. In FNN, the &lt;em&gt;positional encoding&lt;/em&gt; must be used to represent the time in some way. In case of the Transformer authors propose to encode time as &lt;span class=math&gt;\(sine\)&lt;/span&gt; wave, as an added extra input. Such signal is added to inputs and outputs to represent time passing&lt;sup id=sf-Transformer-Attention-is-all-you-need-2-back&gt;&lt;a href=#sf-Transformer-Attention-is-all-you-need-2 class=simple-footnote title="It is interesting how this resembles the brain waves and neural oscillations."&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;In general, adding positional encodings to the input embeddings is a quite interesting topic. One way is to embed the absolute position of input elements (as in ConvS2S). However, authors use "sine and cosine functions of different frequencies". The "sinusoidal" version is quite complicated, while giving similar performance to the absolute position version. The crux is however, that it may allow the model to produce better translation on longer sentences at test time (at least longer than the sentences in the training data). This way sinusoidal method allows the model to extrapolate to longer sequence lengths&lt;sup id=sf-Transformer-Attention-is-all-you-need-3-back&gt;&lt;a href=#sf-Transformer-Attention-is-all-you-need-3 class=simple-footnote title="It reminds a bit the Pointer Networks that address similar problem"&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;This encoding gives the model a sense of which portion of the sequence of the input (or output) it is currently dealing with. The positional encoding can be learned, or fixed. Authors made tests  (PPL, BLEU) showing that both: learned and fixed positional encodings perform similarly.  &lt;/p&gt;
&lt;p&gt;In paper authors have decided on fixed variant using &lt;span class=math&gt;\(sin\)&lt;/span&gt; and &lt;span class=math&gt;\(cos\)&lt;/span&gt; functions to enable the network to learn information about tokens relative positions to the sequence.&lt;/p&gt;
&lt;div class=math&gt;$$ \begin{eqnarray} PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) \end{eqnarray}$$&lt;/div&gt;
&lt;div class=math&gt;$$ \begin{eqnarray} PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})\end{eqnarray}$$&lt;/div&gt;
&lt;p&gt;Of course authors motivate the use of sinusoidal functions due to enabling model to generalize to sequences longer than ones encountered during training.&lt;/p&gt;
&lt;h3 id=attention&gt;Attention&lt;a class=headerlink href=#attention title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Attention between encoder and decoder is crucial in NMT. Authors formulate the definition of &lt;em&gt;attention&lt;/em&gt; that has already been elaborated in &lt;a href="../../../../2017/Sep/01/Primer-NN/#attention-basis"&gt;Attention primer&lt;/a&gt;. Attention is a function that maps the 2-element input (&lt;em&gt;query&lt;/em&gt;, &lt;em&gt;key-value&lt;/em&gt; pairs) to an output. The output given by the mapping function is a weighted sum of the &lt;em&gt;values&lt;/em&gt;. Where weights for each &lt;em&gt;value&lt;/em&gt;  measures how much each input &lt;em&gt;key&lt;/em&gt; interacts with (or answers) the &lt;em&gt;query&lt;/em&gt;. While the attention is a goal for many research, the novelty about transformer attention is that it is &lt;strong&gt;multi-head  self-attention&lt;/strong&gt;.&lt;/p&gt;
&lt;h4 id=scaled-dot-product-attention&gt;Scaled Dot-Product Attention&lt;a class=headerlink href=#scaled-dot-product-attention title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;In terms of encoder-decoder, the &lt;strong&gt;query&lt;/strong&gt; is usually the hidden state of the &lt;em&gt;decoder&lt;/em&gt;. Whereas &lt;strong&gt;key&lt;/strong&gt;, is the hidden state of the &lt;em&gt;encoder&lt;/em&gt;, and the corresponding &lt;strong&gt;value&lt;/strong&gt; is normalized weight, representing how much attention a &lt;em&gt;key&lt;/em&gt; gets.  Output is calculated as a wighted sum ‚Äì here the dot product of &lt;em&gt;query&lt;/em&gt; and &lt;em&gt;key&lt;/em&gt; is used to get a &lt;em&gt;value&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;It is assumed that &lt;em&gt;queries&lt;/em&gt; and &lt;em&gt;keys&lt;/em&gt; are of &lt;span class=math&gt;\(d_k\)&lt;/span&gt; dimension and &lt;em&gt;values&lt;/em&gt; are of &lt;span class=math&gt;\(d_v\)&lt;/span&gt; dimension. Those dimensions are imposed by the linear projection discussed in the multi-head attention section. The  input is represented by three matrices: queries‚Äô matrix &lt;span class=math&gt;\(Q\)&lt;/span&gt;, keys‚Äô matrix &lt;span class=math&gt;\(K\)&lt;/span&gt; and values‚Äô matrix &lt;span class=math&gt;\(V\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;compatibility function&lt;/em&gt; (see &lt;a href="../../../../2017/Sep/01/Primer-NN/#attention-basis"&gt;Attention primer&lt;/a&gt;)  is considered in terms of two, &lt;em&gt;additive&lt;/em&gt; and &lt;em&gt;multiplicative&lt;/em&gt; (dot-product) variants &lt;a href="https://arxiv.org/abs/1409.0473"&gt;Bahdanau et al. 2014&lt;/a&gt; with similar theoretical complexity. However, the dot-product (&lt;span class=math&gt;\(q \cdot  k = \sum_{i=1}^{d_k}q_i k_i\)&lt;/span&gt;) with scaling factor &lt;span class=math&gt;\(1/\sqrt{d_k}\)&lt;/span&gt; is chosen due to being much faster and space-efficient, as it uses  optimized matrix multiplication code&lt;sup id=sf-Transformer-Attention-is-all-you-need-4-back&gt;&lt;a href=#sf-Transformer-Attention-is-all-you-need-4 class=simple-footnote title="The additional scaling factor is advised for large \(d_k\) where dot product grow large in magnitude, as softmax is suspected to be pushed to vanishing gradient area, thus making the additive attention perform better. "&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;div class=math&gt;$$\begin{eqnarray} Attention (Q,K,V) = softmax \Big( \frac{QK^T}{\sqrt{d_k}} \Big) V \end{eqnarray}$$&lt;/div&gt;
&lt;p&gt;Using NumPy:&lt;/p&gt;
&lt;div class=highlight&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=k&gt;def&lt;/span&gt; &lt;span class=nf&gt;attention&lt;/span&gt;&lt;span class=p&gt;(&lt;/span&gt;&lt;span class=n&gt;Q&lt;/span&gt;&lt;span class=p&gt;,&lt;/span&gt; &lt;span class=n&gt;K&lt;/span&gt;&lt;span class=p&gt;,&lt;/span&gt; &lt;span class=n&gt;V&lt;/span&gt;&lt;span class=p&gt;):&lt;/span&gt;
        &lt;span class=n&gt;num&lt;/span&gt; &lt;span class=o&gt;=&lt;/span&gt; &lt;span class=n&gt;np&lt;/span&gt;&lt;span class=o&gt;.&lt;/span&gt;&lt;span class=n&gt;dot&lt;/span&gt;&lt;span class=p&gt;(&lt;/span&gt;&lt;span class=n&gt;Q&lt;/span&gt;&lt;span class=p&gt;,&lt;/span&gt; &lt;span class=n&gt;K&lt;/span&gt;&lt;span class=o&gt;.&lt;/span&gt;&lt;span class=n&gt;T&lt;/span&gt;&lt;span class=p&gt;)&lt;/span&gt;
        &lt;span class=n&gt;denum&lt;/span&gt; &lt;span class=o&gt;=&lt;/span&gt; &lt;span class=n&gt;np&lt;/span&gt;&lt;span class=o&gt;.&lt;/span&gt;&lt;span class=n&gt;sqrt&lt;/span&gt;&lt;span class=p&gt;(&lt;/span&gt;&lt;span class=n&gt;K&lt;/span&gt;&lt;span class=o&gt;.&lt;/span&gt;&lt;span class=n&gt;shape&lt;/span&gt;&lt;span class=p&gt;[&lt;/span&gt;&lt;span class=mi&gt;0&lt;/span&gt;&lt;span class=p&gt;])&lt;/span&gt;
        &lt;span class=k&gt;return&lt;/span&gt; &lt;span class=n&gt;np&lt;/span&gt;&lt;span class=o&gt;.&lt;/span&gt;&lt;span class=n&gt;dot&lt;/span&gt;&lt;span class=p&gt;(&lt;/span&gt;&lt;span class=n&gt;softmax&lt;/span&gt;&lt;span class=p&gt;(&lt;/span&gt;&lt;span class=n&gt;num&lt;/span&gt; &lt;span class=o&gt;/&lt;/span&gt; &lt;span class=n&gt;denum&lt;/span&gt;&lt;span class=p&gt;),&lt;/span&gt; &lt;span class=n&gt;V&lt;/span&gt;&lt;span class=p&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id=multi-head-attention&gt;Multi-head attention&lt;a class=headerlink href=#multi-head-attention title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Transformer reduces the number of operations required to relate (especially distant) positions in input and output sequence to a &lt;span class=math&gt;\(O(1)\)&lt;/span&gt;. However, this comes at cost of reduced effective resolution because of averaging attention-weighted positions.&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="Multihead attetntion" src="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/MultiHead.png"&gt;
&lt;br&gt;
Figure 3. Multi-Head Attention consists of &lt;span class=math&gt;\(h\)&lt;/span&gt; attention layers running in parallel.&lt;/p&gt;
&lt;p&gt;To reduce this cost authors propose the multi-head attention:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;span class=math&gt;\(h=8\)&lt;/span&gt; attention layers (aka ‚Äúheads‚Äù): that represent linear projection (for the purpose of dimension reduction) of key &lt;span class=math&gt;\(K\)&lt;/span&gt; and query &lt;span class=math&gt;\(Q\)&lt;/span&gt; into &lt;span class=math&gt;\(d_k\)&lt;/span&gt;-dimension and value &lt;span class=math&gt;\(V\)&lt;/span&gt; into &lt;span class=math&gt;\(d_v\)&lt;/span&gt;- dimension:&lt;/p&gt;
&lt;p&gt;
&lt;/p&gt;&lt;div class=math&gt;$$head_i = Attention(Q W^Q_i, K W^K_i, V W^V_i) , i=1,\dots,h$$&lt;/div&gt;
where projections are parameter matrices &lt;span class=math&gt;\(W^Q_i, W^K_i\in\mathbb{R}^{d_{model}\times d_k}, W^V_i\in\mathbb{R}^{d_{model}\times d_v}\)&lt;/span&gt;,
for &lt;span class=math&gt;\(d_k=d_v=d_{model}/h = 64\)&lt;/span&gt;&lt;p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;scaled-dot attention applied in parallel on each layer (different linear projections of &lt;span class=math&gt;\(k, q, v\)&lt;/span&gt;) results in &lt;span class=math&gt;\(d_v\)&lt;/span&gt;-dimensional output.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;concatenate outputs of each layer (different linear projection; also referred as &lt;em&gt;‚Äùhead‚Äù&lt;/em&gt;): &lt;span class=math&gt;\(Concat(head_1,\dots,head_h)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;linearly project the concatenation result form the previous step:&lt;/p&gt;
&lt;p&gt;
&lt;/p&gt;&lt;div class=math&gt;$$ MultiHeadAttention(Q,K,V) = Concat(head_1,\dots,head_h) W^O$$&lt;/div&gt;
where  &lt;span class=math&gt;\(W^0\in\mathbb{R}^{d_{hd_v}\times d_{model}}\)&lt;/span&gt;&lt;p&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Transformer use multi-head (&lt;span class=math&gt;\(d_{model}/h\)&lt;/span&gt; parallel attention functions) attention instead of single (&lt;span class=math&gt;\(d_{model}\)&lt;/span&gt;-dimensional) attention function (i.e. &lt;span class=math&gt;\(q,k,v\)&lt;/span&gt; all &lt;span class=math&gt;\(d_{model}\)&lt;/span&gt;-dimensional). It is at similar computational cost as in the case of single-head attention due to reduced dimensions of each head.&lt;/p&gt;
&lt;p&gt;Transformer imitates the classical attention mechanism (known e.g. from &lt;a href="https://arxiv.org/abs/1409.0473"&gt;Bahdanau et al., 2014&lt;/a&gt; or Conv2S2) where in encoder-decoder attention layers &lt;em&gt;queries&lt;/em&gt; are form previous decoder layer, and the (memory) &lt;em&gt;keys&lt;/em&gt; and &lt;em&gt;values&lt;/em&gt; are from output of the encoder.  Therefore, each position in decoder can attend over all positions in the input sequence.&lt;/p&gt;
&lt;h4 id=self-attention-sa&gt;Self-Attention (SA)&lt;a class=headerlink href=#self-attention-sa title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;See &lt;a href="../../../../2017/Sep/01/Primer-NN/#attention-basis"&gt;Attention Primer&lt;/a&gt; for basics on attention.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;encoder&lt;/em&gt;, self-attention layers process input &lt;span class=math&gt;\(queries, keys\)&lt;/span&gt; and &lt;span class=math&gt;\(values\)&lt;/span&gt; that comes form same place i.e. the output of previous layer in encoder. Each position in encoder can attend to all positions from previous layer of the encoder&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;decoder&lt;/em&gt;, self-attention layer enable each position to attend to all previous positions in the decoder, including the current position. To preserve auto-regressive property, the leftward information flow is presented inside the dot-product attention by masking out (set to &lt;span class=math&gt;\(- \infty\)&lt;/span&gt;) all &lt;span class=math&gt;\(values\)&lt;/span&gt; that are input for softmax which correspond to this illegal connections.&lt;/p&gt;
&lt;p&gt;Authors motivates the use of self-attention layers instead of recurrent or convolutional layers with three desiderata:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Minimize total computational complexity per layer&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pros:&lt;/strong&gt; self-attention layers connects all positions with &lt;span class=math&gt;\(O(1)\)&lt;/span&gt; number of sequentially executed operations (eg. vs &lt;span class=math&gt;\(O(n)\)&lt;/span&gt; in RNN)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Maximize amount of parallelizable computations, measured by minimum number of sequential operations required&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pros:&lt;/strong&gt; for sequence length &lt;span class=math&gt;\(n\)&lt;/span&gt; &amp;lt; representation dimensionality &lt;span class=math&gt;\(d\)&lt;/span&gt; (true for SOTA sequence representation models like &lt;em&gt;word-piece, byte-pair&lt;/em&gt;). For very long sequences &lt;span class=math&gt;\(n&amp;gt;d\)&lt;/span&gt; self-attention can consider only neighborhood of some size &lt;span class=math&gt;\(r\)&lt;/span&gt;  in the input sequence centered around the respective output position, thus increasing the max path length to &lt;span class=math&gt;\(O(n/r)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Minimize maximum path length between any two input and output positions in network composed of the different layer types . The shorter the path between any combination of positions in the input and output sequences, the easier to learn long-range dependencies. (See why &lt;a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.7321"&gt;Hochreiter et al, 2001&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=position-wise-ffn&gt;Position-wise FFN&lt;a class=headerlink href=#position-wise-ffn title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;In encoder and decoder the attention sublayers is being processed by a fully connected FNN. It is applied to each position separately and identically meaning two linear transformations and a ReLU
&lt;/p&gt;
&lt;div class=math&gt;$$ FFN(x) = max(0, xW_1+b_1) W_2 + b_2$$&lt;/div&gt;
&lt;p&gt;
Linear transformations are the same for each position, but use different parameters from layer to layer. It works similarly to two convolutions of kernel size 1. The input/output dimension is &lt;span class=math&gt;\(d_{model}=512\)&lt;/span&gt; while inner0layer is &lt;span class=math&gt;\(d_{ff}=2048\)&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=pseudocode-or-flowchart-description-of-the-algorithm&gt;Pseudocode or flowchart description of the algorithm.&lt;a class=headerlink href=#pseudocode-or-flowchart-description-of-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p align=center&gt;&lt;img alt="Transformer  " src="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/transform20fps.gif"&gt;
&lt;br&gt;
Figure 4. Transformer step-by-step sequence transduction in form of English-to-French translation. Adopted from &lt;a href="https://research.googleblog.com/2017/08/transformer-novel-neural-network.html"&gt;Google Blog&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In encoder phase (shown in the Figure 1.), transformer first generates initial representation/embedding for each word in input sentence (empty circle). Next, for each word, self-attention aggregates information form all other words in context of sentence, and creates new representation (filled circles). The process is repeated for each word in sentence. Successively building new representations, based on previous ones is repeated multiple times and in parallel for each word (next layers of filled circles).&lt;/p&gt;
&lt;p&gt;Decoder acts similarly generating one word at a time in a left-to-right-pattern. It attends to previously generated words of decoder and final representation of encoder.&lt;/p&gt;
&lt;p&gt;It is worth noting that this self-attention strategy allows to face the issue of &lt;strong&gt;coreference resolution&lt;/strong&gt; where e.g. word ‚Äú&lt;em&gt;it&lt;/em&gt;‚Äù in a sentence can refer to different noun of the sentence depending on context.&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="coreference resolution" src="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/img/CoreferenceResolution.png"&gt;
&lt;br&gt;
Figure 5. Co-reference resolution. The &lt;em&gt;it&lt;/em&gt; in both cases relates to different token. Adopted from &lt;a href="https://research.googleblog.com/2017/08/transformer-novel-neural-network.html"&gt;Google Blog&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=heuristics-or-rules-of-thumb&gt;Heuristics or rules of thumb.&lt;a class=headerlink href=#heuristics-or-rules-of-thumb title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Authors have conducted a series of tests (Table 3 of the paper) where they discuss recommendation of &lt;span class=math&gt;\(N=6\)&lt;/span&gt; layers with model size 512 based on &lt;span class=math&gt;\(h=8\)&lt;/span&gt; heads with key, values dimensions of 64 using 100K steps.&lt;/p&gt;
&lt;p&gt;It is also stated that dot-product compatibility function might be further optimized due to model quality is decreased with smaller &lt;span class=math&gt;\(d_k\)&lt;/span&gt; (row B).&lt;/p&gt;
&lt;p&gt;The proposed, fixed sinusoidal positional encodings are claimed to produce nearly equal score comparing to learned positional encodings.&lt;/p&gt;
&lt;h3 id=what-classes-of-problem-is-the-algorithm-well-suited&gt;What classes of problem is the algorithm well suited?&lt;a class=headerlink href=#what-classes-of-problem-is-the-algorithm-well-suited title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;sequence transduction (language translation)&lt;/li&gt;
&lt;li&gt;classic language analysis task of syntactic constituency parsing&lt;/li&gt;
&lt;li&gt;different inputs and outputs modalities, such as images and video&lt;/li&gt;
&lt;li&gt;coreference resolution&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=common-benchmark-or-example-datasets-used-to-demonstrate-the-algorithm&gt;Common benchmark or example datasets used to demonstrate the algorithm.&lt;a class=headerlink href=#common-benchmark-or-example-datasets-used-to-demonstrate-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Perplexity (PPL) BLEU&lt;/li&gt;
&lt;li&gt;English-to-German translation development set WMT 2014 English-to-German and WMT 2014
English-to-French translation tasks&lt;/li&gt;
&lt;li&gt;newstest2013&lt;/li&gt;
&lt;li&gt;English constituency parsing&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=useful-resources-for-learning-more-about-the-algorithm&gt;Useful resources for learning more about the algorithm.&lt;a class=headerlink href=#useful-resources-for-learning-more-about-the-algorithm title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://research.googleblog.com/2017/08/transformer-novel-neural-network.html"&gt;Google blog post1&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html"&gt;Google blog post2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are also some more general learning open-source software frameworks for NMT:
* &lt;a href="https://github.com/tensorflow/nmt"&gt;Tensorflow: Neural Machine Translation (seq2seq) Tutorial&lt;/a&gt;
Ground-up explaining NMT concepts tutorial with code, based on Tensorflow by Google Research. Based on &lt;a href="https://research.googleblog.com/2017/07/building-your-own-neural-machine.html"&gt;Google Research blog.&lt;/a&gt;&lt;br&gt;
* &lt;a href="https://github.com/facebookresearch/fairseq"&gt;Torch: FirSeq&lt;/a&gt; seq2seq and ConvS2S from Facebook AI.
* &lt;a href="https://github.com/tensorflow/tensor2tensor"&gt;Tensorflow: Tensor2Tensor&lt;/a&gt; From used by Transformer authors.&lt;/p&gt;
&lt;h3 id=thoughts-on-the-idea&gt;Thoughts on the idea.&lt;a class=headerlink href=#thoughts-on-the-idea title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;To my limited knowledge there are some statements that might benefit form more explanation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How the scaling factor (Equation 3) makes an impact?&lt;/li&gt;
&lt;li&gt;How actually the &lt;strong&gt;positional encoding&lt;/strong&gt; work? Why they have chosen the sin/cos functions and why the position and dimension are in this relation? Finally how sinusoidal helps translate long sentences?&lt;/li&gt;
&lt;li&gt;Does having separate position-wise FFNs help? (comparing to ConvS2S).&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;‚Äúcost of reduced effective resolution due to averaging attention-weighted position‚Äù&lt;/em&gt; is claimed to be a motivation for multi-head attention. How to understand better what is the issue and how multi-head attention helps?&lt;/li&gt;
&lt;li&gt;The Transformer brings a significantly improvement over ConvS2S, but where does the improvement come from? It is not clear from the work. ConvS2S lacks the self-attention, is it what brings the advantage?&lt;/li&gt;
&lt;li&gt;Masked Attention. The problem of using same parts of input on different decoding step is claimed to be solved by penalizing (mask-out to &lt;span class=math&gt;\(-\infty\)&lt;/span&gt;) input tokens that have obtained high attention scores in the past decoding steps ‚Äì a bit vague. How does it work? Maybe explicitly having a position-wise FFN automatically fixes that problem?&lt;/li&gt;
&lt;li&gt;Applying multi-head attention might improve performance due to better parallelization. However, Table 3 also show increasing &lt;span class=math&gt;\(h=1 to 8\)&lt;/span&gt; improves accuracy. Why? Moving  &lt;span class=math&gt;\(h\)&lt;/span&gt; to 16 or 32 is not that beneficial. How to interpret this correctly?&lt;/li&gt;
&lt;li&gt;How important the autoregression is in context of this architecture?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Please leave a comment if you have any other question, or would like to get more explanation on any of the paper‚Äôs particularities.&lt;/p&gt;
&lt;h3 id=primary-references-or-resources-in-which-the-algorithm-was-first-described&gt;Primary references or resources in which the algorithm was first described.&lt;a class=headerlink href=#primary-references-or-resources-in-which-the-algorithm-was-first-described title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/1706.03762"&gt;ArXiv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=some-interesting-future-research&gt;Some interesting future research&lt;a class=headerlink href=#some-interesting-future-research title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Devising more sophisticated compatibility function&lt;/li&gt;
&lt;li&gt;Increase maximum path length to &lt;span class=math&gt;\(O(n/r)\)&lt;/span&gt;, where &lt;span class=math&gt;\(r\)&lt;/span&gt; would be only a neighborhood size of positions to be considered by self-attention instead of all positions in sequence&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=footnotes&gt;Footnotes:&lt;a class=headerlink href=#footnotes title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;ol class=simple-footnotes&gt;&lt;li id=sf-Transformer-Attention-is-all-you-need-1&gt;While applying dimensionality reduction techniques (e.g. PCA, t-SNE) on embeddings, the outcome plot gathers the semantically close sentences  together &lt;a href="https://arxiv.org/abs/1409.3215"&gt;Sutskever et al., 2014&lt;/a&gt;. &lt;a href=#sf-Transformer-Attention-is-all-you-need-1-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Transformer-Attention-is-all-you-need-2&gt;It is interesting how this resembles the brain waves and &lt;a href="https://en.wikipedia.org/wiki/Neural_oscillation"&gt;neural oscillations&lt;/a&gt;. &lt;a href=#sf-Transformer-Attention-is-all-you-need-2-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Transformer-Attention-is-all-you-need-3&gt;It reminds a bit the Pointer Networks that address similar problem &lt;a href=#sf-Transformer-Attention-is-all-you-need-3-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Transformer-Attention-is-all-you-need-4&gt;The additional scaling factor is advised for large &lt;span class=math&gt;\(d_k\)&lt;/span&gt; where dot product grow large in magnitude, as softmax is suspected to be pushed to vanishing gradient area, thus making the additive attention perform better.  &lt;a href=#sf-Transformer-Attention-is-all-you-need-4-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><category term="Sequence Models"></category><category term="NMT"></category><category term="transformer"></category><category term="Sequence transduction"></category><category term="Attention model"></category><category term="Machine translation"></category><category term="seq2seq"></category><category term="NLP"></category></entry><entry><title>Neural Networks Primer</title><link href="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/" rel="alternate"></link><published>2017-09-01T19:30:00+02:00</published><updated>2017-10-01T19:30:00+02:00</updated><author><name>Micha≈Ç Chromiak</name></author><id>tag:mchromiak.github.io,2017-09-01:/articles/2017/Sep/01/Primer-NN/</id><summary type="html">&lt;p&gt;When you approach a new term you often find some Wiki page, Quora answers blogs and it sometimes might take some time before you find the true ground up, clear definition with meaningful example. I will put here the most intuitive explanations of basic topics. Due to extended nature of aspects and terms that are used across NN area, in this post I will place condensed definitions and a brief explanations ‚Äì just to understand the intuition of terms that are mentioned in other posts along this blog.&lt;/p&gt;</summary><content type="html">&lt;p&gt;If any of the topic will grow enough I will put it into a separate post. To get in-depth understanding I recommend for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NLP:&lt;/strong&gt; the Yoav Goldberg's, 2016 &lt;a href="https://www.jair.org/media/4992/live-4992-9623-jair.pdf"&gt;A Primer on Neural Network Models for Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=rnn-primer-with-gradient-descent-gd&gt;RNN primer with Gradient Descent (GD)&lt;a class=headerlink href=#rnn-primer-with-gradient-descent-gd title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;iframe width=420 height=315 src="https://www.youtube.com/embed/aircAruvnKk" frameborder=0 allowfullscreen=""&gt; &lt;/iframe&gt;
&lt;iframe width=420 height=315 src="https://www.youtube.com/embed/IHZwWFHWa-w" frameborder=0 allowfullscreen=""&gt; &lt;/iframe&gt;
&lt;iframe width=420 height=315 src="https://www.youtube.com/embed/Ilg3gGewQ5U" frameborder=0 allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;iframe width=420 height=315 src="https://www.youtube.com/embed/tIeHLnjs5U8" frameborder=0 allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;iframe width=420 height=315 src="https://www.youtube.com/embed/ILsA4nyG7I0" frameborder=0 allowfullscreen=""&gt; &lt;/iframe&gt;

&lt;h3 id=convolutional-neural-networks-cnn&gt;Convolutional Neural Networks (CNN)&lt;a class=headerlink href=#convolutional-neural-networks-cnn title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;iframe width=420 height=315 src="https://www.youtube.com/embed/FmpDIaiMIeA"&gt; &lt;/iframe&gt;

&lt;h3 id=activation-functions&gt;Activation Functions&lt;a class=headerlink href=#activation-functions title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;There are two types of activation functions: &lt;em&gt;saturating&lt;/em&gt; and &lt;em&gt;non-saturating&lt;/em&gt;. &lt;em&gt;Saturating&lt;/em&gt; means that such function squeezes the input.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I.e. &lt;span class=math&gt;\(f\)&lt;/span&gt; is non-saturating ‚áî&lt;span class=math&gt;\((|\lim_{z\rightarrow‚àí\infty}f(z)|=+\infty)\lor|\lim_{z\rightarrow+\infty}f(z)|=+\infty)\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;and if &lt;span class=math&gt;\(f\)&lt;/span&gt; is NOT non-saturating then it is &lt;em&gt;saturating&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;phew! :)&lt;/p&gt;
&lt;p&gt;Useful &lt;a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0"&gt;article&lt;/a&gt; and &lt;a href="https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions"&gt;ReLu vs Softmax&lt;/a&gt;. Also &lt;a href="http://cs231n.github.io/neural-networks-1/"&gt;part of cs231n&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=recurrent-neural-networksrnn-vs-feedforward-neural-nets-fnn&gt;Recurrent Neural Networks(RNN) Vs Feedforward Neural Nets (FNN)&lt;a class=headerlink href=#recurrent-neural-networksrnn-vs-feedforward-neural-nets-fnn title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;RNN bring much improvement over FNN due to provided internal representation of past events (a.k.a. memory).  RNN-based deep learning was so successful in sequence-to-sequence (s2s) due to its capability to handle sequences well. It should be noted that RNNs are &lt;em&gt;Turing-Complete&lt;/em&gt; &lt;a href="http://dl.acm.org/citation.cfm?id=207284"&gt;(H.T. Siegelmann, 1995)&lt;/a&gt;), and therefore have the capacity to simulate arbitrary procedures.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;RNN is thus also able to learn any measurable s2s mapping to arbitrary accuracy (&lt;a href="https://www.researchgate.net/publication/2495760_On_the_Approximation_Capability_of_Recurrent_Neural_Networks"&gt;B.Hammer, 2000, On the Approximation Capability of Recurrent Neural Networks&lt;/a&gt;). So we get great results in handwriting recognition, text generation and language modeling &lt;a href="https://arxiv.org/abs/1409.3215"&gt;(Sutskever, 2014)&lt;/a&gt;. The availability of information on past events is very important in RNN. The problem however arise, in case when lacking prior knowledge on how long the output sequence will be. It is because the training targets have to be pre-aligned with inputs and standard RNN simply maps input to output. Additionally it is not taking into account the information from past &lt;em&gt;outputs&lt;/em&gt;. One solution is to employ &lt;em&gt;structured prediction&lt;/em&gt; where two RNN can be used to: model input-output dependencies (&lt;em&gt;transcription&lt;/em&gt;) and second to model output-output dependencies (&lt;em&gt;prediction&lt;/em&gt;).  This way each output depends on entire input sequence and all past outputs.&lt;/p&gt;
&lt;p&gt;Another limitation of RNN is the size of internal state. It can be seen on an example of encoder-decoder architecture for &lt;strong&gt;Neural Machine Translation (NMT)&lt;/strong&gt;. Here the encoder gets entire input sequence word by word while updating its internal state. While the decoder decodes it into e.g. other language.&lt;/p&gt;
&lt;h3 id=encoder-decoder-scheme&gt;Encoder-Decoder scheme&lt;a class=headerlink href=#encoder-decoder-scheme title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Seq2seq modeling&lt;/strong&gt; is a synonym of &lt;strong&gt;recurrent neural network based encoder-decoder architectures&lt;/strong&gt; (&lt;a href="https://arxiv.org/abs/1409.3215"&gt;Sutskever et al., 2014;&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1409.0473"&gt;Bahdanau et al., 2014&lt;/a&gt;).
Let us &lt;em&gt;‚Äúunroll‚Äù&lt;/em&gt; the scheme into &lt;span class=math&gt;\(M\)&lt;/span&gt; encoder steps and &lt;span class=math&gt;\(N\)&lt;/span&gt; decoder steps of hidden states &lt;span class=math&gt;\(h\)&lt;/span&gt;.&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="Encoder Decoder architecture " src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/EncoderDecoder_MC.png"&gt;
&lt;br&gt;
Figure 1. &lt;b&gt;Encoder-decoder architecture&lt;/b&gt; ‚Äì example of a general approach for NMT.
An encoder converts a source sentence into a "meaning" vector which is passed through a &lt;i&gt;decoder&lt;/i&gt; to produce a translation.&lt;/p&gt;
&lt;p&gt;In encode-decoder architecture&lt;sup id=sf-Primer-NN-1-back&gt;&lt;a href=#sf-Primer-NN-1 class=simple-footnote title="One can often encounter references to autoencoder (AE) neural network. In case of which we are considering a perfect Encoder-Decoder network where its input is matching its output. In such case the network can reconstruct its own input. The input size is reduced/compressed with hidden layers until the input is compressed into required size (also the size of the target hidden layer) of few variables. From this compressed representation the network tries to reconstruct (decode) the input. Autoencoder is a feature extraction algorithm that helps to find a representation for data and so that the representation can be feed to other algorithms, for example a classifier. Autoencoders can be stacked and trained in a progressive way, we train an autoencoder and then we take the middle layer generated by the AE and use it as input for another AE and so on. Great example of autoencoder on Quora "&gt;1&lt;/a&gt;&lt;/sup&gt;, the&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;encoder ‚Äì maps input data &lt;span class=math&gt;\(x\)&lt;/span&gt; to a different (lower dimensional, compressed ‚Äì i.e. encoded) representation, while the&lt;/li&gt;
&lt;li&gt;decoder ‚Äì maps encoder‚Äôs output new feature representation back into the input data space as a output sequence &lt;span class=math&gt;\(y\)&lt;/span&gt;; left to right one at a time. Decoder generates &lt;span class=math&gt;\(y_{i+1}\)&lt;/span&gt; token by computing new hidden state &lt;span class=math&gt;\(h_{i+1}\)&lt;/span&gt; based on:&lt;ul&gt;
&lt;li&gt;previous hidden &lt;span class=math&gt;\(h_i\)&lt;/span&gt; state&lt;/li&gt;
&lt;li&gt;an embedding &lt;span class=math&gt;\(g_i\)&lt;/span&gt; of the previous target language word &lt;span class=math&gt;\(y_i\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;conditional input &lt;span class=math&gt;\(c_i\)&lt;/span&gt; derived form the encoder output - &lt;span class=math&gt;\(z\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=math&gt;$$input:(x_1, \dots, x_m) \xrightarrow[\text{maps}]{\text{encoder}} z=(z_1,\dots,z_m)\xrightarrow[\text{generates}]{\text{decoder}} output: (y_1,\dots, yn)$$&lt;/div&gt;
&lt;p&gt;One can consider two types of models, with or, without ‚Äúattention‚Äù. The latter assumes &lt;span class=math&gt;\(\forall i c_i=z_m\)&lt;/span&gt; (&lt;a href="https://arxiv.org/abs/1406.1078"&gt;Cho et al., 2014&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Additionally, there is a problem with the encoder‚Äôs state &lt;span class=math&gt;\(h^{E}_m\)&lt;/span&gt;, as a compressed and fixed-length vector. It must contain whole information on the source sentence. This looses some information&lt;sup id=sf-Primer-NN-2-back&gt;&lt;a href=#sf-Primer-NN-2 class=simple-footnote title="Due to its nature of compressing the input into lower dimension."&gt;2&lt;/a&gt;&lt;/sup&gt;. One can try to use some heuristics that can help to overcome this issue and improve performance of RNN:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Organize input.&lt;/strong&gt; Feed the input more than one time or provide input followed by reversed input &lt;sup id=sf-Primer-NN-3-back&gt;&lt;a href=#sf-Primer-NN-3 class=simple-footnote title=" 2014 Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. "&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Provide more memory.&lt;/strong&gt; It turns out that the bigger the size of the memory the better (eg. LSTM applied to language modeling&lt;sup id=sf-Primer-NN-4-back&gt;&lt;a href=#sf-Primer-NN-4 class=simple-footnote title=" 2014 Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals, Recurrent Neural Network Regularization. Normally the dropout would perturb the recurrent connections amplifies the noise and making difficult for LSTM to learn to store information for long time, thus presenting lower performance. Here the authors modify the dropout regularization technique for LSTMs at the same time preserving memory by applying the dropout operator only to the non-recurrent connections."&gt;4&lt;/a&gt;&lt;/sup&gt;) the RNN performs on various tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To avoid the memorization problem there has been research on the &lt;em&gt;attention&lt;/em&gt; mechanism.&lt;/p&gt;
&lt;h3 id=attention-basis&gt;Attention basis&lt;a class=headerlink href=#attention-basis title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The most intuitive &lt;strong&gt;attention definition&lt;/strong&gt; I know is the one contained within &lt;a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/"&gt;paper about Transformer architecture&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We will try to formalize this (let me know your suggestions) in the following form:&lt;/p&gt;
&lt;div class=math&gt;$$\begin{eqnarray} A(q, \{(k,v)\} ) \xrightarrow[\text{output}]{\text{maps as}} \sum_{i=1}^k{\overbrace{f_c(q,k_i)}^{\theta_i}}v_i, q \in Q, k \in K, v \in V \end{eqnarray}$$&lt;/div&gt;
&lt;div class=math&gt;$$Q, K, V ‚Äì vector space, f_c- compatibility function$$&lt;/div&gt;
&lt;p&gt;Concretely, an attention  mechanism is distribution of weights over the input states. It takes any number of inputs &lt;span class=math&gt;\({x_1, ..., x_k}\)&lt;/span&gt;, and a query &lt;span class=math&gt;\(q\)&lt;/span&gt;, and then produces weights &lt;span class=math&gt;\({\theta_1, ..., \theta_k}\)&lt;/span&gt; for each input. This  measures how much each input interacts with (or answers) the query. The output of the attention mechanism, &lt;span class=math&gt;\(out\)&lt;/span&gt;, is therefore the weighted average of its inputs:
&lt;/p&gt;
&lt;div class=math&gt;$$ \begin{eqnarray} out = \sum_{i=1}^k \theta_i x_i \end{eqnarray}$$&lt;/div&gt;
&lt;p&gt;Hence, networks that use attention, attend only on a part of the input sequence while still providing the output sentence. So one can image it as giving an auxiliary input for the network in form of linear combination. What is the clue here is that the weights of the linear combination are controlled by the network. It has been tested across multiple areas such as NMT or speech recognition.&lt;/p&gt;
&lt;p&gt;In an encoder-decoder architecture for the embedding (fixed size vector) of a long sentence brings the problem of long dependency (e.g. in fixed-size encoded vector &lt;span class=math&gt;\(h^E_N\)&lt;/span&gt;, where end word of the sentence depends on the starting word &lt;span class=math&gt;\(h^E_1\)&lt;/span&gt;). And long dependencies, is where RNN have problems. Even though ‚Äúhacks‚Äù such as reverse/double feeding of the source sentence, or the LSTMs (memory), are sometimes improving the performance however, they do not always work perfectly&lt;sup id=sf-Primer-NN-5-back&gt;&lt;a href=#sf-Primer-NN-5 class=simple-footnote title="E.g. reversing sentence in language where the first word of output is dependent on the last word of an input will decrease  performance even worse. Then the first output word would depend on a word that is last in the processing chain of a reversed input."&gt;5&lt;/a&gt;&lt;/sup&gt;. It is because the state and the gradient in LSTM would start to make the gradient vanish. It is called ‚Äúlong‚Äù time memory but its not that long to work e.g. for 2000 words. This is where &lt;em&gt;convolutions&lt;/em&gt; came in.&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="WaveNet architecture" src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/WaveNet.gif" title="WaveNet architecture"&gt;
&lt;img alt="ByteNet architecture" src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/ByteNet.png" title="ByteNet architecture"&gt;
&lt;br&gt;
Figure 2. WaveNet (left; sound) (&lt;a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/"&gt;image courtesy&lt;/a&gt;) and ByteNet (right; NLP) architecture (Image acquired from &lt;a href="https://arxiv.org/abs/1610.10099"&gt;Neural Machine Translation in Linear Time, Kalchbrenner et. al 2016&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This is why the fixed size encoding might be a bottleneck of performance. This is where the attention comes in.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Attention&lt;/em&gt;  is used as an alternative to memorizing and input manipulations. Here the model search for parts  of a source sentence (not fixed size vector) that are relevant to predicting a target word, and it should attend to, based on what it has learned in the past &lt;a href="https://arxiv.org/abs/1409.0473"&gt;Bahdanu et al., 2016&lt;/a&gt;.&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="Attention architecture" src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/attention.png" title="Attention architecture2"&gt;
&lt;br&gt;
Figure 2. Attention architecture.&lt;/p&gt;
&lt;p&gt;In this case, the larger  &lt;span class=math&gt;\(\theta_{1,2}\)&lt;/span&gt;  would be the more decoder pays attention for the &lt;span class=math&gt;\(y_3\)&lt;/span&gt; (third output word) in relation to other words as all weights (&lt;span class=math&gt;\(\theta\)&lt;/span&gt;s) are normalized to sum to &lt;span class=math&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="Attention architecture" src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/EncDecAttention.gif" title="Attention architecture3"&gt;
&lt;br&gt;
Figure 3. ‚ÄúAttention‚Äù; the blue link transparency represents how much the decoder pays attention to an encoded word. Less transparent, more attention. &lt;a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html"&gt;Google Blog&lt;/a&gt; also &lt;a href="https://google.github.io/seq2seq/"&gt;general-purpose encoder-decoder framework for Tensorflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It is well visualized in an &lt;span class=math&gt;\(\theta\)&lt;/span&gt; matrix for French-English translation (See Figure Adopted form &lt;a href="https://arxiv.org/abs/1409.0473"&gt;Bahdanu et al., 2016&lt;/a&gt;)&lt;/p&gt;
&lt;p align=center&gt;&lt;img alt="Attention matrix " src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/attentionmatrix.png"&gt;
&lt;br&gt;
Figure 4. Attention matrix. Adopted form &lt;a href="https://arxiv.org/abs/1409.0473"&gt;Bahdanu et al., 2016&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;In French input sentence two words ‚Äú&lt;em&gt;la zone&lt;/em&gt;‚Äù are especially target for attention when translating to English ‚Äú&lt;em&gt;Area&lt;/em&gt;‚Äù.&lt;/p&gt;
&lt;p&gt;This matrix of size &lt;em&gt;InputSize&lt;/em&gt; &lt;span class=math&gt;\(\times\)&lt;/span&gt; &lt;em&gt;OutputSize&lt;/em&gt; is filled by calculating attention for each &lt;em&gt;output&lt;/em&gt; symbol toward over every &lt;em&gt;input&lt;/em&gt; symbol. This gives us &lt;span class=math&gt;\(\#Input^{\#Output}\)&lt;/span&gt;. For a longer sequences it might grow intensively.
This approach requires a complete look-up over all input output elements, which is not actually working as an biological attention would. Intuitively attention should discard irrelevant objects without the need to interacting with them.
What is called ‚Äúattention‚Äù therefore is simply a kind of memory that is available for decoder while producing every single output element. It does not need the fixed-size encoded vector of the entire input. The weights only decide which symbols/words to get from the input-memory of the encoder.
This so called attention is the subject of further development in form of &lt;a href="https://arxiv.org/abs/1503.08895"&gt;End-To-End Memory Networks&lt;/a&gt; where recurrent attention is used where  where multiple computational steps (hops) are performed per output symbol. This includes reading the same sequence many times before generating output and also changing the memory content at each step.&lt;/p&gt;
&lt;p&gt;We can of course employ backpropagation to accustom the weights in an end-to-end learning  model.&lt;/p&gt;
&lt;h3 id=layer-connections&gt;Layer connections &lt;!--http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref:6--&gt;&lt;a class=headerlink href=#layer-connections title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In Deep Learning networks architectures the problem of vanishing gradient is very common issue. Thus, some methods has been develop to mitigate its influence on the networks‚Äô efficiency.  &lt;/p&gt;
&lt;h4 id=highway-layers&gt;Highway layers&lt;a class=headerlink href=#highway-layers title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;h4 id=residual-connections&gt;Residual connections&lt;a class=headerlink href=#residual-connections title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Models with many layers often rely on shortcut or residual connections (Zhou et al., 2016; Wu et al., 2016). This trick has become main factor for &lt;a href="https://arxiv.org/abs/1512.03385"&gt;He et al. 2016 CVPR&lt;/a&gt; winning the ImageNet 2016 Residual connection is a connection between layers that adds the input &lt;span class=math&gt;\(x\)&lt;/span&gt; of the current layer to its output via a short-cut connection.&lt;/p&gt;
&lt;div class=math&gt;$$ h = f(Wx+b) + \mathbf{x} $$&lt;/div&gt;
&lt;p&gt;Residual connection helps with the &lt;em&gt;vanishing gradient&lt;/em&gt; problem because even if the layer nonlinearity &lt;span class=math&gt;\(f\)&lt;/span&gt; is not giving result the output then becomes the identity function in form of the &lt;span class=math&gt;\(\mathbf{x}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=dense-connections&gt;Dense connections&lt;a class=headerlink href=#dense-connections title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;h3 id=position-embeddings&gt;Position Embeddings&lt;a class=headerlink href=#position-embeddings title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Usually used in non-recurrent networks (e.g. CNNs) that needs to store the order of the sequence‚Äôs input tokens in a way different than recurrent one. The RNN learn the exact position in sequence through the recurrent hidden state computation.&lt;/p&gt;
&lt;h4 id=autoregressive-ar&gt;Autoregressive (AR)&lt;a class=headerlink href=#autoregressive-ar title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;The problem of auto-regression is conditional mean of the distribution of future observations &lt;a href="https://arxiv.org/abs/1703.04122"&gt;Binkowski et al, 2017&lt;/a&gt;. In an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable. The term &lt;em&gt;autoregression&lt;/em&gt; indicates that it is a regression of the variable against itself. In NN this refers to autoregressive understood as each unit receives input both from the preceding layer and the preceding units within the same layer.&lt;/p&gt;
&lt;h3 id=end-to-end-learningtrainig&gt;End-to-End learning/trainig&lt;a class=headerlink href=#end-to-end-learningtrainig title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Simplest way to train a model (i.e. to learn) is to put input at one end and get output on the other end. With NN an end-to-end learning would simply mean optimize network weights base on &lt;strong&gt;single model&lt;/strong&gt; with input and output.&lt;/p&gt;
&lt;p&gt;There are however some cases when one model is not enough to achieve the desired output. It would then require a pipeline of independently trained models. It is most of the the case when input and output are of two distinct domains. Another case might be when NN has to many layers to fit into memory.  Hence, it is required to divide this one ‚Äúbig‚Äù NN into a pipeline of smaller ones. As a side note, this decomposition technique might not be effective because the optimizations would be done locally based only on intermediate outputs.&lt;/p&gt;
&lt;h5 id=examples&gt;Examples:&lt;a class=headerlink href=#examples title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In case of a robot trained to move based on vision. One model might be used to pre-process the vision input (raw pixels) representation and pass it as input for another model responsible for decision process of which robot‚Äôs leg to move next. &lt;em&gt;image-to-motion&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Image captioning ‚Äì transforming raw image pixel information into text describing the image &lt;em&gt;image-to-text&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Speech recognition - Transform speech to text  &lt;em&gt;sound-to-text&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=hyperparameters-aka-meta-parameters-free-parameters&gt;Hyperparameters (aka meta-parameters, free-parameters)&lt;a class=headerlink href=#hyperparameters-aka-meta-parameters-free-parameters title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The crux of ML is finding (i.e. model training) a math formula (i.e. the model) with parameters that fit best into the data. However, the training is not able to find some higher level properties of model such as complexity or speed of learning straight from the data. Those properties are referred to as hyperparameters. For each trained model hyperparameters are predefined before even training process starts. Their values are important, however requires additional work. This work is done by trying different values for hyperparameters and training different models on them. Using the tests  than, decides which values of hyperparameters should be chosen i.e. is the fastest to achieve the goal, requires less steps etc. So hyperparameters can be determined from the data indirectly by using model selection.&lt;/p&gt;
&lt;p&gt;General approach to ML problem is making four decisions choosing the exact:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model Type ‚Äì e.g. FNN, RNN, SVM, etc&lt;/li&gt;
&lt;li&gt;Architecture ‚Äì e.g. for RNN you choose number of hidden layers, number of units per hidden layer&lt;/li&gt;
&lt;li&gt;Training parameters ‚Äì e.g. decide learning rate, batch size&lt;/li&gt;
&lt;li&gt;Model parameters ‚Äì model training finds the model parameters such as weights and biases in NN
Hence, the hyper parameters are those considered in the &lt;em&gt;training parameters&lt;/em&gt; and &lt;em&gt;architecture&lt;/em&gt; steps.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=examples-of-hyperparameters&gt;Examples of hyperparameters:&lt;a class=headerlink href=#examples-of-hyperparameters title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;learning rate in gradient algorithms, number of hidden layers, number of clusters in a k-means clustering, number of leaves or depth of a tree, batch size in minibatch gradient descent, regularization parameter     &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=transfer-learning&gt;Transfer learning&lt;a class=headerlink href=#transfer-learning title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Transfer learning is used in context of reinforced or supervised learning. Deep learning requires large datasets to perform well (avoiding overfitting etc.). The problem however arises when there is not enough data for a new task. Here is the idea to use other already existing and large datasets to fine-tunning NN to become useful for this new task with small amount of data (e.g. using &lt;strong&gt;features&lt;/strong&gt; pre-trained from CNN (ConvNet) can be feed linear support vector machine (SVM) ). In other words, one can transfer the learned representation to another problem. We must however avoid negative transfer as it can slow down training of target task.
Searching for function &lt;span class=math&gt;\(g()\)&lt;/span&gt; while having pre-trained &lt;span class=math&gt;\(h()\)&lt;/span&gt; use projection of new inputs &lt;span class=math&gt;\(x_i\)&lt;/span&gt; like this: &lt;span class=math&gt;\((h(g(x_i)))\)&lt;/span&gt;.&lt;/p&gt;
&lt;h5 id=examples_1&gt;Examples:&lt;a class=headerlink href=#examples_1 title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Positive transfer: If you learned previously how to classify a rotten vegetable form not rotten one you can apply this representation of rottenness into fruits even though you have never seen a rotten fruit before.&lt;/li&gt;
&lt;li&gt;Negative transfer: learning one skill makes learning second  skill more difficult. If a kickboxer is about to train how to box. it might be hard to understand what is boxing unless he will be warned that he is not allowed to use kicks to box along with rules.&lt;/li&gt;
&lt;li&gt;Proactive transfer:  When a model learned in the past affects the new model to be learned&lt;/li&gt;
&lt;li&gt;Retroactive transfer: When a new model affects previously learned one.&lt;/li&gt;
&lt;li&gt;Bilateral transfer: Robot learned to use left manipulator now must use also right manipulator however in a bit different symmetry.&lt;/li&gt;
&lt;li&gt;Zero transfer: Two models are independent.&lt;/li&gt;
&lt;li&gt;Stimulus generalization:  Knowing what is rotten fruit does not mean that if you find a rusty metal it is the same but you generalize enough to decide that in both cases it is unusable but not in the same way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=fine-tuning&gt;Fine tuning&lt;a class=headerlink href=#fine-tuning title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Fine tuning is considered mainly in context of supervised learning. When choosing best hyperparameters for an algorithm, fine tuning involves using a large set of mechanisms that solve this problem. When adjusting the the behaviors of the algorithm to further improve performance without manipulating the model itself.
When fine tuning pre-trained model might be considered equivalent to transfer learning. This is true if the data used during the fine tuning procedure is of different nature than the data that the pre-trained model has been trained on.&lt;/p&gt;
&lt;h5 id=examples_2&gt;Examples:&lt;a class=headerlink href=#examples_2 title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Finding the best hyperparameters for the model&lt;/li&gt;
&lt;li&gt;In transfer learning instead of bringing the frozen pre-trained model one cane make it dynamic and let adapt more to the new task.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=evolutionary-algorithms-evolutionary-computation-ec&gt;Evolutionary algorithms (Evolutionary Computation - EC)&lt;a class=headerlink href=#evolutionary-algorithms-evolutionary-computation-ec title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In machine learning problems, you typically have two components:&lt;/p&gt;
&lt;div class=highlight&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class=mf&gt;1.&lt;/span&gt; &lt;span class=n&gt;The&lt;/span&gt; &lt;span class=n&gt;model&lt;/span&gt; &lt;span class=p&gt;(&lt;/span&gt;&lt;span class=n&gt;function&lt;/span&gt; &lt;span class=n&gt;class&lt;/span&gt;&lt;span class=p&gt;,&lt;/span&gt; &lt;span class=n&gt;etc&lt;/span&gt;&lt;span class=p&gt;)&lt;/span&gt;
&lt;span class=mf&gt;2.&lt;/span&gt; &lt;span class=n&gt;Methods&lt;/span&gt; &lt;span class=n&gt;of&lt;/span&gt; &lt;span class=n&gt;fitting&lt;/span&gt; &lt;span class=n&gt;the&lt;/span&gt; &lt;span class=n&gt;model&lt;/span&gt; &lt;span class=p&gt;(&lt;/span&gt;&lt;span class=n&gt;optimization&lt;/span&gt; &lt;span class=n&gt;algorithms&lt;/span&gt;&lt;span class=p&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Neural networks are a model: given a layout and a setting of weights, the neural net produces some output. There exist some canonical methods of fitting neural nets, such as backpropagation, contrastive divergence, etc. However, the big point of neural networks is that if someone gave you the 'right' weights, you'd do well on the problem.&lt;/p&gt;
&lt;p&gt;Evolutionary algorithms address the second part -- fitting the model. Again, there are some canonical models that go with evolutionary algorithms: for example, evolutionary programming typically tries to optimize over all programs of a particular type. However, EAs are essentially a way of finding the right parameter values for a particular model. Usually, you write your model parameters in such a way that the crossover operation is a reasonable thing to do and turn the EA crank to get a reasonable setting of parameters out.&lt;/p&gt;
&lt;p&gt;Evolutionary algorithms are one class of strategies that can be used in machine learning, just like backpropagation and many others.&lt;/p&gt;
&lt;p&gt;Evolutionary algorithms usually converge slowly because they make no use of gradient information. On the other hand they provide at least a chance to escape from local optima and find the global one.&lt;/p&gt;
&lt;p&gt;Now, you could, for example, use evolutionary algorithms to train a neural network and I'm sure it's been done. However, the critical bit that EA require to work is that the crossover operation must be a reasonable thing to do -- by taking part of the parameters from one reasonable setting and the rest from another reasonable setting, you'll often end up with an even better parameter setting. Most times EA is used, this is not the case and it ends up being something like simulated annealing, only more confusing and inefficient.&lt;/p&gt;
&lt;h3 id=learning-rate-schemes-aka-learning-rate-annealing-or-adaptive-learning-rates&gt;Learning rate schemes (aka learning rate annealing or adaptive learning rates)&lt;a class=headerlink href=#learning-rate-schemes-aka-learning-rate-annealing-or-adaptive-learning-rates title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Changing the learning rate along training with e.g. Stochastic Gradient Descent (SGD) increases performance and reduce training time. With adaptive change to &lt;em&gt;learning rate&lt;/em&gt; along training one can use different schemes/schedules on how the learning rate should change.&lt;br&gt;
See &lt;a href="http://cs231n.github.io/neural-networks-3/#anneal"&gt;cs231n&lt;/a&gt; on learning rate annealing and &lt;a href="https://www.tensorflow.org/versions/r0.12/api_docs/python/train/decaying_the_learning_rate"&gt;Tensorflow decaying learning rate&lt;/a&gt;.&lt;/p&gt;
&lt;!--
### ToDos

* training (aka learning: Fitting the "hyper parameters" of the model for all the examples) vs inference (Learning the values of the latent variables for a specific example) vs prediciton https://www.quora.com/What-is-the-difference-between-inference-and-learning
 https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/
* generalize from extremely small data sets classification accuracy of 92.07%. http://cognitivemedium.com/rmnist_anneal_ensemble
* AutoML https://research.googleblog.com/2017/11/automl-for-large-scale-image.html
* wod2vec http://mccormickml.com/2016/04/27/word2vec-resources/
* Pooling
* element embedding
* ROUGE, preplexity, BLEU
* optimizers: GD (BGD, SGD, MBGD), AdaGrad, Adam, ...)
* regularization (L1,and L2, dropout)
* generalization
* Stacked NN (SNN) vs Deep NN (DNN)
* reverse-mode differentiation (a.k.a. backpropagation) http://colah.github.io/posts/2015-08-Backprop/
* 1x(Feed forward) + 1x(backpropagation) = epoch (eg 10-15 epochs)
* Loss (aka cost) functions: Quadratic vs cross-entropy  http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function
http://colah.github.io/posts/2015-09-Visual-Information/   https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation
* Pointer network
* capsule network https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc
* Conjoint analysis https://www.kdnuggets.com/2017/11/conjoint-analysis-primer.html
--&gt;
&lt;hr&gt;
&lt;h4 id=references&gt;References:&lt;a class=headerlink href=#references title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;ol class=simple-footnotes&gt;&lt;li id=sf-Primer-NN-1&gt;One can often encounter references to &lt;strong&gt;autoencoder (AE)&lt;/strong&gt; neural network. In case of which we are considering a perfect Encoder-Decoder network where its input is matching its output. In such case the network can reconstruct its own input. The input size is reduced/compressed with hidden layers until the input is compressed into required size (also the size of the target hidden layer) of few variables. From this compressed representation the network tries to reconstruct (decode) the input. Autoencoder is a feature extraction algorithm that helps to find a representation for data and so that the representation can be feed to other algorithms, for example a classifier. Autoencoders can be stacked and trained in a progressive way, we train an autoencoder and then we take the middle layer generated by the AE and use it as input for another AE and so on. &lt;a href="https://www.quora.com/What-is-an-auto-encoder-in-machine-learning"&gt;Great example of autoencoder on Quora&lt;/a&gt;  &lt;a href=#sf-Primer-NN-1-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Primer-NN-2&gt;Due to its nature of compressing the input into lower dimension. &lt;a href=#sf-Primer-NN-2-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Primer-NN-3&gt; 2014 Ilya Sutskever, Oriol Vinyals, and Quoc V Le. &lt;a href="https://arxiv.org/abs/1409.3215"&gt;Sequence to sequence learning with neural networks.&lt;/a&gt;  &lt;a href=#sf-Primer-NN-3-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Primer-NN-4&gt; 2014 Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals, &lt;a href="https://arxiv.org/abs/1409.2329"&gt;Recurrent Neural Network Regularization&lt;/a&gt;. Normally the dropout would perturb the recurrent connections amplifies the noise and making difficult for LSTM to learn to store information for long time, thus presenting lower performance. Here the authors modify the dropout regularization technique for LSTMs at the same time preserving memory by applying the dropout operator only to the non-recurrent connections. &lt;a href=#sf-Primer-NN-4-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Primer-NN-5&gt;E.g. reversing sentence in language where the first word of output is dependent on the last word of an input will decrease  performance even worse. Then the first output word would depend on a word that is last in the processing chain of a reversed input. &lt;a href=#sf-Primer-NN-5-back class=simple-footnote-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><category term="ML Dojo"></category><category term="basics"></category></entry><entry><title>The power of patterns ‚Äì embrace the hurricanes.</title><link href="https://mchromiak.github.io/articles/2017/Aug/25/embrace-hurricane/" rel="alternate"></link><published>2017-08-25T19:30:00+02:00</published><updated>2017-09-10T19:30:00+02:00</updated><author><name>Micha≈Ç Chromiak</name></author><id>tag:mchromiak.github.io,2017-08-25:/articles/2017/Aug/25/embrace-hurricane/</id><summary type="html">&lt;p&gt;Reducing uncertainty is very challenging and important task in many areas. It it literally often a matter of live and death. If the prediction is accurate, it is easy to imagine how meaningful it is, especially in cases such as weather forecasts in case of hurricanes.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Recent hurricane Harvey has become one of the most devastating and dangerous of its kind along the history. However, the question arises if our science has the tools to do something to help in such cases. Of course, currently there Is no technology that can physically eliminate the threat of hurricane by neutralizing it or changing its path but one can think of a more mathematical way of predicting its behavior.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="Forecast 2005-2015" src="https://mchromiak.github.io/articles/2017/Aug/25/embrace-hurricane/img/cover_irma.jpg"&gt;
&lt;br&gt;
Figure 1. Forecast 2005-2015 Katrina&lt;/p&gt;
&lt;p&gt;Now we all know, that predicting weather is not an easy task which is very resource consuming and above all is unstable and its accuracy decrees dramatically over time. There are two main factors deciding on  safety of inhabitants of a potentially endangered area.  &lt;br&gt;
First is the time. Due to meteorologists observations we can track hurricanes quite well, so even thought hurricane can change its characteristics (e.g. wind speed, direction etc.), we know where it currently is localized. The satellites and aerial observations makes a decent job here. Therefore the speed and vector of the storm may change, but we have means to track it down.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="Forecast for Hurricane Harvey August 24" src="https://mchromiak.github.io/articles/2017/Aug/25/embrace-hurricane/img/forecast1.png"&gt;
&lt;br&gt;
Figure 2. Forecast for Hurricane Harvey August 24.&lt;/p&gt;
&lt;p&gt;However, the question is: where will it hit next? This is how we came to the second factor which is determining the evacuation area. As it is not possible to predict the exact area that the hurricane will strike the evacuation must be urged over vast areas that potentially can be in danger. This results in millions of people urged by the authorities to evacuate. While evacuation always covers all areas likely to be destroyed by the storm, the predictions might help to narrow the area of hurricane impact. This gives us a chance to relocate rescue units and resources so that the evacuation could be more accurately supervised in areas that will suffer from the actual hurricane hit. Additionally the better forecasting brings the potential to estimate the losses and future demands of supplies and help for the most endangered areas.&lt;/p&gt;
&lt;h2 id="patterns-to-the-rescue"&gt;Patterns to the rescue&lt;a class="headerlink" href="#patterns-to-the-rescue" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Due to the revolution of AI today we can use a fresh take on hurricane forecasting. Contingency plans must be ready for any occasion, but in a final call they must meet up the dynamic changes in weather, accordingly. Millions of decisions must be made along with forecasting machine learning pipelines constantly refining, based on data conclusions while eliminating noise and zero-in on the important data.&lt;/p&gt;
&lt;p&gt;The Hurricane Harvey was target to a machine learning systems that were based on global hurricane forecasts like the Canadian Model, the European Centre, and the U.S.‚Äô National Hurricane Centre model. The system used the pattern recognition with to bias-correct forecasts originating from the historical performance of forecasts along last 30 years.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="Forecast for Hurricane Harvey August 30" src="https://mchromiak.github.io/articles/2017/Aug/25/embrace-hurricane/img/forecast2.png"&gt;
&lt;br&gt;
Figure 3. This figure shows the overall recorded track of Hurricane Harvey as of August 30th.&lt;/p&gt;
&lt;p&gt;As the weather forecast becomes more ‚Äúintelligent‚Äù it is extremely important to work on this kind of application further due to enormous possibilities to save not only economy but also potentially many human lives.&lt;/p&gt;
&lt;p align="center"&gt;&lt;img alt="Forecast for Hurricane Harvey two days prior to making landfall" src="https://mchromiak.github.io/articles/2017/Aug/25/embrace-hurricane/img/forecast3.png"&gt;
&lt;br&gt;
Figure 4. This figure from &lt;a href="https://www.weatheranalytics.com/solutions/beacon/"&gt;Weather Analytics‚Äô Beacon Hurricane&lt;/a&gt; platform shows a forecast of Harvey two days prior to making landfall.&lt;/p&gt;
&lt;p&gt;Let us hope that upcoming cat.5 Irma Hurricane will be a well foretasted one.&lt;/p&gt;
&lt;h4 id="side-notes"&gt;Side notes&lt;a class="headerlink" href="#side-notes" title="Permanent link"&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;There is also an interesting news &lt;a href="https://www.today.com/video/hurricane-harvey-damage-is-being-assessed-by-new-technology-1037443651962"&gt;1&lt;/a&gt;, &lt;a href="https://www.eagleview.com/2017/09/providing-rapid-answers-machine-learning-post-hurricane-harvey/"&gt;2&lt;/a&gt; and article on &lt;a href="https://medium.com/towards-data-science/hurricane-harvey-insurtech-case-study-visual-intelligence-is-transforming-claim-response-times-3275042fbd8d"&gt;how to InsureTech&lt;/a&gt; companies use visual intelligence to transform claim response times using unstructured data. The insurers use machine learning to approximate a claim size likely due to the hurricane event. The visual inspection can be additionally processed based on aerial and satellites images. This can be also useful for deciding if the claim would be directly related to hurricane.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;
Unfortunately this has again came up to a point where the predictions failed when  today (as of 10th , Sept.  2017) Hurricane Irma has striked the south of Florida. It was forecast that it will focus on Miami, FL area, instead it rushed towards the west coast of Florida moving towards Tampa.&lt;/p&gt;</content><category term="Applications"></category><category term="application"></category><category term="PatternRecognition"></category></entry></feed>