<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Micha≈Ç Chromiak's blog - ML Dojo</title><link href="https://mchromiak.github.io/" rel="alternate"></link><link href="/feeds/ml-dojo.atom.xml" rel="self"></link><id>https://mchromiak.github.io/</id><updated>2017-10-01T19:30:00+02:00</updated><entry><title>Neural Networks Primer</title><link href="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/" rel="alternate"></link><published>2017-09-01T19:30:00+02:00</published><updated>2017-10-01T19:30:00+02:00</updated><author><name>Micha≈Ç Chromiak</name></author><id>tag:mchromiak.github.io,2017-09-01:/articles/2017/Sep/01/Primer-NN/</id><summary type="html">&lt;p&gt;When you approach a new term you often find some Wiki page, Quora answers blogs and it sometimes might take some time before you find the true ground up, clear definition with meaningful example. I will put here the most intuitive explanations of basic topics. Due to extended nature of aspects and terms that are used across NN area, in this post I will place condensed definitions and a brief explanations ‚Äì just to understand the intuition of terms that are mentioned in other posts along this blog.&lt;/p&gt;</summary><content type="html">&lt;p&gt;If any of the topic will grow enough I will put it into a separate post. To get in-depth understanding I recommend for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NLP:&lt;/strong&gt; the Yoav Goldberg's, 2016 &lt;a href="https://www.jair.org/media/4992/live-4992-9623-jair.pdf"&gt;A Primer on Neural Network Models for Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=rnn-primer-with-gradient-descent-gd&gt;RNN primer with Gradient Descent (GD)&lt;a title="Permanent link" class=headerlink href=#rnn-primer-with-gradient-descent-gd&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;iframe frameborder=0 width=420 height=315 src="https://www.youtube.com/embed/aircAruvnKk" allowfullscreen=""&gt; &lt;/iframe&gt;
&lt;iframe frameborder=0 width=420 height=315 src="https://www.youtube.com/embed/IHZwWFHWa-w" allowfullscreen=""&gt; &lt;/iframe&gt;
&lt;iframe frameborder=0 width=420 height=315 src="https://www.youtube.com/embed/Ilg3gGewQ5U" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;iframe frameborder=0 width=420 height=315 src="https://www.youtube.com/embed/ILsA4nyG7I0" allowfullscreen=""&gt; &lt;/iframe&gt;&lt;/p&gt;
&lt;h3 id=convolutional-neural-networks-cnn&gt;Convolutional Neural Networks (CNN)&lt;a title="Permanent link" class=headerlink href=#convolutional-neural-networks-cnn&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;iframe width=420 height=315 src="https://www.youtube.com/embed/FmpDIaiMIeA"&gt; &lt;/iframe&gt;&lt;/p&gt;
&lt;h3 id=activation-functions&gt;Activation Functions&lt;a title="Permanent link" class=headerlink href=#activation-functions&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;There are two types of activation functions: &lt;em&gt;saturating&lt;/em&gt; and &lt;em&gt;non-saturating&lt;/em&gt;. &lt;em&gt;Saturating&lt;/em&gt; means that such function squeezes the input.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I.e. &lt;span class=math&gt;\(f\)&lt;/span&gt; is non-saturating ‚áî&lt;span class=math&gt;\((|\lim_{z\rightarrow‚àí\infty}f(z)|=+\infty)\lor|\lim_{z\rightarrow+\infty}f(z)|=+\infty)\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;and if &lt;span class=math&gt;\(f\)&lt;/span&gt; is NOT non-saturating then it is &lt;em&gt;saturating&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;phew! :)&lt;/p&gt;
&lt;p&gt;Useful &lt;a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0"&gt;article&lt;/a&gt; and &lt;a href="https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions"&gt;ReLu vs Softmax&lt;/a&gt;. Also &lt;a href="http://cs231n.github.io/neural-networks-1/"&gt;part of cs231n&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=recurrent-neural-networksrnn-vs-feedforward-neural-nets-fnn&gt;Recurrent Neural Networks(RNN) Vs Feedforward Neural Nets (FNN)&lt;a title="Permanent link" class=headerlink href=#recurrent-neural-networksrnn-vs-feedforward-neural-nets-fnn&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;RNN bring much improvement over FNN due to provided internal representation of past events (a.k.a. memory).  RNN-based deep learning was so successful in sequence-to-sequence (s2s) due to its capability to handle sequences well. It should be noted that RNNs are &lt;em&gt;Turing-Complete&lt;/em&gt; &lt;a href="http://dl.acm.org/citation.cfm?id=207284"&gt;(H.T. Siegelmann, 1995)&lt;/a&gt;), and therefore have the capacity to simulate arbitrary procedures.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;RNN is thus also able to learn any measurable s2s mapping to arbitrary accuracy (&lt;a href="https://www.researchgate.net/publication/2495760_On_the_Approximation_Capability_of_Recurrent_Neural_Networks"&gt;B.Hammer, 2000, On the Approximation Capability of Recurrent Neural Networks&lt;/a&gt;). So we get great results in handwriting recognition, text generation and language modeling &lt;a href="https://arxiv.org/abs/1409.3215"&gt;(Sutskever, 2014)&lt;/a&gt;. The availability of information on past events is very important in RNN. The problem however arise, in case when lacking prior knowledge on how long the output sequence will be. It is because the training targets have to be pre-aligned with inputs and standard RNN simply maps input to output. Additionally it is not taking into account the information from past &lt;em&gt;outputs&lt;/em&gt;. One solution is to employ &lt;em&gt;structured prediction&lt;/em&gt; where two RNN can be used to: model input-output dependencies (&lt;em&gt;transcription&lt;/em&gt;) and second to model output-output dependencies (&lt;em&gt;prediction&lt;/em&gt;).  This way each output depends on entire input sequence and all past outputs.&lt;/p&gt;
&lt;p&gt;Another limitation of RNN is the size of internal state. It can be seen on an example of encoder-decoder architecture for &lt;strong&gt;Neural Machine Translation (NMT)&lt;/strong&gt;. Here the encoder gets entire input sequence word by word while updating its internal state. While the decoder decodes it into e.g. other language.&lt;/p&gt;
&lt;h3 id=encoder-decoder-scheme&gt;Encoder-Decoder scheme&lt;a title="Permanent link" class=headerlink href=#encoder-decoder-scheme&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Seq2seq modeling&lt;/strong&gt; is a synonym of &lt;strong&gt;recurrent neural network based encoder-decoder architectures&lt;/strong&gt; (&lt;a href="https://arxiv.org/abs/1409.3215"&gt;Sutskever et al., 2014;&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1409.0473"&gt;Bahdanau et al., 2014&lt;/a&gt;).
Let us &lt;em&gt;‚Äúunroll‚Äù&lt;/em&gt; the scheme into &lt;span class=math&gt;\(N\)&lt;/span&gt; encoder steps and &lt;span class=math&gt;\(M\)&lt;/span&gt; decoder steps of hidden states &lt;span class=math&gt;\(h\)&lt;/span&gt;.&lt;/p&gt;
&lt;p align=center&gt;&lt;img src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/EncoderDecoder_MC.png" alt="Encoder Decoder architecture "&gt;
&lt;br&gt;
Figure 1. &lt;b&gt;Encoder-decoder architecture&lt;/b&gt; ‚Äì example of a general approach for NMT.
An encoder converts a source sentence into a "meaning" vector which is passed through a &lt;i&gt;decoder&lt;/i&gt; to produce a translation.&lt;/p&gt;
&lt;p&gt;In encode-decoder architecture&lt;sup id=sf-Primer-NN-1-back&gt;&lt;a title="One can often encounter references to autoencoder (AE) neural network. In case of which we are considering a perfect Encoder-Decoder network where its input is matching its output. In such case the network can reconstruct its own input. The input size is reduced/compressed with hidden layers until the input is compressed into required size (also the size of the target hidden layer) of few variables. From this compressed representation the network tries to reconstruct (decode) the input. Autoencoder is a feature extraction algorithm that helps to find a representation for data and so that the representation can be feed to other algorithms, for example a classifier. Autoencoders can be stacked and trained in a progressive way, we train an autoencoder and then we take the middle layer generated by the AE and use it as input for another AE and so on. Great example of autoencoder on Quora " class=simple-footnote href=#sf-Primer-NN-1&gt;1&lt;/a&gt;&lt;/sup&gt;, the&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;encoder ‚Äì maps input data &lt;span class=math&gt;\(x\)&lt;/span&gt; to a different (lower dimensional, compressed ‚Äì i.e. encoded) representation, while the&lt;/li&gt;
&lt;li&gt;decoder ‚Äì maps encoder‚Äôs output new feature representation back into the input data space as a output sequence &lt;span class=math&gt;\(y\)&lt;/span&gt;; left to right one at a time. Decoder generates &lt;span class=math&gt;\(y_{i+1}\)&lt;/span&gt; token by computing new hidden state &lt;span class=math&gt;\(h_{i+1}\)&lt;/span&gt; based on:&lt;ul&gt;
&lt;li&gt;previous hidden &lt;span class=math&gt;\(h_i\)&lt;/span&gt; state&lt;/li&gt;
&lt;li&gt;an embedding &lt;span class=math&gt;\(g_i\)&lt;/span&gt; of the previous target lonaguage word &lt;span class=math&gt;\(y_i\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;conditional input &lt;span class=math&gt;\(c_i\)&lt;/span&gt; derived form the encoder output - &lt;span class=math&gt;\(z\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=math&gt;$$input:(x_1, \dots, x_m) \xrightarrow[\text{maps}]{\text{encoder}} z=(z_1,\dots,z_m)\xrightarrow[\text{generates}]{\text{decoder}} output: (y_1,\dots, yn)$$&lt;/div&gt;
&lt;p&gt;One can consider two types of models, with or, without ‚Äúattention‚Äù. The latter assumes &lt;span class=math&gt;\(\forall i c_i=z_m\)&lt;/span&gt; (&lt;a href="https://arxiv.org/abs/1406.1078"&gt;Cho et al., 2014&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Additionally, there is a problem with the encoder‚Äôs state &lt;span class=math&gt;\(h^{E}_m\)&lt;/span&gt;, as a compressed and fixed-length vector. It must contain whole information on the source sentence. This looses some information&lt;sup id=sf-Primer-NN-2-back&gt;&lt;a title="Due to its nature of compressing the input into lower dimension." class=simple-footnote href=#sf-Primer-NN-2&gt;2&lt;/a&gt;&lt;/sup&gt;. One can try to use some heuristics that can help to overcome this issue and improve performance of RNN:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Organize input.&lt;/strong&gt; Feed the input more than one time or provide input followed by reversed input &lt;sup id=sf-Primer-NN-3-back&gt;&lt;a title=" 2014 Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. " class=simple-footnote href=#sf-Primer-NN-3&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Provide more memory.&lt;/strong&gt; It turns out that the bigger the size of the memory the better (eg. LSTM applied to language modeling&lt;sup id=sf-Primer-NN-4-back&gt;&lt;a title=" 2014 Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals, Recurrent Neural Network Regularization. Normally the dropout would perturb the recurrent connections amplifies the noise and making difficult for LSTM to learn to store information for long time, thus presenting lower performance. Here the authors modify the dropout regularization technique for LSTMs at the same time preserving memory by applying the dropout operator only to the non-recurrent connections." class=simple-footnote href=#sf-Primer-NN-4&gt;4&lt;/a&gt;&lt;/sup&gt;) the RNN performs on various tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To avoid the memorization problem there has been research on the &lt;em&gt;attention&lt;/em&gt; mechanism.&lt;/p&gt;
&lt;h3 id=attention-basis&gt;Attention basis&lt;a title="Permanent link" class=headerlink href=#attention-basis&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The most intuitive &lt;strong&gt;attention definition&lt;/strong&gt; I know is the one contained within &lt;a href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/"&gt;paper about Transformer architecture&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We will try to formalize this (let me know your suggestions) in the following form:&lt;/p&gt;
&lt;div class=math&gt;$$\begin{eqnarray} A(q, \{(k,v)\} ) \xrightarrow[\text{output}]{\text{maps as}} \sum_{i=1}^k{\overbrace{f_c(q,k_i)}^{\theta_i}}v_i, q \in Q, k \in K, v \in V \end{eqnarray}$$&lt;/div&gt;
&lt;div class=math&gt;$$Q, K, V ‚Äì vector space, f_c- compatibility function$$&lt;/div&gt;
&lt;p&gt;Concretely, an attention  mechanism is distribution of weights over the input states. It takes any number of inputs &lt;span class=math&gt;\({x_1, ..., x_k}\)&lt;/span&gt;, and a query &lt;span class=math&gt;\(q\)&lt;/span&gt;, and then produces weights &lt;span class=math&gt;\({\theta_1, ..., \theta_k}\)&lt;/span&gt; for each input. This  measures how much each input interacts with (or answers) the query. The output of the attention mechanism, &lt;span class=math&gt;\(out\)&lt;/span&gt;, is therefore the weighted average of its inputs:
&lt;/p&gt;
&lt;div class=math&gt;$$ \begin{eqnarray} out = \sum_{i=1}^k \theta_i x_i \end{eqnarray}$$&lt;/div&gt;
&lt;p&gt;Hence, networks that use attention, attend only on a part of the input sequence while still providing the output sentence. So one can image it as giving an auxiliary input for the network in form of linear combination. What is the clue here is that the weights of the linear combination are controlled by the network. It has been tested across multiple areas such as NMT or speech recognition.&lt;/p&gt;
&lt;p&gt;In an encoder-decoder architecture for the embedding (fixed size vector) of a long sentence brings the problem of long dependency (e.g. in fixed-size encoded vector &lt;span class=math&gt;\(h^E_N\)&lt;/span&gt;, where end word of the sentence depends on the starting word &lt;span class=math&gt;\(h^E_1\)&lt;/span&gt;). And long dependencies, is where RNN have problems. Even though ‚Äúhacks‚Äù such as reverse/double feeding of the source sentence, or the LSTMs (memory), are sometimes improving the performance however, they do not always work perfectly&lt;sup id=sf-Primer-NN-5-back&gt;&lt;a title="E.g. reversing sentence in language where the first word of output is dependent on the last word of an input will decrease  performance even worse. Then the first output word would depend on a word that is last in the processing chain of a reversed input." class=simple-footnote href=#sf-Primer-NN-5&gt;5&lt;/a&gt;&lt;/sup&gt;. It is because the state and the gradient in LSTM would start to make the gradient vanish. It is called ‚Äúlong‚Äù time memory but its not that long to work e.g. for 2000 words. This is where &lt;em&gt;convolutions&lt;/em&gt; came in.&lt;/p&gt;
&lt;p align=center&gt;&lt;img src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/WaveNet.gif" title="WaveNet architecture" alt="WaveNet architecture"&gt;
&lt;img src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/ByteNet.png" title="ByteNet architecture" alt="ByteNet architecture"&gt;
&lt;br&gt;
Figure 2. WaveNet (left; sound) (&lt;a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/"&gt;image courtesy&lt;/a&gt;) and ByteNet (right; NLP) architecture (Image acquired from &lt;a href="https://arxiv.org/abs/1610.10099"&gt;Neural Machine Translation in Linear Time, Kalchbrenner et. al 2016&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This is why the fixed size encoding might be a bottleneck of performance. This is where the attention comes in.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Attention&lt;/em&gt;  is used as an alternative to memorizing and input manipulations. Here the model search for parts  of a source sentence (not fixed size vector) that are relevant to predicting a target word, and it should attend to, based on what it has learned in the past &lt;a href="https://arxiv.org/abs/1409.0473"&gt;Bahdanu et al., 2016&lt;/a&gt;.&lt;/p&gt;
&lt;p align=center&gt;&lt;img src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/attention.png" title="Attention architecture2" alt="Attention architecture"&gt;
&lt;br&gt;
Figure 2. Attention architecture.&lt;/p&gt;
&lt;p&gt;In this case, the larger  &lt;span class=math&gt;\(\theta_{1,2}\)&lt;/span&gt;  would be the more decoder pays attention for the &lt;span class=math&gt;\(y_3\)&lt;/span&gt; (third output word) in relation to other words as all weights (&lt;span class=math&gt;\(\theta\)&lt;/span&gt;s) are normalized to sum to &lt;span class=math&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p align=center&gt;&lt;img src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/EncDecAttention.gif" title="Attention architecture3" alt="Attention architecture"&gt;
&lt;br&gt;
Figure 3. ‚ÄúAttention‚Äù; the blue link transparency represents how much the decoder pays attention to an encoded word. Less transparent, more attention. &lt;a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html"&gt;Google Blog&lt;/a&gt; also &lt;a href="https://google.github.io/seq2seq/"&gt;general-purpose encoder-decoder framework for Tensorflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It is well visualized in an &lt;span class=math&gt;\(\theta\)&lt;/span&gt; matrix for French-English translation (See Figure Adopted form &lt;a href="https://arxiv.org/abs/1409.0473"&gt;Bahdanu et al., 2016&lt;/a&gt;)&lt;/p&gt;
&lt;p align=center&gt;&lt;img src="https://mchromiak.github.io/articles/2017/Sep/01/Primer-NN/img/attentionmatrix.png" alt="Attention matrix "&gt;
&lt;br&gt;
Figure 4. Attention matrix. Adopted form &lt;a href="https://arxiv.org/abs/1409.0473"&gt;Bahdanu et al., 2016&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;In French input sentence two words ‚Äú&lt;em&gt;la zone&lt;/em&gt;‚Äù are especially target for attention when translating to English ‚Äú&lt;em&gt;Area&lt;/em&gt;‚Äù.&lt;/p&gt;
&lt;p&gt;This matrix of size &lt;em&gt;InputSize&lt;/em&gt; &lt;span class=math&gt;\(\times\)&lt;/span&gt; &lt;em&gt;OutputSize&lt;/em&gt; is filled by calculating attention for each &lt;em&gt;output&lt;/em&gt; symbol toward over every &lt;em&gt;input&lt;/em&gt; symbol. This gives us &lt;span class=math&gt;\(\#Input^{\#Output}\)&lt;/span&gt;. For a longer sequences it might grow intensively.
This approach requires a complete look-up over all input output elements, which is not actually working as an biological attention would. Intuitively attention should discard irrelevant objects without the need to interacting with them.
What is called ‚Äúattention‚Äù therefore is simply a kind of memory that is available for decoder while producing every single output element. It does not need the fixed-size encoded vector of the entire input. The weights only decide which symbols/words to get from the input-memory of the encoder.
This so called attention is the subject of further development in form of &lt;a href="https://arxiv.org/abs/1503.08895"&gt;End-To-End Memory Networks&lt;/a&gt; where recurrent attention is used where  where multiple computational steps (hops) are performed per output symbol. This includes reading the same sequence many times before generating output and also changing the memory content at each step.&lt;/p&gt;
&lt;p&gt;We can of course employ backpropagation to accustom the weights in an end-to-end learning  model.&lt;/p&gt;
&lt;h3 id=layer-connections&gt;Layer connections &lt;!--http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref:6--&gt;&lt;a title="Permanent link" class=headerlink href=#layer-connections&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In Deep Learning networks architectures the problem of vanishing gradient is very common issue. Thus, some methods has been develop to mitigate its influence on the networks‚Äô efficiency.  &lt;/p&gt;
&lt;h4 id=highway-layers&gt;Highway layers&lt;a title="Permanent link" class=headerlink href=#highway-layers&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;h4 id=residual-connections&gt;Residual connections&lt;a title="Permanent link" class=headerlink href=#residual-connections&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Models with many layers often rely on shortcut or residual connections (Zhou et al., 2016; Wu et al., 2016). This trick has become main factor for &lt;a href="https://arxiv.org/abs/1512.03385"&gt;He et al. 2016 CVPR&lt;/a&gt; winning the ImageNet 2016 Residual connection is a connection between layers that adds the input &lt;span class=math&gt;\(x\)&lt;/span&gt; of the current layer to its output via a short-cut connection.&lt;/p&gt;
&lt;div class=math&gt;$$ h = f(Wx+b) + \mathbf{x} $$&lt;/div&gt;
&lt;p&gt;Residual connection helps with the &lt;em&gt;vanishing gradient&lt;/em&gt; problem because even if the layer nonlinearity &lt;span class=math&gt;\(f\)&lt;/span&gt; is not giving result the output then becomes the identity function in form of the &lt;span class=math&gt;\(\mathbf{x}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4 id=dense-connections&gt;Dense connections&lt;a title="Permanent link" class=headerlink href=#dense-connections&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;h3 id=position-embeddings&gt;Position Embeddings&lt;a title="Permanent link" class=headerlink href=#position-embeddings&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Usually used in non-recurrent networks (e.g. CNNs) that needs to store the order of the sequence‚Äôs input tokens in a way different than recurrent one. The RNN learn the exact position in sequence through the recurrent hidden state computation.&lt;/p&gt;
&lt;h4 id=autoregressive-ar&gt;Autoregressive (AR)&lt;a title="Permanent link" class=headerlink href=#autoregressive-ar&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;The problem of auto-regression is conditional mean of the distribution of future observations &lt;a href="https://arxiv.org/abs/1703.04122"&gt;Binkowski et al, 2017&lt;/a&gt;. In an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable. The term &lt;em&gt;autoregression&lt;/em&gt; indicates that it is a regression of the variable against itself. In NN this refers to autoregressive understood as each unit receives input both from the preceding layer and the preceding units within the same layer.&lt;/p&gt;
&lt;h3 id=end-to-end-learningtrainig&gt;End-to-End learning/trainig&lt;a title="Permanent link" class=headerlink href=#end-to-end-learningtrainig&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Simplest way to train a model (i.e. to learn) is to put input at one end and get output on the other end. With NN an end-to-end learning would simply mean optimize network weights base on &lt;strong&gt;single model&lt;/strong&gt; with input and output.&lt;/p&gt;
&lt;p&gt;There are however some cases when one model is not enough to achieve the desired output. It would then require a pipeline of independently trained models. It is most of the the case when input and output are of two distinct domains. Another case might be when NN has to many layers to fit into memory.  Hence, it is required to divide this one ‚Äúbig‚Äù NN into a pipeline of smaller ones. As a side note, this decomposition technique might not be effective because the optimizations would be done locally based only on intermediate outputs.&lt;/p&gt;
&lt;h5 id=examples&gt;Examples:&lt;a title="Permanent link" class=headerlink href=#examples&gt;üîó&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In case of a robot trained to move based on vision. One model might be used to pre-process the vision input (raw pixels) representation and pass it as input for another model responsible for decision process of which robot‚Äôs leg to move next. &lt;em&gt;image-to-motion&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Image captioning ‚Äì transforming raw image pixel information into text describing the image &lt;em&gt;image-to-text&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Speech recognition - Transform speech to text  &lt;em&gt;sound-to-text&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=hyperparameters-aka-meta-parameters-free-parameters&gt;Hyperparameters (aka meta-parameters, free-parameters)&lt;a title="Permanent link" class=headerlink href=#hyperparameters-aka-meta-parameters-free-parameters&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The crux of ML is finding (i.e. model training) a math formula (i.e. the model) with parameters that fit best into the data. However, the training is not able to find some higher level properties of model such as complexity or speed of learning straight from the data. Those properties are referred to as hyperparameters. For each trained model hyperparameters are predefined before even training process starts. Their values are important, however requires additional work. This work is done by trying different values for hyperparameters and training different models on them. Using the tests  than, decides which values of hyperparameters should be chosen i.e. is the fastest to achieve the goal, requires less steps etc. So hyperparameters can be determined from the data indirectly by using model selection.&lt;/p&gt;
&lt;p&gt;General approach to ML problem is making four decisions choosing the exact:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model Type ‚Äì e.g. FNN, RNN, SVM, etc&lt;/li&gt;
&lt;li&gt;Architecture ‚Äì e.g. for RNN you choose number of hidden layers, number of units per hidden layer&lt;/li&gt;
&lt;li&gt;Training parameters ‚Äì e.g. decide learning rate, batch size&lt;/li&gt;
&lt;li&gt;Model parameters ‚Äì model training finds the model parameters such as weights and biases in NN
Hence, the hyper parameters are those considered in the &lt;em&gt;training parameters&lt;/em&gt; and &lt;em&gt;architecture&lt;/em&gt; steps.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=examples-of-hyperparameters&gt;Examples of hyperparameters:&lt;a title="Permanent link" class=headerlink href=#examples-of-hyperparameters&gt;üîó&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;learning rate in gradient algorithms, number of hidden layers, number of clusters in a k-means clustering, number of leaves or depth of a tree, batch size in minibatch gradient descent, regularization parameter     &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=transfer-learning&gt;Transfer learning&lt;a title="Permanent link" class=headerlink href=#transfer-learning&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Transfer learning is used in context of reinforced or supervised learning. Deep learning requires large datasets to perform well (avoiding overfitting etc.). The problem however arises when there is not enough data for a new task. Here is the idea to use other already existing and large datasets to fine-tunning NN to become useful for this new task with small amount of data (e.g. using &lt;strong&gt;features&lt;/strong&gt; pre-trained from CNN (ConvNet) can be feed linear support vector machine (SVM) ). In other words, one can transfer the learned representation to another problem. We must however avoid negative transfer as it can slow down training of target task.
Searching for function &lt;span class=math&gt;\(g()\)&lt;/span&gt; while having pre-trained &lt;span class=math&gt;\(h()\)&lt;/span&gt; use projection of new inputs &lt;span class=math&gt;\(x_i\)&lt;/span&gt; like this: &lt;span class=math&gt;\((h(f(x_i)))\)&lt;/span&gt;.&lt;/p&gt;
&lt;h5 id=examples_1&gt;Examples:&lt;a title="Permanent link" class=headerlink href=#examples_1&gt;üîó&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Positive transfer: If you learned previously how to classify a rotten vegetable form not rotten one you can apply this representation of rottenness into fruits even though you have never seen a rotten fruit before.&lt;/li&gt;
&lt;li&gt;Negative transfer: learning one skill makes learning second  skill more difficult. If a kickboxer is about to train how to box. it might be hard to understand what is boxing unless he will be warned that he is not allowed to use kicks to box along with rules.&lt;/li&gt;
&lt;li&gt;Proactive transfer:  When a model learned in the past affects the new model to be learned&lt;/li&gt;
&lt;li&gt;Retroactive transfer: When a new model affects previously learned one.&lt;/li&gt;
&lt;li&gt;Bilateral transfer: Robot learned to use left manipulator now must use also right manipulator however in a bit different symmetry.&lt;/li&gt;
&lt;li&gt;Zero transfer: Two models are independent.&lt;/li&gt;
&lt;li&gt;Stimulus generalization:  Knowing what is rotten fruit does not mean that if you find a rusty metal it is the same but you generalize enough to decide that in both cases it is unusable but not in the same way.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=fine-tuning&gt;Fine tuning&lt;a title="Permanent link" class=headerlink href=#fine-tuning&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Fine tuning is considered mainly in context of supervised learning. When choosing best hyperparameters for an algorithm, fine tuning involves using a large set of mechanisms that solve this problem. When adjusting the the behaviors of the algorithm to further improve performance without manipulating the model itself.
When fine tuning pre-trained model might be considered equivalent to transfer learning. This is true if the data used during the fine tuning procedure is of different nature than the data that the pre-trained model has been trained on.&lt;/p&gt;
&lt;h5 id=examples_2&gt;Examples:&lt;a title="Permanent link" class=headerlink href=#examples_2&gt;üîó&lt;/a&gt;&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Finding the best hyperparameters for the model&lt;/li&gt;
&lt;li&gt;In transfer learning instead of bringing the frozen pre-trained model one cane make it dynamic and let adapt more to the new task.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=evolutionary-algorithms-evolutionary-computation-ec&gt;Evolutionary algorithms (Evolutionary Computation - EC)&lt;a title="Permanent link" class=headerlink href=#evolutionary-algorithms-evolutionary-computation-ec&gt;üîó&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;In machine learning problems, you typically have two components:&lt;/p&gt;
&lt;div class=highlight&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;1. The model (function class, etc)
2. Methods of fitting the model (optimization algorithms)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Neural networks are a model: given a layout and a setting of weights, the neural net produces some output. There exist some canonical methods of fitting neural nets, such as backpropagation, contrastive divergence, etc. However, the big point of neural networks is that if someone gave you the 'right' weights, you'd do well on the problem.&lt;/p&gt;
&lt;p&gt;Evolutionary algorithms address the second part -- fitting the model. Again, there are some canonical models that go with evolutionary algorithms: for example, evolutionary programming typically tries to optimize over all programs of a particular type. However, EAs are essentially a way of finding the right parameter values for a particular model. Usually, you write your model parameters in such a way that the crossover operation is a reasonable thing to do and turn the EA crank to get a reasonable setting of parameters out.&lt;/p&gt;
&lt;p&gt;Evolutionary algorithms are one class of strategies that can be used in machine learning, just like backpropagation and many others.&lt;/p&gt;
&lt;p&gt;Evolutionary algorithms usually converge slowly because they make no use of gradient information. On the other hand they provide at least a chance to escape from local optima and find the global one.&lt;/p&gt;
&lt;p&gt;Now, you could, for example, use evolutionary algorithms to train a neural network and I'm sure it's been done. However, the critical bit that EA require to work is that the crossover operation must be a reasonable thing to do -- by taking part of the parameters from one reasonable setting and the rest from another reasonable setting, you'll often end up with an even better parameter setting. Most times EA is used, this is not the case and it ends up being something like simulated annealing, only more confusing and inefficient.&lt;/p&gt;
&lt;!--
### ToDos

* training (aka learning: Fitting the "hyper parameters" of the model for all the examples) vs inference (Learning the values of the latent variables for a specific example) vs prediciton https://www.quora.com/What-is-the-difference-between-inference-and-learning
 https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/

* Pooling
* element embedding
* ROUGE, preplexity, BLEU
* optimizers: GD (BGD, SGD, MBGD), AdaGrad, Adam, ...)
* regularization (L1,and L2, dropout)
* generalization
* Stacked NN (SNN) vs Deep NN (DNN)
* reverse-mode differentiation (a.k.a. backpropagation) http://colah.github.io/posts/2015-08-Backprop/
* 1x(Feed forward) + 1x(backpropagation) = epoch (eg 10-15 epochs)
* Loss (aka cost) functions: Quadratic vs cross-entropy  http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function
http://colah.github.io/posts/2015-09-Visual-Information/   https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation

--&gt;

&lt;hr&gt;
&lt;h4 id=references&gt;References:&lt;a title="Permanent link" class=headerlink href=#references&gt;üîó&lt;/a&gt;&lt;/h4&gt;
&lt;script&gt;
&lt;!-- This adds {}--&gt;
(function() {
    var hostname = window.location.hostname;
    var new_tab = true;
    var set_icon = true;
    for (var links = document.links, i = 0, a; a = links[i]; i++) {
        if (a.hostname !== hostname) {
            if (new_tab)
                a.target = '_blank';
            if (set_icon)
                a.innerHTML +=
                    '&amp;nbsp&lt;i class="fa fa-external-link fa-1 external-link-margin" /&gt;';
        }
    }
})();
&lt;/script&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;ol class=simple-footnotes&gt;&lt;li id=sf-Primer-NN-1&gt;One can often encounter references to &lt;strong&gt;autoencoder (AE)&lt;/strong&gt; neural network. In case of which we are considering a perfect Encoder-Decoder network where its input is matching its output. In such case the network can reconstruct its own input. The input size is reduced/compressed with hidden layers until the input is compressed into required size (also the size of the target hidden layer) of few variables. From this compressed representation the network tries to reconstruct (decode) the input. Autoencoder is a feature extraction algorithm that helps to find a representation for data and so that the representation can be feed to other algorithms, for example a classifier. Autoencoders can be stacked and trained in a progressive way, we train an autoencoder and then we take the middle layer generated by the AE and use it as input for another AE and so on. &lt;a href="https://www.quora.com/What-is-an-auto-encoder-in-machine-learning"&gt;Great example of autoencoder on Quora&lt;/a&gt;  &lt;a class=simple-footnote-back href=#sf-Primer-NN-1-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Primer-NN-2&gt;Due to its nature of compressing the input into lower dimension. &lt;a class=simple-footnote-back href=#sf-Primer-NN-2-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Primer-NN-3&gt; 2014 Ilya Sutskever, Oriol Vinyals, and Quoc V Le. &lt;a href="https://arxiv.org/abs/1409.3215"&gt;Sequence to sequence learning with neural networks.&lt;/a&gt;  &lt;a class=simple-footnote-back href=#sf-Primer-NN-3-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Primer-NN-4&gt; 2014 Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals, &lt;a href="https://arxiv.org/abs/1409.2329"&gt;Recurrent Neural Network Regularization&lt;/a&gt;. Normally the dropout would perturb the recurrent connections amplifies the noise and making difficult for LSTM to learn to store information for long time, thus presenting lower performance. Here the authors modify the dropout regularization technique for LSTMs at the same time preserving memory by applying the dropout operator only to the non-recurrent connections. &lt;a class=simple-footnote-back href=#sf-Primer-NN-4-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;li id=sf-Primer-NN-5&gt;E.g. reversing sentence in language where the first word of output is dependent on the last word of an input will decrease  performance even worse. Then the first output word would depend on a word that is last in the processing chain of a reversed input. &lt;a class=simple-footnote-back href=#sf-Primer-NN-5-back&gt;‚Ü©&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</content><category term="basics"></category></entry></feed>